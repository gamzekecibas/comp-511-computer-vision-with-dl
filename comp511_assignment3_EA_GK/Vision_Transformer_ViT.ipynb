{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3294ab",
   "metadata": {
    "id": "L8tQwIsaWdLk",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ad38c",
   "metadata": {
    "id": "BQV3ocAXWdLn"
   },
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045c8c62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoHKTP5SWgm5",
    "outputId": "2745213d-30b9-42c3-dda0-2fb6d943c540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ee271",
   "metadata": {
    "id": "oZ4l7VqZWdLo",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c96c14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5IsrKjrWdLp",
    "outputId": "773b09b3-72fd-4712-b2b9-e4e863b65883",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a45ab",
   "metadata": {
    "id": "6RMEllRrWdLq",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4b2447",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJbSCWFFWdLr",
    "outputId": "004b1d9a-3a44-49a7-f146-f951bd573c04",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e004b",
   "metadata": {
    "id": "VeB-icVsWdLr"
   },
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a70d088",
   "metadata": {
    "id": "206ecd45"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "    \n",
    "        attention_logits = torch.matmul(q, k.permute(0,1,3,2)) / np.sqrt(self.head_dims)     \n",
    "        \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        ## Input is Attention logits with size [B,H,N,N]\n",
    "        ## Output should be of size [B,H,N,N]\n",
    "\n",
    "        normalized_attention_logits = torch.softmax(attention_logits, dim=-1)\n",
    "\n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "\n",
    "        ## attn_out = self.W_O(attention_output_logits)\n",
    "        attn_out = torch.matmul(normalized_attention_logits, v).permute(0,2,1,3).reshape(b,n,self.proj_dims)\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e82b6",
   "metadata": {
    "id": "033e8d01"
   },
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f92356",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d3355be",
    "outputId": "4d9fa424-7086-4deb-80ad-0e0cfdeb71c3",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d2c33",
   "metadata": {
    "id": "58ed9133"
   },
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ce49e5",
   "metadata": {
    "id": "0f1aad75"
   },
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.elu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebb8c50",
   "metadata": {
    "id": "e33aa313"
   },
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "    ###############################################################\n",
    "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "    ###############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.attention_norm_layer = nn.LayerNorm(hidden_dims)\n",
    "        self.multihead_attention  = SelfAttention(hidden_dims, head_dims=32, num_heads=num_heads,  bias=False)\n",
    "        self.mlp_norm_layer       = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp                  = MLP(hidden_dims, hidden_dims*4, hidden_dims, bias=True)\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ##############################################################\n",
    "    # TODO: Complete the forward of TransformerBlock module      #\n",
    "    ##############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        attention_norm_layer = self.attention_norm_layer(x)\n",
    "        multihead_attention  = self.multihead_attention(attention_norm_layer)\n",
    "        mlp_norm_layer       = self.mlp_norm_layer(multihead_attention + x)\n",
    "        mlp                  = self.mlp(mlp_norm_layer)\n",
    "        output               = mlp + mlp_norm_layer\n",
    "        return output\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19386f",
   "metadata": {
    "id": "5585d6dc"
   },
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18207b1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11f594a7",
    "outputId": "9b8db908-b68f-4ef9-a6c8-ba10ea7b1a62",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f8e4e",
   "metadata": {
    "id": "1e918eff"
   },
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed845f1c",
   "metadata": {
    "id": "41e640ad"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14115d9",
   "metadata": {
    "id": "d75bf697"
   },
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbce67f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1937800f",
    "outputId": "13f56bcd-7da0-414e-b4b6-fc5ba06709d0",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca5774",
   "metadata": {
    "id": "c8c0fa09"
   },
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77231ebf",
   "metadata": {
    "id": "6ea9ae7e"
   },
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd7e04d",
   "metadata": {
    "id": "38d5a8fd",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639484c",
   "metadata": {
    "id": "9c0e406a"
   },
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc01b30",
   "metadata": {
    "id": "112844f3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c007eda",
   "metadata": {
    "id": "e3a4f54c"
   },
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6964e0ea",
   "metadata": {
    "id": "f314d4eb"
   },
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82ddf9",
   "metadata": {
    "id": "83b031c6"
   },
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64d3954d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e846960",
    "outputId": "5e12acc9-13d5-4a12-c0cc-79192ee36514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a322bc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88d8decc",
    "outputId": "0ed39abb-6b70-484b-de1f-badcc15118a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return correct, np.float(correct) / (b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.692 | Acc: 8.000% (2/25)\n",
      "Loss: 4.750 | Acc: 10.000% (5/50)\n",
      "Loss: 4.581 | Acc: 8.000% (6/75)\n",
      "Loss: 4.342 | Acc: 15.000% (15/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 15.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 4.969 | Acc: 24.000% (6/25)\n",
      "Loss: 5.264 | Acc: 18.000% (9/50)\n",
      "Loss: 5.187 | Acc: 17.333% (13/75)\n",
      "Loss: 5.229 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 4.048 | Acc: 24.000% (6/25)\n",
      "Loss: 4.026 | Acc: 16.000% (8/50)\n",
      "Loss: 4.040 | Acc: 16.000% (12/75)\n",
      "Loss: 3.796 | Acc: 14.000% (14/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 14.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.510 | Acc: 8.000% (2/25)\n",
      "Loss: 3.767 | Acc: 8.000% (4/50)\n",
      "Loss: 3.895 | Acc: 8.000% (6/75)\n",
      "Loss: 3.921 | Acc: 10.000% (10/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 3.169 | Acc: 8.000% (2/25)\n",
      "Loss: 2.999 | Acc: 12.000% (6/50)\n",
      "Loss: 2.927 | Acc: 13.333% (10/75)\n",
      "Loss: 2.743 | Acc: 15.000% (15/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 15.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.604 | Acc: 4.000% (1/25)\n",
      "Loss: 2.428 | Acc: 10.000% (5/50)\n",
      "Loss: 2.398 | Acc: 9.333% (7/75)\n",
      "Loss: 2.416 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.700 | Acc: 8.000% (2/25)\n",
      "Loss: 2.489 | Acc: 12.000% (6/50)\n",
      "Loss: 2.385 | Acc: 14.667% (11/75)\n",
      "Loss: 2.333 | Acc: 14.000% (14/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 14.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.433 | Acc: 20.000% (5/25)\n",
      "Loss: 2.541 | Acc: 14.000% (7/50)\n",
      "Loss: 2.546 | Acc: 12.000% (9/75)\n",
      "Loss: 2.542 | Acc: 13.000% (13/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 13.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.996 | Acc: 20.000% (5/25)\n",
      "Loss: 1.972 | Acc: 24.000% (12/50)\n",
      "Loss: 2.215 | Acc: 17.333% (13/75)\n",
      "Loss: 2.182 | Acc: 19.000% (19/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 19.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.413 | Acc: 20.000% (5/25)\n",
      "Loss: 2.633 | Acc: 16.000% (8/50)\n",
      "Loss: 2.638 | Acc: 14.667% (11/75)\n",
      "Loss: 2.687 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 2.069 | Acc: 24.000% (6/25)\n",
      "Loss: 2.120 | Acc: 18.000% (9/50)\n",
      "Loss: 2.112 | Acc: 24.000% (18/75)\n",
      "Loss: 2.088 | Acc: 24.000% (24/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 24.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.196 | Acc: 16.000% (4/25)\n",
      "Loss: 2.288 | Acc: 16.000% (8/50)\n",
      "Loss: 2.275 | Acc: 18.667% (14/75)\n",
      "Loss: 2.316 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.950 | Acc: 44.000% (11/25)\n",
      "Loss: 2.173 | Acc: 34.000% (17/50)\n",
      "Loss: 2.096 | Acc: 30.667% (23/75)\n",
      "Loss: 2.129 | Acc: 28.000% (28/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 28.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.131 | Acc: 28.000% (7/25)\n",
      "Loss: 2.286 | Acc: 20.000% (10/50)\n",
      "Loss: 2.286 | Acc: 20.000% (15/75)\n",
      "Loss: 2.293 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 2.033 | Acc: 20.000% (5/25)\n",
      "Loss: 1.884 | Acc: 30.000% (15/50)\n",
      "Loss: 2.017 | Acc: 26.667% (20/75)\n",
      "Loss: 1.996 | Acc: 25.000% (25/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 25.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.529 | Acc: 4.000% (1/25)\n",
      "Loss: 2.480 | Acc: 6.000% (3/50)\n",
      "Loss: 2.389 | Acc: 9.333% (7/75)\n",
      "Loss: 2.439 | Acc: 13.000% (13/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 13.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 2.039 | Acc: 24.000% (6/25)\n",
      "Loss: 2.012 | Acc: 24.000% (12/50)\n",
      "Loss: 1.990 | Acc: 21.333% (16/75)\n",
      "Loss: 1.983 | Acc: 23.000% (23/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 23.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.199 | Acc: 24.000% (6/25)\n",
      "Loss: 2.490 | Acc: 16.000% (8/50)\n",
      "Loss: 2.462 | Acc: 14.667% (11/75)\n",
      "Loss: 2.476 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 2.010 | Acc: 24.000% (6/25)\n",
      "Loss: 2.179 | Acc: 22.000% (11/50)\n",
      "Loss: 2.016 | Acc: 25.333% (19/75)\n",
      "Loss: 1.984 | Acc: 25.000% (25/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 25.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.842 | Acc: 4.000% (1/25)\n",
      "Loss: 2.577 | Acc: 10.000% (5/50)\n",
      "Loss: 2.456 | Acc: 12.000% (9/75)\n",
      "Loss: 2.478 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Final train set accuracy is 25.0\n",
      "Final val set accuracy is 14.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers=num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, \n",
    "            bias=False).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(10):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75659f46",
   "metadata": {
    "id": "2039676f"
   },
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c15eb6cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d261fa5c",
    "outputId": "d0291023-b37e-4b4d-a8ae-86a9db499b4c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.219 | Acc: 14.062% (9/64)\n",
      "Loss: 3.765 | Acc: 13.281% (17/128)\n",
      "Loss: 4.083 | Acc: 14.062% (27/192)\n",
      "Loss: 4.342 | Acc: 13.281% (34/256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return correct, np.float(correct) / (b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.125 | Acc: 13.750% (44/320)\n",
      "Loss: 3.979 | Acc: 15.104% (58/384)\n",
      "Loss: 3.833 | Acc: 15.179% (68/448)\n",
      "Loss: 3.640 | Acc: 16.602% (85/512)\n",
      "Loss: 3.513 | Acc: 16.319% (94/576)\n",
      "Loss: 3.398 | Acc: 15.781% (101/640)\n",
      "Loss: 3.308 | Acc: 15.625% (110/704)\n",
      "Loss: 3.246 | Acc: 15.234% (117/768)\n",
      "Loss: 3.173 | Acc: 15.745% (131/832)\n",
      "Loss: 3.114 | Acc: 16.071% (144/896)\n",
      "Loss: 3.055 | Acc: 16.562% (159/960)\n",
      "Loss: 3.011 | Acc: 15.918% (163/1024)\n",
      "Loss: 2.970 | Acc: 15.625% (170/1088)\n",
      "Loss: 2.936 | Acc: 15.365% (177/1152)\n",
      "Loss: 2.901 | Acc: 15.707% (191/1216)\n",
      "Loss: 2.866 | Acc: 15.781% (202/1280)\n",
      "Loss: 2.842 | Acc: 15.551% (209/1344)\n",
      "Loss: 2.813 | Acc: 15.625% (220/1408)\n",
      "Loss: 2.784 | Acc: 15.965% (235/1472)\n",
      "Loss: 2.757 | Acc: 16.211% (249/1536)\n",
      "Loss: 2.741 | Acc: 16.125% (258/1600)\n",
      "Loss: 2.715 | Acc: 16.346% (272/1664)\n",
      "Loss: 2.695 | Acc: 16.262% (281/1728)\n",
      "Loss: 2.670 | Acc: 16.462% (295/1792)\n",
      "Loss: 2.651 | Acc: 16.272% (302/1856)\n",
      "Loss: 2.627 | Acc: 16.667% (320/1920)\n",
      "Loss: 2.620 | Acc: 16.583% (329/1984)\n",
      "Loss: 2.608 | Acc: 16.748% (343/2048)\n",
      "Loss: 2.594 | Acc: 16.951% (358/2112)\n",
      "Loss: 2.583 | Acc: 16.958% (369/2176)\n",
      "Loss: 2.569 | Acc: 17.009% (381/2240)\n",
      "Loss: 2.557 | Acc: 17.014% (392/2304)\n",
      "Loss: 2.545 | Acc: 17.272% (409/2368)\n",
      "Loss: 2.534 | Acc: 17.270% (420/2432)\n",
      "Loss: 2.521 | Acc: 17.548% (438/2496)\n",
      "Loss: 2.506 | Acc: 17.734% (454/2560)\n",
      "Loss: 2.502 | Acc: 17.721% (465/2624)\n",
      "Loss: 2.492 | Acc: 17.894% (481/2688)\n",
      "Loss: 2.488 | Acc: 17.987% (495/2752)\n",
      "Loss: 2.482 | Acc: 18.253% (514/2816)\n",
      "Loss: 2.476 | Acc: 18.194% (524/2880)\n",
      "Loss: 2.471 | Acc: 18.139% (534/2944)\n",
      "Loss: 2.466 | Acc: 18.118% (545/3008)\n",
      "Loss: 2.462 | Acc: 18.001% (553/3072)\n",
      "Loss: 2.458 | Acc: 17.953% (563/3136)\n",
      "Loss: 2.451 | Acc: 18.062% (578/3200)\n",
      "Loss: 2.442 | Acc: 18.290% (597/3264)\n",
      "Loss: 2.439 | Acc: 18.329% (610/3328)\n",
      "Loss: 2.435 | Acc: 18.249% (619/3392)\n",
      "Loss: 2.429 | Acc: 18.316% (633/3456)\n",
      "Loss: 2.424 | Acc: 18.239% (642/3520)\n",
      "Loss: 2.417 | Acc: 18.276% (655/3584)\n",
      "Loss: 2.411 | Acc: 18.229% (665/3648)\n",
      "Loss: 2.407 | Acc: 18.292% (679/3712)\n",
      "Loss: 2.403 | Acc: 18.432% (696/3776)\n",
      "Loss: 2.398 | Acc: 18.438% (708/3840)\n",
      "Loss: 2.393 | Acc: 18.443% (720/3904)\n",
      "Loss: 2.391 | Acc: 18.347% (728/3968)\n",
      "Loss: 2.385 | Acc: 18.328% (739/4032)\n",
      "Loss: 2.380 | Acc: 18.311% (750/4096)\n",
      "Loss: 2.377 | Acc: 18.365% (764/4160)\n",
      "Loss: 2.373 | Acc: 18.395% (777/4224)\n",
      "Loss: 2.369 | Acc: 18.400% (789/4288)\n",
      "Loss: 2.366 | Acc: 18.405% (801/4352)\n",
      "Loss: 2.361 | Acc: 18.456% (815/4416)\n",
      "Loss: 2.358 | Acc: 18.504% (829/4480)\n",
      "Loss: 2.352 | Acc: 18.508% (841/4544)\n",
      "Loss: 2.347 | Acc: 18.728% (863/4608)\n",
      "Loss: 2.345 | Acc: 18.750% (876/4672)\n",
      "Loss: 2.342 | Acc: 18.877% (894/4736)\n",
      "Loss: 2.339 | Acc: 18.896% (907/4800)\n",
      "Loss: 2.338 | Acc: 18.853% (917/4864)\n",
      "Loss: 2.335 | Acc: 18.953% (934/4928)\n",
      "Loss: 2.330 | Acc: 18.950% (946/4992)\n",
      "Loss: 2.326 | Acc: 19.027% (962/5056)\n",
      "Loss: 2.322 | Acc: 19.121% (979/5120)\n",
      "Loss: 2.317 | Acc: 19.194% (995/5184)\n",
      "Loss: 2.315 | Acc: 19.207% (1008/5248)\n",
      "Loss: 2.312 | Acc: 19.258% (1023/5312)\n",
      "Loss: 2.309 | Acc: 19.308% (1038/5376)\n",
      "Loss: 2.306 | Acc: 19.338% (1052/5440)\n",
      "Loss: 2.302 | Acc: 19.350% (1065/5504)\n",
      "Loss: 2.297 | Acc: 19.486% (1085/5568)\n",
      "Loss: 2.293 | Acc: 19.513% (1099/5632)\n",
      "Loss: 2.290 | Acc: 19.628% (1118/5696)\n",
      "Loss: 2.287 | Acc: 19.653% (1132/5760)\n",
      "Loss: 2.284 | Acc: 19.729% (1149/5824)\n",
      "Loss: 2.280 | Acc: 19.769% (1164/5888)\n",
      "Loss: 2.277 | Acc: 19.808% (1179/5952)\n",
      "Loss: 2.274 | Acc: 19.847% (1194/6016)\n",
      "Loss: 2.271 | Acc: 19.885% (1209/6080)\n",
      "Loss: 2.270 | Acc: 19.971% (1227/6144)\n",
      "Loss: 2.269 | Acc: 19.974% (1240/6208)\n",
      "Loss: 2.267 | Acc: 20.057% (1258/6272)\n",
      "Loss: 2.265 | Acc: 20.044% (1270/6336)\n",
      "Loss: 2.265 | Acc: 19.969% (1278/6400)\n",
      "Loss: 2.263 | Acc: 19.972% (1291/6464)\n",
      "Loss: 2.260 | Acc: 20.021% (1307/6528)\n",
      "Loss: 2.258 | Acc: 20.085% (1324/6592)\n",
      "Loss: 2.255 | Acc: 20.207% (1345/6656)\n",
      "Loss: 2.254 | Acc: 20.179% (1356/6720)\n",
      "Loss: 2.252 | Acc: 20.209% (1371/6784)\n",
      "Loss: 2.251 | Acc: 20.225% (1385/6848)\n",
      "Loss: 2.249 | Acc: 20.269% (1401/6912)\n",
      "Loss: 2.246 | Acc: 20.298% (1416/6976)\n",
      "Loss: 2.243 | Acc: 20.355% (1433/7040)\n",
      "Loss: 2.242 | Acc: 20.467% (1454/7104)\n",
      "Loss: 2.240 | Acc: 20.466% (1467/7168)\n",
      "Loss: 2.239 | Acc: 20.506% (1483/7232)\n",
      "Loss: 2.237 | Acc: 20.587% (1502/7296)\n",
      "Loss: 2.235 | Acc: 20.584% (1515/7360)\n",
      "Loss: 2.232 | Acc: 20.609% (1530/7424)\n",
      "Loss: 2.230 | Acc: 20.660% (1547/7488)\n",
      "Loss: 2.227 | Acc: 20.736% (1566/7552)\n",
      "Loss: 2.226 | Acc: 20.825% (1586/7616)\n",
      "Loss: 2.225 | Acc: 20.833% (1600/7680)\n",
      "Loss: 2.224 | Acc: 20.855% (1615/7744)\n",
      "Loss: 2.223 | Acc: 20.799% (1624/7808)\n",
      "Loss: 2.221 | Acc: 20.808% (1638/7872)\n",
      "Loss: 2.219 | Acc: 20.892% (1658/7936)\n",
      "Loss: 2.216 | Acc: 20.900% (1672/8000)\n",
      "Loss: 2.214 | Acc: 20.920% (1687/8064)\n",
      "Loss: 2.212 | Acc: 20.989% (1706/8128)\n",
      "Loss: 2.210 | Acc: 21.033% (1723/8192)\n",
      "Loss: 2.209 | Acc: 21.051% (1738/8256)\n",
      "Loss: 2.207 | Acc: 21.034% (1750/8320)\n",
      "Loss: 2.205 | Acc: 21.040% (1764/8384)\n",
      "Loss: 2.202 | Acc: 21.082% (1781/8448)\n",
      "Loss: 2.201 | Acc: 21.100% (1796/8512)\n",
      "Loss: 2.197 | Acc: 21.210% (1819/8576)\n",
      "Loss: 2.196 | Acc: 21.215% (1833/8640)\n",
      "Loss: 2.193 | Acc: 21.324% (1856/8704)\n",
      "Loss: 2.192 | Acc: 21.362% (1873/8768)\n",
      "Loss: 2.191 | Acc: 21.377% (1888/8832)\n",
      "Loss: 2.190 | Acc: 21.392% (1903/8896)\n",
      "Loss: 2.187 | Acc: 21.440% (1921/8960)\n",
      "Loss: 2.186 | Acc: 21.487% (1939/9024)\n",
      "Loss: 2.185 | Acc: 21.446% (1949/9088)\n",
      "Loss: 2.184 | Acc: 21.416% (1960/9152)\n",
      "Loss: 2.183 | Acc: 21.441% (1976/9216)\n",
      "Loss: 2.181 | Acc: 21.455% (1991/9280)\n",
      "Loss: 2.179 | Acc: 21.511% (2010/9344)\n",
      "Loss: 2.178 | Acc: 21.535% (2026/9408)\n",
      "Loss: 2.175 | Acc: 21.622% (2048/9472)\n",
      "Loss: 2.173 | Acc: 21.676% (2067/9536)\n",
      "Loss: 2.174 | Acc: 21.667% (2080/9600)\n",
      "Loss: 2.171 | Acc: 21.751% (2102/9664)\n",
      "Loss: 2.169 | Acc: 21.803% (2121/9728)\n",
      "Loss: 2.169 | Acc: 21.793% (2134/9792)\n",
      "Loss: 2.167 | Acc: 21.895% (2158/9856)\n",
      "Loss: 2.165 | Acc: 21.935% (2176/9920)\n",
      "Loss: 2.165 | Acc: 21.955% (2192/9984)\n",
      "Loss: 2.163 | Acc: 21.945% (2205/10048)\n",
      "Loss: 2.162 | Acc: 21.944% (2219/10112)\n",
      "Loss: 2.159 | Acc: 22.081% (2247/10176)\n",
      "Loss: 2.157 | Acc: 22.139% (2267/10240)\n",
      "Loss: 2.155 | Acc: 22.137% (2281/10304)\n",
      "Loss: 2.155 | Acc: 22.126% (2294/10368)\n",
      "Loss: 2.153 | Acc: 22.201% (2316/10432)\n",
      "Loss: 2.152 | Acc: 22.247% (2335/10496)\n",
      "Loss: 2.151 | Acc: 22.254% (2350/10560)\n",
      "Loss: 2.150 | Acc: 22.355% (2375/10624)\n",
      "Loss: 2.149 | Acc: 22.418% (2396/10688)\n",
      "Loss: 2.148 | Acc: 22.452% (2414/10752)\n",
      "Loss: 2.148 | Acc: 22.439% (2427/10816)\n",
      "Loss: 2.147 | Acc: 22.417% (2439/10880)\n",
      "Loss: 2.146 | Acc: 22.414% (2453/10944)\n",
      "Loss: 2.145 | Acc: 22.438% (2470/11008)\n",
      "Loss: 2.144 | Acc: 22.444% (2485/11072)\n",
      "Loss: 2.145 | Acc: 22.378% (2492/11136)\n",
      "Loss: 2.144 | Acc: 22.366% (2505/11200)\n",
      "Loss: 2.143 | Acc: 22.399% (2523/11264)\n",
      "Loss: 2.142 | Acc: 22.422% (2540/11328)\n",
      "Loss: 2.142 | Acc: 22.410% (2553/11392)\n",
      "Loss: 2.140 | Acc: 22.451% (2572/11456)\n",
      "Loss: 2.141 | Acc: 22.448% (2586/11520)\n",
      "Loss: 2.140 | Acc: 22.488% (2605/11584)\n",
      "Loss: 2.139 | Acc: 22.485% (2619/11648)\n",
      "Loss: 2.137 | Acc: 22.507% (2636/11712)\n",
      "Loss: 2.137 | Acc: 22.537% (2654/11776)\n",
      "Loss: 2.136 | Acc: 22.542% (2669/11840)\n",
      "Loss: 2.135 | Acc: 22.547% (2684/11904)\n",
      "Loss: 2.133 | Acc: 22.577% (2702/11968)\n",
      "Loss: 2.132 | Acc: 22.606% (2720/12032)\n",
      "Loss: 2.131 | Acc: 22.636% (2738/12096)\n",
      "Loss: 2.129 | Acc: 22.632% (2752/12160)\n",
      "Loss: 2.128 | Acc: 22.652% (2769/12224)\n",
      "Loss: 2.127 | Acc: 22.656% (2784/12288)\n",
      "Loss: 2.125 | Acc: 22.668% (2800/12352)\n",
      "Loss: 2.123 | Acc: 22.713% (2820/12416)\n",
      "Loss: 2.122 | Acc: 22.756% (2840/12480)\n",
      "Loss: 2.120 | Acc: 22.832% (2864/12544)\n",
      "Loss: 2.119 | Acc: 22.898% (2887/12608)\n",
      "Loss: 2.118 | Acc: 22.932% (2906/12672)\n",
      "Loss: 2.116 | Acc: 22.982% (2927/12736)\n",
      "Loss: 2.115 | Acc: 22.992% (2943/12800)\n",
      "Loss: 2.114 | Acc: 23.033% (2963/12864)\n",
      "Loss: 2.111 | Acc: 23.082% (2984/12928)\n",
      "Loss: 2.111 | Acc: 23.099% (3001/12992)\n",
      "Loss: 2.109 | Acc: 23.185% (3027/13056)\n",
      "Loss: 2.108 | Acc: 23.194% (3043/13120)\n",
      "Loss: 2.107 | Acc: 23.233% (3063/13184)\n",
      "Loss: 2.106 | Acc: 23.241% (3079/13248)\n",
      "Loss: 2.105 | Acc: 23.250% (3095/13312)\n",
      "Loss: 2.105 | Acc: 23.273% (3113/13376)\n",
      "Loss: 2.105 | Acc: 23.274% (3128/13440)\n",
      "Loss: 2.104 | Acc: 23.275% (3143/13504)\n",
      "Loss: 2.103 | Acc: 23.253% (3155/13568)\n",
      "Loss: 2.102 | Acc: 23.261% (3171/13632)\n",
      "Loss: 2.102 | Acc: 23.277% (3188/13696)\n",
      "Loss: 2.101 | Acc: 23.285% (3204/13760)\n",
      "Loss: 2.100 | Acc: 23.322% (3224/13824)\n",
      "Loss: 2.099 | Acc: 23.329% (3240/13888)\n",
      "Loss: 2.098 | Acc: 23.359% (3259/13952)\n",
      "Loss: 2.096 | Acc: 23.402% (3280/14016)\n",
      "Loss: 2.095 | Acc: 23.395% (3294/14080)\n",
      "Loss: 2.093 | Acc: 23.423% (3313/14144)\n",
      "Loss: 2.093 | Acc: 23.445% (3331/14208)\n",
      "Loss: 2.092 | Acc: 23.459% (3348/14272)\n",
      "Loss: 2.091 | Acc: 23.493% (3368/14336)\n",
      "Loss: 2.091 | Acc: 23.493% (3383/14400)\n",
      "Loss: 2.090 | Acc: 23.507% (3400/14464)\n",
      "Loss: 2.090 | Acc: 23.541% (3420/14528)\n",
      "Loss: 2.090 | Acc: 23.540% (3435/14592)\n",
      "Loss: 2.089 | Acc: 23.547% (3451/14656)\n",
      "Loss: 2.089 | Acc: 23.533% (3464/14720)\n",
      "Loss: 2.088 | Acc: 23.512% (3476/14784)\n",
      "Loss: 2.088 | Acc: 23.498% (3489/14848)\n",
      "Loss: 2.087 | Acc: 23.471% (3500/14912)\n",
      "Loss: 2.087 | Acc: 23.498% (3519/14976)\n",
      "Loss: 2.087 | Acc: 23.497% (3534/15040)\n",
      "Loss: 2.086 | Acc: 23.524% (3553/15104)\n",
      "Loss: 2.085 | Acc: 23.556% (3573/15168)\n",
      "Loss: 2.084 | Acc: 23.615% (3597/15232)\n",
      "Loss: 2.084 | Acc: 23.607% (3611/15296)\n",
      "Loss: 2.083 | Acc: 23.613% (3627/15360)\n",
      "Loss: 2.082 | Acc: 23.658% (3649/15424)\n",
      "Loss: 2.082 | Acc: 23.644% (3662/15488)\n",
      "Loss: 2.081 | Acc: 23.650% (3678/15552)\n",
      "Loss: 2.080 | Acc: 23.694% (3700/15616)\n",
      "Loss: 2.079 | Acc: 23.712% (3718/15680)\n",
      "Loss: 2.078 | Acc: 23.736% (3737/15744)\n",
      "Loss: 2.078 | Acc: 23.785% (3760/15808)\n",
      "Loss: 2.077 | Acc: 23.816% (3780/15872)\n",
      "Loss: 2.076 | Acc: 23.864% (3803/15936)\n",
      "Loss: 2.075 | Acc: 23.881% (3821/16000)\n",
      "Loss: 2.075 | Acc: 23.911% (3841/16064)\n",
      "Loss: 2.073 | Acc: 23.971% (3866/16128)\n",
      "Loss: 2.073 | Acc: 23.993% (3885/16192)\n",
      "Loss: 2.073 | Acc: 24.003% (3902/16256)\n",
      "Loss: 2.072 | Acc: 24.026% (3921/16320)\n",
      "Loss: 2.072 | Acc: 24.030% (3937/16384)\n",
      "Loss: 2.071 | Acc: 24.070% (3959/16448)\n",
      "Loss: 2.070 | Acc: 24.079% (3976/16512)\n",
      "Loss: 2.070 | Acc: 24.089% (3993/16576)\n",
      "Loss: 2.068 | Acc: 24.147% (4018/16640)\n",
      "Loss: 2.067 | Acc: 24.192% (4041/16704)\n",
      "Loss: 2.066 | Acc: 24.225% (4062/16768)\n",
      "Loss: 2.065 | Acc: 24.251% (4082/16832)\n",
      "Loss: 2.064 | Acc: 24.254% (4098/16896)\n",
      "Loss: 2.064 | Acc: 24.281% (4118/16960)\n",
      "Loss: 2.062 | Acc: 24.313% (4139/17024)\n",
      "Loss: 2.062 | Acc: 24.327% (4157/17088)\n",
      "Loss: 2.061 | Acc: 24.312% (4170/17152)\n",
      "Loss: 2.060 | Acc: 24.344% (4191/17216)\n",
      "Loss: 2.059 | Acc: 24.363% (4210/17280)\n",
      "Loss: 2.058 | Acc: 24.377% (4228/17344)\n",
      "Loss: 2.057 | Acc: 24.426% (4252/17408)\n",
      "Loss: 2.057 | Acc: 24.439% (4270/17472)\n",
      "Loss: 2.056 | Acc: 24.487% (4294/17536)\n",
      "Loss: 2.055 | Acc: 24.506% (4313/17600)\n",
      "Loss: 2.054 | Acc: 24.541% (4335/17664)\n",
      "Loss: 2.053 | Acc: 24.566% (4355/17728)\n",
      "Loss: 2.051 | Acc: 24.607% (4378/17792)\n",
      "Loss: 2.050 | Acc: 24.647% (4401/17856)\n",
      "Loss: 2.049 | Acc: 24.665% (4420/17920)\n",
      "Loss: 2.049 | Acc: 24.672% (4437/17984)\n",
      "Loss: 2.048 | Acc: 24.673% (4453/18048)\n",
      "Loss: 2.047 | Acc: 24.674% (4469/18112)\n",
      "Loss: 2.046 | Acc: 24.736% (4496/18176)\n",
      "Loss: 2.045 | Acc: 24.781% (4520/18240)\n",
      "Loss: 2.044 | Acc: 24.787% (4537/18304)\n",
      "Loss: 2.043 | Acc: 24.760% (4548/18368)\n",
      "Loss: 2.043 | Acc: 24.767% (4565/18432)\n",
      "Loss: 2.042 | Acc: 24.768% (4581/18496)\n",
      "Loss: 2.042 | Acc: 24.784% (4600/18560)\n",
      "Loss: 2.041 | Acc: 24.812% (4621/18624)\n",
      "Loss: 2.040 | Acc: 24.823% (4639/18688)\n",
      "Loss: 2.039 | Acc: 24.856% (4661/18752)\n",
      "Loss: 2.039 | Acc: 24.878% (4681/18816)\n",
      "Loss: 2.038 | Acc: 24.889% (4699/18880)\n",
      "Loss: 2.038 | Acc: 24.884% (4714/18944)\n",
      "Loss: 2.036 | Acc: 24.900% (4733/19008)\n",
      "Loss: 2.037 | Acc: 24.906% (4750/19072)\n",
      "Loss: 2.036 | Acc: 24.901% (4765/19136)\n",
      "Loss: 2.036 | Acc: 24.911% (4783/19200)\n",
      "Loss: 2.035 | Acc: 24.953% (4807/19264)\n",
      "Loss: 2.034 | Acc: 24.990% (4830/19328)\n",
      "Loss: 2.033 | Acc: 25.021% (4852/19392)\n",
      "Loss: 2.033 | Acc: 25.046% (4873/19456)\n",
      "Loss: 2.032 | Acc: 25.046% (4889/19520)\n",
      "Loss: 2.031 | Acc: 25.061% (4908/19584)\n",
      "Loss: 2.030 | Acc: 25.092% (4930/19648)\n",
      "Loss: 2.030 | Acc: 25.086% (4945/19712)\n",
      "Loss: 2.030 | Acc: 25.106% (4965/19776)\n",
      "Loss: 2.029 | Acc: 25.126% (4985/19840)\n",
      "Loss: 2.028 | Acc: 25.166% (5009/19904)\n",
      "Loss: 2.028 | Acc: 25.150% (5022/19968)\n",
      "Loss: 2.027 | Acc: 25.170% (5042/20032)\n",
      "Loss: 2.027 | Acc: 25.174% (5059/20096)\n",
      "Loss: 2.026 | Acc: 25.184% (5077/20160)\n",
      "Loss: 2.025 | Acc: 25.218% (5100/20224)\n",
      "Loss: 2.024 | Acc: 25.266% (5126/20288)\n",
      "Loss: 2.023 | Acc: 25.319% (5153/20352)\n",
      "Loss: 2.023 | Acc: 25.333% (5172/20416)\n",
      "Loss: 2.022 | Acc: 25.386% (5199/20480)\n",
      "Loss: 2.022 | Acc: 25.365% (5211/20544)\n",
      "Loss: 2.021 | Acc: 25.378% (5230/20608)\n",
      "Loss: 2.020 | Acc: 25.411% (5253/20672)\n",
      "Loss: 2.020 | Acc: 25.458% (5279/20736)\n",
      "Loss: 2.019 | Acc: 25.495% (5303/20800)\n",
      "Loss: 2.019 | Acc: 25.479% (5316/20864)\n",
      "Loss: 2.018 | Acc: 25.516% (5340/20928)\n",
      "Loss: 2.017 | Acc: 25.534% (5360/20992)\n",
      "Loss: 2.017 | Acc: 25.556% (5381/21056)\n",
      "Loss: 2.016 | Acc: 25.587% (5404/21120)\n",
      "Loss: 2.016 | Acc: 25.600% (5423/21184)\n",
      "Loss: 2.015 | Acc: 25.612% (5442/21248)\n",
      "Loss: 2.014 | Acc: 25.638% (5464/21312)\n",
      "Loss: 2.014 | Acc: 25.674% (5488/21376)\n",
      "Loss: 2.013 | Acc: 25.718% (5514/21440)\n",
      "Loss: 2.012 | Acc: 25.753% (5538/21504)\n",
      "Loss: 2.012 | Acc: 25.742% (5552/21568)\n",
      "Loss: 2.012 | Acc: 25.735% (5567/21632)\n",
      "Loss: 2.012 | Acc: 25.751% (5587/21696)\n",
      "Loss: 2.011 | Acc: 25.744% (5602/21760)\n",
      "Loss: 2.010 | Acc: 25.816% (5634/21824)\n",
      "Loss: 2.010 | Acc: 25.845% (5657/21888)\n",
      "Loss: 2.009 | Acc: 25.838% (5672/21952)\n",
      "Loss: 2.008 | Acc: 25.854% (5692/22016)\n",
      "Loss: 2.007 | Acc: 25.879% (5714/22080)\n",
      "Loss: 2.007 | Acc: 25.858% (5726/22144)\n",
      "Loss: 2.007 | Acc: 25.856% (5742/22208)\n",
      "Loss: 2.006 | Acc: 25.876% (5763/22272)\n",
      "Loss: 2.006 | Acc: 25.891% (5783/22336)\n",
      "Loss: 2.005 | Acc: 25.933% (5809/22400)\n",
      "Loss: 2.004 | Acc: 25.962% (5832/22464)\n",
      "Loss: 2.003 | Acc: 25.985% (5854/22528)\n",
      "Loss: 2.003 | Acc: 26.009% (5876/22592)\n",
      "Loss: 2.003 | Acc: 26.015% (5894/22656)\n",
      "Loss: 2.002 | Acc: 26.052% (5919/22720)\n",
      "Loss: 2.001 | Acc: 26.045% (5934/22784)\n",
      "Loss: 2.000 | Acc: 26.064% (5955/22848)\n",
      "Loss: 2.000 | Acc: 26.052% (5969/22912)\n",
      "Loss: 2.000 | Acc: 26.062% (5988/22976)\n",
      "Loss: 1.999 | Acc: 26.076% (6008/23040)\n",
      "Loss: 1.998 | Acc: 26.156% (6043/23104)\n",
      "Loss: 1.999 | Acc: 26.148% (6058/23168)\n",
      "Loss: 1.999 | Acc: 26.145% (6074/23232)\n",
      "Loss: 1.998 | Acc: 26.163% (6095/23296)\n",
      "Loss: 1.998 | Acc: 26.177% (6115/23360)\n",
      "Loss: 1.997 | Acc: 26.178% (6132/23424)\n",
      "Loss: 1.997 | Acc: 26.167% (6146/23488)\n",
      "Loss: 1.997 | Acc: 26.163% (6162/23552)\n",
      "Loss: 1.997 | Acc: 26.177% (6182/23616)\n",
      "Loss: 1.996 | Acc: 26.199% (6204/23680)\n",
      "Loss: 1.996 | Acc: 26.200% (6221/23744)\n",
      "Loss: 1.995 | Acc: 26.218% (6242/23808)\n",
      "Loss: 1.995 | Acc: 26.227% (6261/23872)\n",
      "Loss: 1.994 | Acc: 26.228% (6278/23936)\n",
      "Loss: 1.993 | Acc: 26.242% (6298/24000)\n",
      "Loss: 1.992 | Acc: 26.263% (6320/24064)\n",
      "Loss: 1.992 | Acc: 26.268% (6338/24128)\n",
      "Loss: 1.991 | Acc: 26.252% (6351/24192)\n",
      "Loss: 1.991 | Acc: 26.278% (6374/24256)\n",
      "Loss: 1.990 | Acc: 26.299% (6396/24320)\n",
      "Loss: 1.990 | Acc: 26.321% (6418/24384)\n",
      "Loss: 1.990 | Acc: 26.301% (6430/24448)\n",
      "Loss: 1.989 | Acc: 26.301% (6447/24512)\n",
      "Loss: 1.989 | Acc: 26.314% (6467/24576)\n",
      "Loss: 1.989 | Acc: 26.335% (6489/24640)\n",
      "Loss: 1.988 | Acc: 26.372% (6515/24704)\n",
      "Loss: 1.987 | Acc: 26.413% (6542/24768)\n",
      "Loss: 1.987 | Acc: 26.422% (6561/24832)\n",
      "Loss: 1.986 | Acc: 26.450% (6585/24896)\n",
      "Loss: 1.986 | Acc: 26.482% (6610/24960)\n",
      "Loss: 1.986 | Acc: 26.487% (6628/25024)\n",
      "Loss: 1.985 | Acc: 26.495% (6647/25088)\n",
      "Loss: 1.984 | Acc: 26.519% (6670/25152)\n",
      "Loss: 1.983 | Acc: 26.574% (6701/25216)\n",
      "Loss: 1.983 | Acc: 26.574% (6718/25280)\n",
      "Loss: 1.982 | Acc: 26.590% (6739/25344)\n",
      "Loss: 1.981 | Acc: 26.637% (6768/25408)\n",
      "Loss: 1.980 | Acc: 26.680% (6796/25472)\n",
      "Loss: 1.980 | Acc: 26.711% (6821/25536)\n",
      "Loss: 1.979 | Acc: 26.719% (6840/25600)\n",
      "Loss: 1.979 | Acc: 26.746% (6864/25664)\n",
      "Loss: 1.978 | Acc: 26.757% (6884/25728)\n",
      "Loss: 1.978 | Acc: 26.780% (6907/25792)\n",
      "Loss: 1.977 | Acc: 26.775% (6923/25856)\n",
      "Loss: 1.977 | Acc: 26.802% (6947/25920)\n",
      "Loss: 1.975 | Acc: 26.863% (6980/25984)\n",
      "Loss: 1.975 | Acc: 26.866% (6998/26048)\n",
      "Loss: 1.975 | Acc: 26.873% (7017/26112)\n",
      "Loss: 1.974 | Acc: 26.872% (7034/26176)\n",
      "Loss: 1.974 | Acc: 26.871% (7051/26240)\n",
      "Loss: 1.973 | Acc: 26.889% (7073/26304)\n",
      "Loss: 1.973 | Acc: 26.892% (7091/26368)\n",
      "Loss: 1.972 | Acc: 26.926% (7117/26432)\n",
      "Loss: 1.972 | Acc: 26.947% (7140/26496)\n",
      "Loss: 1.971 | Acc: 26.969% (7163/26560)\n",
      "Loss: 1.971 | Acc: 26.961% (7178/26624)\n",
      "Loss: 1.970 | Acc: 26.986% (7202/26688)\n",
      "Loss: 1.970 | Acc: 27.015% (7227/26752)\n",
      "Loss: 1.969 | Acc: 27.040% (7251/26816)\n",
      "Loss: 1.969 | Acc: 27.050% (7271/26880)\n",
      "Loss: 1.968 | Acc: 27.082% (7297/26944)\n",
      "Loss: 1.967 | Acc: 27.088% (7316/27008)\n",
      "Loss: 1.967 | Acc: 27.109% (7339/27072)\n",
      "Loss: 1.967 | Acc: 27.119% (7359/27136)\n",
      "Loss: 1.966 | Acc: 27.136% (7381/27200)\n",
      "Loss: 1.965 | Acc: 27.164% (7406/27264)\n",
      "Loss: 1.965 | Acc: 27.177% (7427/27328)\n",
      "Loss: 1.965 | Acc: 27.205% (7452/27392)\n",
      "Loss: 1.964 | Acc: 27.222% (7474/27456)\n",
      "Loss: 1.964 | Acc: 27.227% (7493/27520)\n",
      "Loss: 1.963 | Acc: 27.269% (7522/27584)\n",
      "Loss: 1.962 | Acc: 27.308% (7550/27648)\n",
      "Loss: 1.961 | Acc: 27.335% (7575/27712)\n",
      "Loss: 1.961 | Acc: 27.344% (7595/27776)\n",
      "Loss: 1.961 | Acc: 27.353% (7615/27840)\n",
      "Loss: 1.961 | Acc: 27.380% (7640/27904)\n",
      "Loss: 1.961 | Acc: 27.396% (7662/27968)\n",
      "Loss: 1.960 | Acc: 27.415% (7685/28032)\n",
      "Loss: 1.960 | Acc: 27.413% (7702/28096)\n",
      "Loss: 1.959 | Acc: 27.418% (7721/28160)\n",
      "Loss: 1.959 | Acc: 27.413% (7737/28224)\n",
      "Loss: 1.959 | Acc: 27.418% (7756/28288)\n",
      "Loss: 1.959 | Acc: 27.409% (7771/28352)\n",
      "Loss: 1.959 | Acc: 27.421% (7792/28416)\n",
      "Loss: 1.958 | Acc: 27.426% (7811/28480)\n",
      "Loss: 1.958 | Acc: 27.431% (7830/28544)\n",
      "Loss: 1.957 | Acc: 27.447% (7852/28608)\n",
      "Loss: 1.956 | Acc: 27.473% (7877/28672)\n",
      "Loss: 1.956 | Acc: 27.478% (7896/28736)\n",
      "Loss: 1.956 | Acc: 27.472% (7912/28800)\n",
      "Loss: 1.956 | Acc: 27.488% (7934/28864)\n",
      "Loss: 1.956 | Acc: 27.482% (7950/28928)\n",
      "Loss: 1.955 | Acc: 27.477% (7966/28992)\n",
      "Loss: 1.955 | Acc: 27.499% (7990/29056)\n",
      "Loss: 1.954 | Acc: 27.531% (8017/29120)\n",
      "Loss: 1.954 | Acc: 27.529% (8034/29184)\n",
      "Loss: 1.954 | Acc: 27.527% (8051/29248)\n",
      "Loss: 1.954 | Acc: 27.508% (8063/29312)\n",
      "Loss: 1.953 | Acc: 27.516% (8083/29376)\n",
      "Loss: 1.953 | Acc: 27.534% (8106/29440)\n",
      "Loss: 1.953 | Acc: 27.518% (8119/29504)\n",
      "Loss: 1.953 | Acc: 27.506% (8133/29568)\n",
      "Loss: 1.952 | Acc: 27.528% (8157/29632)\n",
      "Loss: 1.952 | Acc: 27.536% (8177/29696)\n",
      "Loss: 1.952 | Acc: 27.557% (8201/29760)\n",
      "Loss: 1.952 | Acc: 27.548% (8216/29824)\n",
      "Loss: 1.951 | Acc: 27.546% (8233/29888)\n",
      "Loss: 1.951 | Acc: 27.537% (8248/29952)\n",
      "Loss: 1.950 | Acc: 27.552% (8270/30016)\n",
      "Loss: 1.950 | Acc: 27.557% (8289/30080)\n",
      "Loss: 1.950 | Acc: 27.554% (8306/30144)\n",
      "Loss: 1.950 | Acc: 27.562% (8326/30208)\n",
      "Loss: 1.949 | Acc: 27.567% (8345/30272)\n",
      "Loss: 1.949 | Acc: 27.574% (8365/30336)\n",
      "Loss: 1.949 | Acc: 27.562% (8379/30400)\n",
      "Loss: 1.949 | Acc: 27.567% (8398/30464)\n",
      "Loss: 1.948 | Acc: 27.585% (8421/30528)\n",
      "Loss: 1.948 | Acc: 27.592% (8441/30592)\n",
      "Loss: 1.948 | Acc: 27.606% (8463/30656)\n",
      "Loss: 1.947 | Acc: 27.627% (8487/30720)\n",
      "Loss: 1.947 | Acc: 27.621% (8503/30784)\n",
      "Loss: 1.947 | Acc: 27.636% (8525/30848)\n",
      "Loss: 1.946 | Acc: 27.666% (8552/30912)\n",
      "Loss: 1.946 | Acc: 27.692% (8578/30976)\n",
      "Loss: 1.945 | Acc: 27.716% (8603/31040)\n",
      "Loss: 1.944 | Acc: 27.733% (8626/31104)\n",
      "Loss: 1.944 | Acc: 27.756% (8651/31168)\n",
      "Loss: 1.943 | Acc: 27.763% (8671/31232)\n",
      "Loss: 1.943 | Acc: 27.783% (8695/31296)\n",
      "Loss: 1.942 | Acc: 27.816% (8723/31360)\n",
      "Loss: 1.942 | Acc: 27.816% (8741/31424)\n",
      "Loss: 1.941 | Acc: 27.842% (8767/31488)\n",
      "Loss: 1.941 | Acc: 27.843% (8785/31552)\n",
      "Loss: 1.941 | Acc: 27.847% (8804/31616)\n",
      "Loss: 1.941 | Acc: 27.863% (8827/31680)\n",
      "Loss: 1.940 | Acc: 27.867% (8846/31744)\n",
      "Loss: 1.940 | Acc: 27.877% (8867/31808)\n",
      "Loss: 1.940 | Acc: 27.871% (8883/31872)\n",
      "Loss: 1.939 | Acc: 27.890% (8907/31936)\n",
      "Loss: 1.939 | Acc: 27.916% (8933/32000)\n",
      "Loss: 1.938 | Acc: 27.938% (8958/32064)\n",
      "Loss: 1.938 | Acc: 27.954% (8981/32128)\n",
      "Loss: 1.938 | Acc: 27.945% (8996/32192)\n",
      "Loss: 1.938 | Acc: 27.948% (9015/32256)\n",
      "Loss: 1.937 | Acc: 27.967% (9039/32320)\n",
      "Loss: 1.937 | Acc: 27.980% (9061/32384)\n",
      "Loss: 1.937 | Acc: 27.989% (9082/32448)\n",
      "Loss: 1.937 | Acc: 28.008% (9106/32512)\n",
      "Loss: 1.937 | Acc: 28.002% (9122/32576)\n",
      "Loss: 1.936 | Acc: 28.024% (9147/32640)\n",
      "Loss: 1.936 | Acc: 28.042% (9171/32704)\n",
      "Loss: 1.936 | Acc: 28.061% (9195/32768)\n",
      "Loss: 1.935 | Acc: 28.088% (9222/32832)\n",
      "Loss: 1.936 | Acc: 28.079% (9237/32896)\n",
      "Loss: 1.935 | Acc: 28.098% (9261/32960)\n",
      "Loss: 1.935 | Acc: 28.110% (9283/33024)\n",
      "Loss: 1.935 | Acc: 28.107% (9300/33088)\n",
      "Loss: 1.935 | Acc: 28.128% (9325/33152)\n",
      "Loss: 1.934 | Acc: 28.155% (9352/33216)\n",
      "Loss: 1.934 | Acc: 28.170% (9375/33280)\n",
      "Loss: 1.934 | Acc: 28.170% (9393/33344)\n",
      "Loss: 1.933 | Acc: 28.182% (9415/33408)\n",
      "Loss: 1.933 | Acc: 28.185% (9434/33472)\n",
      "Loss: 1.932 | Acc: 28.211% (9461/33536)\n",
      "Loss: 1.932 | Acc: 28.241% (9489/33600)\n",
      "Loss: 1.932 | Acc: 28.250% (9510/33664)\n",
      "Loss: 1.931 | Acc: 28.258% (9531/33728)\n",
      "Loss: 1.931 | Acc: 28.264% (9551/33792)\n",
      "Loss: 1.931 | Acc: 28.282% (9575/33856)\n",
      "Loss: 1.931 | Acc: 28.293% (9597/33920)\n",
      "Loss: 1.931 | Acc: 28.304% (9619/33984)\n",
      "Loss: 1.930 | Acc: 28.316% (9641/34048)\n",
      "Loss: 1.930 | Acc: 28.330% (9664/34112)\n",
      "Loss: 1.929 | Acc: 28.368% (9695/34176)\n",
      "Loss: 1.929 | Acc: 28.373% (9715/34240)\n",
      "Loss: 1.930 | Acc: 28.370% (9732/34304)\n",
      "Loss: 1.929 | Acc: 28.369% (9750/34368)\n",
      "Loss: 1.929 | Acc: 28.383% (9773/34432)\n",
      "Loss: 1.929 | Acc: 28.383% (9791/34496)\n",
      "Loss: 1.928 | Acc: 28.426% (9824/34560)\n",
      "Loss: 1.928 | Acc: 28.431% (9844/34624)\n",
      "Loss: 1.927 | Acc: 28.454% (9870/34688)\n",
      "Loss: 1.927 | Acc: 28.456% (9889/34752)\n",
      "Loss: 1.927 | Acc: 28.461% (9909/34816)\n",
      "Loss: 1.926 | Acc: 28.472% (9931/34880)\n",
      "Loss: 1.926 | Acc: 28.483% (9953/34944)\n",
      "Loss: 1.926 | Acc: 28.479% (9970/35008)\n",
      "Loss: 1.925 | Acc: 28.487% (9991/35072)\n",
      "Loss: 1.925 | Acc: 28.504% (10015/35136)\n",
      "Loss: 1.925 | Acc: 28.509% (10035/35200)\n",
      "Loss: 1.924 | Acc: 28.516% (10056/35264)\n",
      "Loss: 1.924 | Acc: 28.521% (10076/35328)\n",
      "Loss: 1.923 | Acc: 28.557% (10107/35392)\n",
      "Loss: 1.923 | Acc: 28.551% (10123/35456)\n",
      "Loss: 1.922 | Acc: 28.587% (10154/35520)\n",
      "Loss: 1.922 | Acc: 28.622% (10185/35584)\n",
      "Loss: 1.921 | Acc: 28.630% (10206/35648)\n",
      "Loss: 1.921 | Acc: 28.635% (10226/35712)\n",
      "Loss: 1.920 | Acc: 28.645% (10248/35776)\n",
      "Loss: 1.920 | Acc: 28.664% (10273/35840)\n",
      "Loss: 1.920 | Acc: 28.671% (10294/35904)\n",
      "Loss: 1.919 | Acc: 28.701% (10323/35968)\n",
      "Loss: 1.919 | Acc: 28.711% (10345/36032)\n",
      "Loss: 1.919 | Acc: 28.701% (10360/36096)\n",
      "Loss: 1.919 | Acc: 28.714% (10383/36160)\n",
      "Loss: 1.919 | Acc: 28.721% (10404/36224)\n",
      "Loss: 1.918 | Acc: 28.737% (10428/36288)\n",
      "Loss: 1.918 | Acc: 28.744% (10449/36352)\n",
      "Loss: 1.918 | Acc: 28.737% (10465/36416)\n",
      "Loss: 1.917 | Acc: 28.747% (10487/36480)\n",
      "Loss: 1.917 | Acc: 28.743% (10504/36544)\n",
      "Loss: 1.917 | Acc: 28.753% (10526/36608)\n",
      "Loss: 1.917 | Acc: 28.752% (10544/36672)\n",
      "Loss: 1.917 | Acc: 28.759% (10565/36736)\n",
      "Loss: 1.916 | Acc: 28.761% (10584/36800)\n",
      "Loss: 1.916 | Acc: 28.773% (10607/36864)\n",
      "Loss: 1.916 | Acc: 28.780% (10628/36928)\n",
      "Loss: 1.916 | Acc: 28.782% (10647/36992)\n",
      "Loss: 1.916 | Acc: 28.794% (10670/37056)\n",
      "Loss: 1.915 | Acc: 28.815% (10696/37120)\n",
      "Loss: 1.915 | Acc: 28.830% (10720/37184)\n",
      "Loss: 1.915 | Acc: 28.836% (10741/37248)\n",
      "Loss: 1.914 | Acc: 28.838% (10760/37312)\n",
      "Loss: 1.914 | Acc: 28.850% (10783/37376)\n",
      "Loss: 1.913 | Acc: 28.865% (10807/37440)\n",
      "Loss: 1.913 | Acc: 28.874% (10829/37504)\n",
      "Loss: 1.912 | Acc: 28.892% (10854/37568)\n",
      "Loss: 1.912 | Acc: 28.890% (10872/37632)\n",
      "Loss: 1.912 | Acc: 28.894% (10892/37696)\n",
      "Loss: 1.911 | Acc: 28.925% (10922/37760)\n",
      "Loss: 1.911 | Acc: 28.926% (10941/37824)\n",
      "Loss: 1.911 | Acc: 28.935% (10963/37888)\n",
      "Loss: 1.911 | Acc: 28.939% (10983/37952)\n",
      "Loss: 1.910 | Acc: 28.961% (11010/38016)\n",
      "Loss: 1.910 | Acc: 28.976% (11034/38080)\n",
      "Loss: 1.909 | Acc: 28.985% (11056/38144)\n",
      "Loss: 1.909 | Acc: 28.994% (11078/38208)\n",
      "Loss: 1.909 | Acc: 29.013% (11104/38272)\n",
      "Loss: 1.908 | Acc: 29.025% (11127/38336)\n",
      "Loss: 1.908 | Acc: 29.047% (11154/38400)\n",
      "Loss: 1.908 | Acc: 29.051% (11174/38464)\n",
      "Loss: 1.907 | Acc: 29.067% (11199/38528)\n",
      "Loss: 1.907 | Acc: 29.066% (11217/38592)\n",
      "Loss: 1.907 | Acc: 29.087% (11244/38656)\n",
      "Loss: 1.907 | Acc: 29.081% (11260/38720)\n",
      "Loss: 1.906 | Acc: 29.079% (11278/38784)\n",
      "Loss: 1.906 | Acc: 29.077% (11296/38848)\n",
      "Loss: 1.906 | Acc: 29.086% (11318/38912)\n",
      "Loss: 1.906 | Acc: 29.108% (11345/38976)\n",
      "Loss: 1.906 | Acc: 29.111% (11365/39040)\n",
      "Loss: 1.905 | Acc: 29.120% (11387/39104)\n",
      "Loss: 1.905 | Acc: 29.128% (11409/39168)\n",
      "Loss: 1.905 | Acc: 29.122% (11425/39232)\n",
      "Loss: 1.904 | Acc: 29.130% (11447/39296)\n",
      "Loss: 1.904 | Acc: 29.159% (11477/39360)\n",
      "Loss: 1.904 | Acc: 29.157% (11495/39424)\n",
      "Loss: 1.904 | Acc: 29.176% (11521/39488)\n",
      "Loss: 1.904 | Acc: 29.167% (11536/39552)\n",
      "Loss: 1.903 | Acc: 29.188% (11563/39616)\n",
      "Loss: 1.903 | Acc: 29.211% (11591/39680)\n",
      "Loss: 1.902 | Acc: 29.232% (11618/39744)\n",
      "Loss: 1.902 | Acc: 29.223% (11633/39808)\n",
      "Loss: 1.902 | Acc: 29.241% (11659/39872)\n",
      "Loss: 1.902 | Acc: 29.242% (11678/39936)\n",
      "Loss: 1.901 | Acc: 29.250% (11700/40000)\n",
      "Loss: 1.901 | Acc: 29.263% (11724/40064)\n",
      "Loss: 1.901 | Acc: 29.269% (11745/40128)\n",
      "Loss: 1.900 | Acc: 29.287% (11771/40192)\n",
      "Loss: 1.901 | Acc: 29.297% (11794/40256)\n",
      "Loss: 1.900 | Acc: 29.298% (11813/40320)\n",
      "Loss: 1.900 | Acc: 29.289% (11828/40384)\n",
      "Loss: 1.900 | Acc: 29.317% (11858/40448)\n",
      "Loss: 1.899 | Acc: 29.315% (11876/40512)\n",
      "Loss: 1.899 | Acc: 29.323% (11898/40576)\n",
      "Loss: 1.899 | Acc: 29.345% (11926/40640)\n",
      "Loss: 1.899 | Acc: 29.334% (11940/40704)\n",
      "Loss: 1.898 | Acc: 29.359% (11969/40768)\n",
      "Loss: 1.898 | Acc: 29.374% (11994/40832)\n",
      "Loss: 1.897 | Acc: 29.375% (12013/40896)\n",
      "Loss: 1.897 | Acc: 29.380% (12034/40960)\n",
      "Loss: 1.897 | Acc: 29.383% (12054/41024)\n",
      "Loss: 1.896 | Acc: 29.393% (12077/41088)\n",
      "Loss: 1.896 | Acc: 29.393% (12096/41152)\n",
      "Loss: 1.896 | Acc: 29.401% (12118/41216)\n",
      "Loss: 1.896 | Acc: 29.416% (12143/41280)\n",
      "Loss: 1.896 | Acc: 29.421% (12164/41344)\n",
      "Loss: 1.896 | Acc: 29.429% (12186/41408)\n",
      "Loss: 1.895 | Acc: 29.434% (12207/41472)\n",
      "Loss: 1.895 | Acc: 29.444% (12230/41536)\n",
      "Loss: 1.895 | Acc: 29.450% (12251/41600)\n",
      "Loss: 1.894 | Acc: 29.447% (12269/41664)\n",
      "Loss: 1.894 | Acc: 29.474% (12299/41728)\n",
      "Loss: 1.893 | Acc: 29.489% (12324/41792)\n",
      "Loss: 1.893 | Acc: 29.506% (12350/41856)\n",
      "Loss: 1.892 | Acc: 29.525% (12377/41920)\n",
      "Loss: 1.892 | Acc: 29.530% (12398/41984)\n",
      "Loss: 1.892 | Acc: 29.535% (12419/42048)\n",
      "Loss: 1.892 | Acc: 29.533% (12437/42112)\n",
      "Loss: 1.891 | Acc: 29.552% (12464/42176)\n",
      "Loss: 1.891 | Acc: 29.567% (12489/42240)\n",
      "Loss: 1.890 | Acc: 29.588% (12517/42304)\n",
      "Loss: 1.890 | Acc: 29.607% (12544/42368)\n",
      "Loss: 1.890 | Acc: 29.612% (12565/42432)\n",
      "Loss: 1.890 | Acc: 29.612% (12584/42496)\n",
      "Loss: 1.889 | Acc: 29.612% (12603/42560)\n",
      "Loss: 1.889 | Acc: 29.617% (12624/42624)\n",
      "Loss: 1.889 | Acc: 29.624% (12646/42688)\n",
      "Loss: 1.888 | Acc: 29.641% (12672/42752)\n",
      "Loss: 1.888 | Acc: 29.655% (12697/42816)\n",
      "Loss: 1.887 | Acc: 29.667% (12721/42880)\n",
      "Loss: 1.888 | Acc: 29.674% (12743/42944)\n",
      "Loss: 1.887 | Acc: 29.678% (12764/43008)\n",
      "Loss: 1.887 | Acc: 29.699% (12792/43072)\n",
      "Loss: 1.886 | Acc: 29.732% (12825/43136)\n",
      "Loss: 1.886 | Acc: 29.741% (12848/43200)\n",
      "Loss: 1.886 | Acc: 29.743% (12868/43264)\n",
      "Loss: 1.886 | Acc: 29.759% (12894/43328)\n",
      "Loss: 1.886 | Acc: 29.757% (12912/43392)\n",
      "Loss: 1.886 | Acc: 29.752% (12929/43456)\n",
      "Loss: 1.885 | Acc: 29.768% (12955/43520)\n",
      "Loss: 1.885 | Acc: 29.761% (12971/43584)\n",
      "Loss: 1.885 | Acc: 29.779% (12998/43648)\n",
      "Loss: 1.885 | Acc: 29.781% (13018/43712)\n",
      "Loss: 1.885 | Acc: 29.790% (13041/43776)\n",
      "Loss: 1.885 | Acc: 29.802% (13065/43840)\n",
      "Loss: 1.884 | Acc: 29.804% (13085/43904)\n",
      "Loss: 1.884 | Acc: 29.799% (13102/43968)\n",
      "Loss: 1.884 | Acc: 29.801% (13122/44032)\n",
      "Loss: 1.884 | Acc: 29.824% (13151/44096)\n",
      "Loss: 1.883 | Acc: 29.819% (13168/44160)\n",
      "Loss: 1.883 | Acc: 29.825% (13190/44224)\n",
      "Loss: 1.883 | Acc: 29.832% (13212/44288)\n",
      "Loss: 1.883 | Acc: 29.850% (13239/44352)\n",
      "Loss: 1.882 | Acc: 29.863% (13264/44416)\n",
      "Loss: 1.882 | Acc: 29.867% (13285/44480)\n",
      "Loss: 1.882 | Acc: 29.883% (13311/44544)\n",
      "Loss: 1.882 | Acc: 29.891% (13334/44608)\n",
      "Loss: 1.881 | Acc: 29.907% (13360/44672)\n",
      "Loss: 1.881 | Acc: 29.911% (13381/44736)\n",
      "Loss: 1.881 | Acc: 29.924% (13406/44800)\n",
      "Loss: 1.881 | Acc: 29.935% (13430/44864)\n",
      "Loss: 1.880 | Acc: 29.972% (13466/44928)\n",
      "Loss: 1.880 | Acc: 29.976% (13487/44992)\n",
      "Loss: 1.879 | Acc: 29.983% (13509/45056)\n",
      "Loss: 1.880 | Acc: 29.976% (13525/45120)\n",
      "Loss: 1.879 | Acc: 29.980% (13546/45184)\n",
      "Loss: 1.879 | Acc: 29.992% (13571/45248)\n",
      "Loss: 1.878 | Acc: 30.016% (13601/45312)\n",
      "Loss: 1.878 | Acc: 30.027% (13625/45376)\n",
      "Loss: 1.878 | Acc: 30.033% (13647/45440)\n",
      "Loss: 1.878 | Acc: 30.035% (13667/45504)\n",
      "Loss: 1.878 | Acc: 30.039% (13688/45568)\n",
      "Loss: 1.878 | Acc: 30.040% (13708/45632)\n",
      "Loss: 1.877 | Acc: 30.046% (13730/45696)\n",
      "Loss: 1.877 | Acc: 30.061% (13756/45760)\n",
      "Loss: 1.877 | Acc: 30.067% (13778/45824)\n",
      "Loss: 1.877 | Acc: 30.082% (13804/45888)\n",
      "Loss: 1.876 | Acc: 30.090% (13827/45952)\n",
      "Loss: 1.876 | Acc: 30.103% (13852/46016)\n",
      "Loss: 1.876 | Acc: 30.109% (13874/46080)\n",
      "Loss: 1.875 | Acc: 30.110% (13894/46144)\n",
      "Loss: 1.875 | Acc: 30.105% (13911/46208)\n",
      "Loss: 1.875 | Acc: 30.111% (13933/46272)\n",
      "Loss: 1.875 | Acc: 30.113% (13953/46336)\n",
      "Loss: 1.875 | Acc: 30.129% (13980/46400)\n",
      "Loss: 1.875 | Acc: 30.139% (14004/46464)\n",
      "Loss: 1.875 | Acc: 30.135% (14021/46528)\n",
      "Loss: 1.874 | Acc: 30.153% (14049/46592)\n",
      "Loss: 1.874 | Acc: 30.163% (14073/46656)\n",
      "Loss: 1.874 | Acc: 30.167% (14094/46720)\n",
      "Loss: 1.873 | Acc: 30.173% (14116/46784)\n",
      "Loss: 1.873 | Acc: 30.170% (14134/46848)\n",
      "Loss: 1.873 | Acc: 30.176% (14156/46912)\n",
      "Loss: 1.873 | Acc: 30.194% (14184/46976)\n",
      "Loss: 1.873 | Acc: 30.208% (14210/47040)\n",
      "Loss: 1.872 | Acc: 30.229% (14239/47104)\n",
      "Loss: 1.872 | Acc: 30.230% (14259/47168)\n",
      "Loss: 1.872 | Acc: 30.244% (14285/47232)\n",
      "Loss: 1.871 | Acc: 30.244% (14304/47296)\n",
      "Loss: 1.871 | Acc: 30.258% (14330/47360)\n",
      "Loss: 1.871 | Acc: 30.272% (14356/47424)\n",
      "Loss: 1.870 | Acc: 30.286% (14382/47488)\n",
      "Loss: 1.870 | Acc: 30.299% (14408/47552)\n",
      "Loss: 1.870 | Acc: 30.299% (14427/47616)\n",
      "Loss: 1.869 | Acc: 30.315% (14454/47680)\n",
      "Loss: 1.869 | Acc: 30.322% (14477/47744)\n",
      "Loss: 1.869 | Acc: 30.340% (14505/47808)\n",
      "Loss: 1.868 | Acc: 30.352% (14530/47872)\n",
      "Loss: 1.868 | Acc: 30.359% (14553/47936)\n",
      "Loss: 1.868 | Acc: 30.363% (14574/48000)\n",
      "Loss: 1.867 | Acc: 30.374% (14599/48064)\n",
      "Loss: 1.867 | Acc: 30.381% (14622/48128)\n",
      "Loss: 1.866 | Acc: 30.395% (14648/48192)\n",
      "Loss: 1.866 | Acc: 30.429% (14684/48256)\n",
      "Loss: 1.866 | Acc: 30.437% (14707/48320)\n",
      "Loss: 1.865 | Acc: 30.448% (14732/48384)\n",
      "Loss: 1.865 | Acc: 30.462% (14758/48448)\n",
      "Loss: 1.865 | Acc: 30.477% (14785/48512)\n",
      "Loss: 1.865 | Acc: 30.482% (14807/48576)\n",
      "Loss: 1.865 | Acc: 30.487% (14829/48640)\n",
      "Loss: 1.864 | Acc: 30.490% (14850/48704)\n",
      "Loss: 1.864 | Acc: 30.500% (14874/48768)\n",
      "Loss: 1.864 | Acc: 30.500% (14894/48832)\n",
      "Loss: 1.864 | Acc: 30.522% (14924/48896)\n",
      "Loss: 1.863 | Acc: 30.531% (14948/48960)\n",
      "Loss: 1.863 | Acc: 30.537% (14963/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 30.536734693877552\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.636 | Acc: 46.875% (30/64)\n",
      "Loss: 1.622 | Acc: 46.875% (60/128)\n",
      "Loss: 1.675 | Acc: 40.625% (78/192)\n",
      "Loss: 1.784 | Acc: 35.938% (92/256)\n",
      "Loss: 1.763 | Acc: 36.562% (117/320)\n",
      "Loss: 1.785 | Acc: 35.938% (138/384)\n",
      "Loss: 1.793 | Acc: 34.821% (156/448)\n",
      "Loss: 1.774 | Acc: 34.961% (179/512)\n",
      "Loss: 1.754 | Acc: 35.069% (202/576)\n",
      "Loss: 1.746 | Acc: 34.844% (223/640)\n",
      "Loss: 1.761 | Acc: 34.943% (246/704)\n",
      "Loss: 1.755 | Acc: 34.896% (268/768)\n",
      "Loss: 1.739 | Acc: 35.457% (295/832)\n",
      "Loss: 1.738 | Acc: 35.156% (315/896)\n",
      "Loss: 1.733 | Acc: 35.625% (342/960)\n",
      "Loss: 1.724 | Acc: 36.328% (372/1024)\n",
      "Loss: 1.726 | Acc: 36.121% (393/1088)\n",
      "Loss: 1.728 | Acc: 36.198% (417/1152)\n",
      "Loss: 1.729 | Acc: 36.349% (442/1216)\n",
      "Loss: 1.735 | Acc: 36.406% (466/1280)\n",
      "Loss: 1.733 | Acc: 36.086% (485/1344)\n",
      "Loss: 1.729 | Acc: 35.795% (504/1408)\n",
      "Loss: 1.719 | Acc: 36.345% (535/1472)\n",
      "Loss: 1.713 | Acc: 36.589% (562/1536)\n",
      "Loss: 1.716 | Acc: 36.312% (581/1600)\n",
      "Loss: 1.716 | Acc: 36.418% (606/1664)\n",
      "Loss: 1.713 | Acc: 36.632% (633/1728)\n",
      "Loss: 1.714 | Acc: 36.551% (655/1792)\n",
      "Loss: 1.713 | Acc: 36.530% (678/1856)\n",
      "Loss: 1.714 | Acc: 36.562% (702/1920)\n",
      "Loss: 1.712 | Acc: 36.593% (726/1984)\n",
      "Loss: 1.714 | Acc: 36.475% (747/2048)\n",
      "Loss: 1.715 | Acc: 36.222% (765/2112)\n",
      "Loss: 1.721 | Acc: 36.213% (788/2176)\n",
      "Loss: 1.720 | Acc: 36.116% (809/2240)\n",
      "Loss: 1.720 | Acc: 35.938% (828/2304)\n",
      "Loss: 1.722 | Acc: 36.106% (855/2368)\n",
      "Loss: 1.723 | Acc: 36.184% (880/2432)\n",
      "Loss: 1.721 | Acc: 36.298% (906/2496)\n",
      "Loss: 1.725 | Acc: 36.094% (924/2560)\n",
      "Loss: 1.726 | Acc: 36.166% (949/2624)\n",
      "Loss: 1.726 | Acc: 36.198% (973/2688)\n",
      "Loss: 1.727 | Acc: 36.119% (994/2752)\n",
      "Loss: 1.730 | Acc: 36.222% (1020/2816)\n",
      "Loss: 1.729 | Acc: 36.250% (1044/2880)\n",
      "Loss: 1.729 | Acc: 36.243% (1067/2944)\n",
      "Loss: 1.731 | Acc: 36.203% (1089/3008)\n",
      "Loss: 1.731 | Acc: 36.361% (1117/3072)\n",
      "Loss: 1.729 | Acc: 36.511% (1145/3136)\n",
      "Loss: 1.727 | Acc: 36.719% (1175/3200)\n",
      "Loss: 1.730 | Acc: 36.642% (1196/3264)\n",
      "Loss: 1.728 | Acc: 36.719% (1222/3328)\n",
      "Loss: 1.728 | Acc: 36.645% (1243/3392)\n",
      "Loss: 1.731 | Acc: 36.458% (1260/3456)\n",
      "Loss: 1.734 | Acc: 36.364% (1280/3520)\n",
      "Loss: 1.732 | Acc: 36.468% (1307/3584)\n",
      "Loss: 1.731 | Acc: 36.486% (1331/3648)\n",
      "Loss: 1.730 | Acc: 36.449% (1353/3712)\n",
      "Loss: 1.728 | Acc: 36.573% (1381/3776)\n",
      "Loss: 1.726 | Acc: 36.484% (1401/3840)\n",
      "Loss: 1.725 | Acc: 36.527% (1426/3904)\n",
      "Loss: 1.723 | Acc: 36.492% (1448/3968)\n",
      "Loss: 1.723 | Acc: 36.558% (1474/4032)\n",
      "Loss: 1.724 | Acc: 36.523% (1496/4096)\n",
      "Loss: 1.730 | Acc: 36.442% (1516/4160)\n",
      "Loss: 1.729 | Acc: 36.411% (1538/4224)\n",
      "Loss: 1.730 | Acc: 36.404% (1561/4288)\n",
      "Loss: 1.729 | Acc: 36.512% (1589/4352)\n",
      "Loss: 1.727 | Acc: 36.617% (1617/4416)\n",
      "Loss: 1.724 | Acc: 36.674% (1643/4480)\n",
      "Loss: 1.725 | Acc: 36.642% (1665/4544)\n",
      "Loss: 1.728 | Acc: 36.632% (1688/4608)\n",
      "Loss: 1.726 | Acc: 36.751% (1717/4672)\n",
      "Loss: 1.723 | Acc: 36.824% (1744/4736)\n",
      "Loss: 1.725 | Acc: 36.833% (1768/4800)\n",
      "Loss: 1.723 | Acc: 36.822% (1791/4864)\n",
      "Loss: 1.724 | Acc: 36.790% (1813/4928)\n",
      "Loss: 1.724 | Acc: 36.819% (1838/4992)\n",
      "Loss: 1.725 | Acc: 36.808% (1861/5056)\n",
      "Loss: 1.727 | Acc: 36.738% (1881/5120)\n",
      "Loss: 1.728 | Acc: 36.709% (1903/5184)\n",
      "Loss: 1.729 | Acc: 36.681% (1925/5248)\n",
      "Loss: 1.728 | Acc: 36.596% (1944/5312)\n",
      "Loss: 1.730 | Acc: 36.607% (1968/5376)\n",
      "Loss: 1.729 | Acc: 36.544% (1988/5440)\n",
      "Loss: 1.729 | Acc: 36.573% (2013/5504)\n",
      "Loss: 1.730 | Acc: 36.458% (2030/5568)\n",
      "Loss: 1.733 | Acc: 36.364% (2048/5632)\n",
      "Loss: 1.733 | Acc: 36.359% (2071/5696)\n",
      "Loss: 1.731 | Acc: 36.424% (2098/5760)\n",
      "Loss: 1.730 | Acc: 36.470% (2124/5824)\n",
      "Loss: 1.731 | Acc: 36.345% (2140/5888)\n",
      "Loss: 1.732 | Acc: 36.358% (2164/5952)\n",
      "Loss: 1.734 | Acc: 36.287% (2183/6016)\n",
      "Loss: 1.735 | Acc: 36.168% (2199/6080)\n",
      "Loss: 1.736 | Acc: 36.133% (2220/6144)\n",
      "Loss: 1.736 | Acc: 36.115% (2242/6208)\n",
      "Loss: 1.737 | Acc: 36.113% (2265/6272)\n",
      "Loss: 1.736 | Acc: 36.095% (2287/6336)\n",
      "Loss: 1.736 | Acc: 36.078% (2309/6400)\n",
      "Loss: 1.738 | Acc: 35.953% (2324/6464)\n",
      "Loss: 1.738 | Acc: 35.953% (2347/6528)\n",
      "Loss: 1.741 | Acc: 35.846% (2363/6592)\n",
      "Loss: 1.741 | Acc: 35.817% (2384/6656)\n",
      "Loss: 1.741 | Acc: 35.744% (2402/6720)\n",
      "Loss: 1.741 | Acc: 35.731% (2424/6784)\n",
      "Loss: 1.739 | Acc: 35.762% (2449/6848)\n",
      "Loss: 1.741 | Acc: 35.720% (2469/6912)\n",
      "Loss: 1.743 | Acc: 35.636% (2486/6976)\n",
      "Loss: 1.744 | Acc: 35.597% (2506/7040)\n",
      "Loss: 1.744 | Acc: 35.572% (2527/7104)\n",
      "Loss: 1.744 | Acc: 35.533% (2547/7168)\n",
      "Loss: 1.745 | Acc: 35.523% (2569/7232)\n",
      "Loss: 1.744 | Acc: 35.458% (2587/7296)\n",
      "Loss: 1.742 | Acc: 35.516% (2614/7360)\n",
      "Loss: 1.742 | Acc: 35.520% (2637/7424)\n",
      "Loss: 1.741 | Acc: 35.550% (2662/7488)\n",
      "Loss: 1.740 | Acc: 35.461% (2678/7552)\n",
      "Loss: 1.740 | Acc: 35.452% (2700/7616)\n",
      "Loss: 1.739 | Acc: 35.469% (2724/7680)\n",
      "Loss: 1.738 | Acc: 35.460% (2746/7744)\n",
      "Loss: 1.738 | Acc: 35.412% (2765/7808)\n",
      "Loss: 1.739 | Acc: 35.379% (2785/7872)\n",
      "Loss: 1.739 | Acc: 35.358% (2806/7936)\n",
      "Loss: 1.740 | Acc: 35.288% (2823/8000)\n",
      "Loss: 1.740 | Acc: 35.342% (2850/8064)\n",
      "Loss: 1.740 | Acc: 35.335% (2872/8128)\n",
      "Loss: 1.739 | Acc: 35.413% (2901/8192)\n",
      "Loss: 1.739 | Acc: 35.429% (2925/8256)\n",
      "Loss: 1.740 | Acc: 35.361% (2942/8320)\n",
      "Loss: 1.739 | Acc: 35.365% (2965/8384)\n",
      "Loss: 1.738 | Acc: 35.405% (2991/8448)\n",
      "Loss: 1.740 | Acc: 35.338% (3008/8512)\n",
      "Loss: 1.740 | Acc: 35.389% (3035/8576)\n",
      "Loss: 1.738 | Acc: 35.475% (3065/8640)\n",
      "Loss: 1.738 | Acc: 35.466% (3087/8704)\n",
      "Loss: 1.738 | Acc: 35.493% (3112/8768)\n",
      "Loss: 1.737 | Acc: 35.575% (3142/8832)\n",
      "Loss: 1.737 | Acc: 35.589% (3166/8896)\n",
      "Loss: 1.736 | Acc: 35.614% (3191/8960)\n",
      "Loss: 1.736 | Acc: 35.627% (3215/9024)\n",
      "Loss: 1.737 | Acc: 35.519% (3228/9088)\n",
      "Loss: 1.738 | Acc: 35.500% (3249/9152)\n",
      "Loss: 1.736 | Acc: 35.547% (3276/9216)\n",
      "Loss: 1.736 | Acc: 35.571% (3301/9280)\n",
      "Loss: 1.736 | Acc: 35.616% (3328/9344)\n",
      "Loss: 1.736 | Acc: 35.576% (3347/9408)\n",
      "Loss: 1.736 | Acc: 35.536% (3366/9472)\n",
      "Loss: 1.737 | Acc: 35.529% (3388/9536)\n",
      "Loss: 1.737 | Acc: 35.490% (3407/9600)\n",
      "Loss: 1.736 | Acc: 35.493% (3430/9664)\n",
      "Loss: 1.736 | Acc: 35.475% (3451/9728)\n",
      "Loss: 1.737 | Acc: 35.437% (3470/9792)\n",
      "Loss: 1.737 | Acc: 35.410% (3490/9856)\n",
      "Loss: 1.738 | Acc: 35.393% (3511/9920)\n",
      "Loss: 1.738 | Acc: 35.357% (3530/9984)\n",
      "Loss: 1.737 | Acc: 35.380% (3538/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 35.38\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.771 | Acc: 34.375% (22/64)\n",
      "Loss: 1.806 | Acc: 36.719% (47/128)\n",
      "Loss: 1.765 | Acc: 35.938% (69/192)\n",
      "Loss: 1.763 | Acc: 36.328% (93/256)\n",
      "Loss: 1.734 | Acc: 37.812% (121/320)\n",
      "Loss: 1.689 | Acc: 38.802% (149/384)\n",
      "Loss: 1.698 | Acc: 37.723% (169/448)\n",
      "Loss: 1.721 | Acc: 36.914% (189/512)\n",
      "Loss: 1.689 | Acc: 37.500% (216/576)\n",
      "Loss: 1.680 | Acc: 37.969% (243/640)\n",
      "Loss: 1.695 | Acc: 37.074% (261/704)\n",
      "Loss: 1.687 | Acc: 37.500% (288/768)\n",
      "Loss: 1.680 | Acc: 37.861% (315/832)\n",
      "Loss: 1.668 | Acc: 38.393% (344/896)\n",
      "Loss: 1.672 | Acc: 38.229% (367/960)\n",
      "Loss: 1.662 | Acc: 38.770% (397/1024)\n",
      "Loss: 1.653 | Acc: 39.062% (425/1088)\n",
      "Loss: 1.646 | Acc: 39.583% (456/1152)\n",
      "Loss: 1.650 | Acc: 39.309% (478/1216)\n",
      "Loss: 1.644 | Acc: 39.453% (505/1280)\n",
      "Loss: 1.644 | Acc: 39.509% (531/1344)\n",
      "Loss: 1.645 | Acc: 39.631% (558/1408)\n",
      "Loss: 1.644 | Acc: 39.402% (580/1472)\n",
      "Loss: 1.651 | Acc: 39.193% (602/1536)\n",
      "Loss: 1.651 | Acc: 38.875% (622/1600)\n",
      "Loss: 1.649 | Acc: 38.822% (646/1664)\n",
      "Loss: 1.658 | Acc: 38.831% (671/1728)\n",
      "Loss: 1.657 | Acc: 38.337% (687/1792)\n",
      "Loss: 1.656 | Acc: 38.039% (706/1856)\n",
      "Loss: 1.658 | Acc: 37.865% (727/1920)\n",
      "Loss: 1.659 | Acc: 38.054% (755/1984)\n",
      "Loss: 1.663 | Acc: 37.695% (772/2048)\n",
      "Loss: 1.674 | Acc: 37.453% (791/2112)\n",
      "Loss: 1.677 | Acc: 37.362% (813/2176)\n",
      "Loss: 1.683 | Acc: 37.143% (832/2240)\n",
      "Loss: 1.682 | Acc: 37.066% (854/2304)\n",
      "Loss: 1.684 | Acc: 37.035% (877/2368)\n",
      "Loss: 1.680 | Acc: 37.212% (905/2432)\n",
      "Loss: 1.681 | Acc: 37.019% (924/2496)\n",
      "Loss: 1.683 | Acc: 36.875% (944/2560)\n",
      "Loss: 1.687 | Acc: 36.852% (967/2624)\n",
      "Loss: 1.685 | Acc: 36.868% (991/2688)\n",
      "Loss: 1.685 | Acc: 36.882% (1015/2752)\n",
      "Loss: 1.688 | Acc: 36.719% (1034/2816)\n",
      "Loss: 1.685 | Acc: 36.910% (1063/2880)\n",
      "Loss: 1.684 | Acc: 37.024% (1090/2944)\n",
      "Loss: 1.691 | Acc: 36.769% (1106/3008)\n",
      "Loss: 1.689 | Acc: 36.849% (1132/3072)\n",
      "Loss: 1.684 | Acc: 37.022% (1161/3136)\n",
      "Loss: 1.683 | Acc: 36.938% (1182/3200)\n",
      "Loss: 1.689 | Acc: 36.765% (1200/3264)\n",
      "Loss: 1.690 | Acc: 36.899% (1228/3328)\n",
      "Loss: 1.694 | Acc: 36.733% (1246/3392)\n",
      "Loss: 1.694 | Acc: 36.834% (1273/3456)\n",
      "Loss: 1.692 | Acc: 36.932% (1300/3520)\n",
      "Loss: 1.691 | Acc: 36.998% (1326/3584)\n",
      "Loss: 1.686 | Acc: 37.171% (1356/3648)\n",
      "Loss: 1.688 | Acc: 37.123% (1378/3712)\n",
      "Loss: 1.689 | Acc: 37.103% (1401/3776)\n",
      "Loss: 1.687 | Acc: 37.188% (1428/3840)\n",
      "Loss: 1.689 | Acc: 37.116% (1449/3904)\n",
      "Loss: 1.690 | Acc: 36.996% (1468/3968)\n",
      "Loss: 1.686 | Acc: 37.227% (1501/4032)\n",
      "Loss: 1.686 | Acc: 37.207% (1524/4096)\n",
      "Loss: 1.686 | Acc: 37.212% (1548/4160)\n",
      "Loss: 1.687 | Acc: 37.216% (1572/4224)\n",
      "Loss: 1.685 | Acc: 37.220% (1596/4288)\n",
      "Loss: 1.685 | Acc: 37.316% (1624/4352)\n",
      "Loss: 1.682 | Acc: 37.409% (1652/4416)\n",
      "Loss: 1.681 | Acc: 37.522% (1681/4480)\n",
      "Loss: 1.682 | Acc: 37.566% (1707/4544)\n",
      "Loss: 1.683 | Acc: 37.522% (1729/4608)\n",
      "Loss: 1.683 | Acc: 37.457% (1750/4672)\n",
      "Loss: 1.681 | Acc: 37.542% (1778/4736)\n",
      "Loss: 1.683 | Acc: 37.375% (1794/4800)\n",
      "Loss: 1.682 | Acc: 37.377% (1818/4864)\n",
      "Loss: 1.683 | Acc: 37.520% (1849/4928)\n",
      "Loss: 1.681 | Acc: 37.520% (1873/4992)\n",
      "Loss: 1.681 | Acc: 37.480% (1895/5056)\n",
      "Loss: 1.681 | Acc: 37.480% (1919/5120)\n",
      "Loss: 1.679 | Acc: 37.635% (1951/5184)\n",
      "Loss: 1.678 | Acc: 37.710% (1979/5248)\n",
      "Loss: 1.678 | Acc: 37.839% (2010/5312)\n",
      "Loss: 1.677 | Acc: 37.816% (2033/5376)\n",
      "Loss: 1.676 | Acc: 37.868% (2060/5440)\n",
      "Loss: 1.677 | Acc: 37.736% (2077/5504)\n",
      "Loss: 1.677 | Acc: 37.841% (2107/5568)\n",
      "Loss: 1.677 | Acc: 37.784% (2128/5632)\n",
      "Loss: 1.677 | Acc: 37.851% (2156/5696)\n",
      "Loss: 1.678 | Acc: 37.865% (2181/5760)\n",
      "Loss: 1.679 | Acc: 37.861% (2205/5824)\n",
      "Loss: 1.680 | Acc: 37.857% (2229/5888)\n",
      "Loss: 1.678 | Acc: 37.954% (2259/5952)\n",
      "Loss: 1.678 | Acc: 37.916% (2281/6016)\n",
      "Loss: 1.679 | Acc: 37.829% (2300/6080)\n",
      "Loss: 1.679 | Acc: 37.793% (2322/6144)\n",
      "Loss: 1.681 | Acc: 37.774% (2345/6208)\n",
      "Loss: 1.683 | Acc: 37.628% (2360/6272)\n",
      "Loss: 1.685 | Acc: 37.547% (2379/6336)\n",
      "Loss: 1.688 | Acc: 37.516% (2401/6400)\n",
      "Loss: 1.689 | Acc: 37.423% (2419/6464)\n",
      "Loss: 1.690 | Acc: 37.377% (2440/6528)\n",
      "Loss: 1.693 | Acc: 37.288% (2458/6592)\n",
      "Loss: 1.691 | Acc: 37.320% (2484/6656)\n",
      "Loss: 1.689 | Acc: 37.366% (2511/6720)\n",
      "Loss: 1.690 | Acc: 37.294% (2530/6784)\n",
      "Loss: 1.690 | Acc: 37.237% (2550/6848)\n",
      "Loss: 1.689 | Acc: 37.283% (2577/6912)\n",
      "Loss: 1.688 | Acc: 37.328% (2604/6976)\n",
      "Loss: 1.687 | Acc: 37.372% (2631/7040)\n",
      "Loss: 1.686 | Acc: 37.416% (2658/7104)\n",
      "Loss: 1.684 | Acc: 37.500% (2688/7168)\n",
      "Loss: 1.684 | Acc: 37.459% (2709/7232)\n",
      "Loss: 1.683 | Acc: 37.541% (2739/7296)\n",
      "Loss: 1.683 | Acc: 37.595% (2767/7360)\n",
      "Loss: 1.685 | Acc: 37.513% (2785/7424)\n",
      "Loss: 1.684 | Acc: 37.527% (2810/7488)\n",
      "Loss: 1.684 | Acc: 37.474% (2830/7552)\n",
      "Loss: 1.684 | Acc: 37.447% (2852/7616)\n",
      "Loss: 1.685 | Acc: 37.461% (2877/7680)\n",
      "Loss: 1.685 | Acc: 37.487% (2903/7744)\n",
      "Loss: 1.685 | Acc: 37.449% (2924/7808)\n",
      "Loss: 1.685 | Acc: 37.360% (2941/7872)\n",
      "Loss: 1.685 | Acc: 37.286% (2959/7936)\n",
      "Loss: 1.687 | Acc: 37.212% (2977/8000)\n",
      "Loss: 1.687 | Acc: 37.178% (2998/8064)\n",
      "Loss: 1.687 | Acc: 37.180% (3022/8128)\n",
      "Loss: 1.686 | Acc: 37.195% (3047/8192)\n",
      "Loss: 1.686 | Acc: 37.197% (3071/8256)\n",
      "Loss: 1.686 | Acc: 37.236% (3098/8320)\n",
      "Loss: 1.686 | Acc: 37.214% (3120/8384)\n",
      "Loss: 1.686 | Acc: 37.180% (3141/8448)\n",
      "Loss: 1.687 | Acc: 37.089% (3157/8512)\n",
      "Loss: 1.686 | Acc: 37.127% (3184/8576)\n",
      "Loss: 1.686 | Acc: 37.153% (3210/8640)\n",
      "Loss: 1.687 | Acc: 37.144% (3233/8704)\n",
      "Loss: 1.687 | Acc: 37.146% (3257/8768)\n",
      "Loss: 1.687 | Acc: 37.149% (3281/8832)\n",
      "Loss: 1.686 | Acc: 37.107% (3301/8896)\n",
      "Loss: 1.687 | Acc: 37.132% (3327/8960)\n",
      "Loss: 1.687 | Acc: 37.112% (3349/9024)\n",
      "Loss: 1.688 | Acc: 37.082% (3370/9088)\n",
      "Loss: 1.687 | Acc: 37.118% (3397/9152)\n",
      "Loss: 1.687 | Acc: 37.088% (3418/9216)\n",
      "Loss: 1.686 | Acc: 37.112% (3444/9280)\n",
      "Loss: 1.687 | Acc: 37.093% (3466/9344)\n",
      "Loss: 1.687 | Acc: 37.064% (3487/9408)\n",
      "Loss: 1.688 | Acc: 37.057% (3510/9472)\n",
      "Loss: 1.689 | Acc: 37.049% (3533/9536)\n",
      "Loss: 1.687 | Acc: 37.062% (3558/9600)\n",
      "Loss: 1.687 | Acc: 37.014% (3577/9664)\n",
      "Loss: 1.686 | Acc: 37.007% (3600/9728)\n",
      "Loss: 1.687 | Acc: 36.979% (3621/9792)\n",
      "Loss: 1.687 | Acc: 36.942% (3641/9856)\n",
      "Loss: 1.686 | Acc: 37.026% (3673/9920)\n",
      "Loss: 1.685 | Acc: 37.029% (3697/9984)\n",
      "Loss: 1.684 | Acc: 37.032% (3721/10048)\n",
      "Loss: 1.685 | Acc: 37.075% (3749/10112)\n",
      "Loss: 1.685 | Acc: 37.048% (3770/10176)\n",
      "Loss: 1.685 | Acc: 37.041% (3793/10240)\n",
      "Loss: 1.684 | Acc: 37.054% (3818/10304)\n",
      "Loss: 1.685 | Acc: 37.066% (3843/10368)\n",
      "Loss: 1.684 | Acc: 37.088% (3869/10432)\n",
      "Loss: 1.683 | Acc: 37.128% (3897/10496)\n",
      "Loss: 1.682 | Acc: 37.150% (3923/10560)\n",
      "Loss: 1.681 | Acc: 37.171% (3949/10624)\n",
      "Loss: 1.681 | Acc: 37.266% (3983/10688)\n",
      "Loss: 1.680 | Acc: 37.267% (4007/10752)\n",
      "Loss: 1.679 | Acc: 37.287% (4033/10816)\n",
      "Loss: 1.679 | Acc: 37.243% (4052/10880)\n",
      "Loss: 1.678 | Acc: 37.281% (4080/10944)\n",
      "Loss: 1.677 | Acc: 37.373% (4114/11008)\n",
      "Loss: 1.677 | Acc: 37.410% (4142/11072)\n",
      "Loss: 1.677 | Acc: 37.383% (4163/11136)\n",
      "Loss: 1.675 | Acc: 37.482% (4198/11200)\n",
      "Loss: 1.675 | Acc: 37.464% (4220/11264)\n",
      "Loss: 1.676 | Acc: 37.465% (4244/11328)\n",
      "Loss: 1.677 | Acc: 37.447% (4266/11392)\n",
      "Loss: 1.676 | Acc: 37.500% (4296/11456)\n",
      "Loss: 1.678 | Acc: 37.465% (4316/11520)\n",
      "Loss: 1.678 | Acc: 37.465% (4340/11584)\n",
      "Loss: 1.678 | Acc: 37.406% (4357/11648)\n",
      "Loss: 1.678 | Acc: 37.406% (4381/11712)\n",
      "Loss: 1.678 | Acc: 37.449% (4410/11776)\n",
      "Loss: 1.679 | Acc: 37.432% (4432/11840)\n",
      "Loss: 1.678 | Acc: 37.458% (4459/11904)\n",
      "Loss: 1.676 | Acc: 37.550% (4494/11968)\n",
      "Loss: 1.677 | Acc: 37.500% (4512/12032)\n",
      "Loss: 1.676 | Acc: 37.508% (4537/12096)\n",
      "Loss: 1.676 | Acc: 37.525% (4563/12160)\n",
      "Loss: 1.675 | Acc: 37.541% (4589/12224)\n",
      "Loss: 1.673 | Acc: 37.606% (4621/12288)\n",
      "Loss: 1.673 | Acc: 37.686% (4655/12352)\n",
      "Loss: 1.672 | Acc: 37.717% (4683/12416)\n",
      "Loss: 1.672 | Acc: 37.732% (4709/12480)\n",
      "Loss: 1.670 | Acc: 37.803% (4742/12544)\n",
      "Loss: 1.670 | Acc: 37.817% (4768/12608)\n",
      "Loss: 1.670 | Acc: 37.800% (4790/12672)\n",
      "Loss: 1.670 | Acc: 37.814% (4816/12736)\n",
      "Loss: 1.670 | Acc: 37.852% (4845/12800)\n",
      "Loss: 1.670 | Acc: 37.858% (4870/12864)\n",
      "Loss: 1.671 | Acc: 37.871% (4896/12928)\n",
      "Loss: 1.670 | Acc: 37.869% (4920/12992)\n",
      "Loss: 1.669 | Acc: 37.898% (4948/13056)\n",
      "Loss: 1.669 | Acc: 37.942% (4978/13120)\n",
      "Loss: 1.668 | Acc: 37.955% (5004/13184)\n",
      "Loss: 1.669 | Acc: 37.945% (5027/13248)\n",
      "Loss: 1.670 | Acc: 37.913% (5047/13312)\n",
      "Loss: 1.671 | Acc: 37.926% (5073/13376)\n",
      "Loss: 1.671 | Acc: 37.924% (5097/13440)\n",
      "Loss: 1.671 | Acc: 37.900% (5118/13504)\n",
      "Loss: 1.671 | Acc: 37.876% (5139/13568)\n",
      "Loss: 1.671 | Acc: 37.837% (5158/13632)\n",
      "Loss: 1.672 | Acc: 37.814% (5179/13696)\n",
      "Loss: 1.670 | Acc: 37.842% (5207/13760)\n",
      "Loss: 1.670 | Acc: 37.883% (5237/13824)\n",
      "Loss: 1.669 | Acc: 37.946% (5270/13888)\n",
      "Loss: 1.669 | Acc: 37.930% (5292/13952)\n",
      "Loss: 1.669 | Acc: 37.921% (5315/14016)\n",
      "Loss: 1.667 | Acc: 37.955% (5344/14080)\n",
      "Loss: 1.668 | Acc: 37.945% (5367/14144)\n",
      "Loss: 1.668 | Acc: 37.936% (5390/14208)\n",
      "Loss: 1.668 | Acc: 37.920% (5412/14272)\n",
      "Loss: 1.669 | Acc: 37.898% (5433/14336)\n",
      "Loss: 1.669 | Acc: 37.868% (5453/14400)\n",
      "Loss: 1.669 | Acc: 37.873% (5478/14464)\n",
      "Loss: 1.668 | Acc: 37.906% (5507/14528)\n",
      "Loss: 1.668 | Acc: 37.925% (5534/14592)\n",
      "Loss: 1.668 | Acc: 37.882% (5552/14656)\n",
      "Loss: 1.667 | Acc: 37.908% (5580/14720)\n",
      "Loss: 1.666 | Acc: 37.940% (5609/14784)\n",
      "Loss: 1.667 | Acc: 37.924% (5631/14848)\n",
      "Loss: 1.666 | Acc: 37.956% (5660/14912)\n",
      "Loss: 1.666 | Acc: 37.941% (5682/14976)\n",
      "Loss: 1.665 | Acc: 37.972% (5711/15040)\n",
      "Loss: 1.666 | Acc: 37.997% (5739/15104)\n",
      "Loss: 1.665 | Acc: 38.021% (5767/15168)\n",
      "Loss: 1.664 | Acc: 38.065% (5798/15232)\n",
      "Loss: 1.663 | Acc: 38.121% (5831/15296)\n",
      "Loss: 1.663 | Acc: 38.138% (5858/15360)\n",
      "Loss: 1.663 | Acc: 38.155% (5885/15424)\n",
      "Loss: 1.663 | Acc: 38.165% (5911/15488)\n",
      "Loss: 1.662 | Acc: 38.175% (5937/15552)\n",
      "Loss: 1.662 | Acc: 38.198% (5965/15616)\n",
      "Loss: 1.662 | Acc: 38.176% (5986/15680)\n",
      "Loss: 1.661 | Acc: 38.224% (6018/15744)\n",
      "Loss: 1.660 | Acc: 38.234% (6044/15808)\n",
      "Loss: 1.660 | Acc: 38.256% (6072/15872)\n",
      "Loss: 1.659 | Acc: 38.291% (6102/15936)\n",
      "Loss: 1.659 | Acc: 38.344% (6135/16000)\n",
      "Loss: 1.658 | Acc: 38.353% (6161/16064)\n",
      "Loss: 1.658 | Acc: 38.380% (6190/16128)\n",
      "Loss: 1.658 | Acc: 38.377% (6214/16192)\n",
      "Loss: 1.658 | Acc: 38.398% (6242/16256)\n",
      "Loss: 1.658 | Acc: 38.401% (6267/16320)\n",
      "Loss: 1.658 | Acc: 38.409% (6293/16384)\n",
      "Loss: 1.658 | Acc: 38.418% (6319/16448)\n",
      "Loss: 1.657 | Acc: 38.408% (6342/16512)\n",
      "Loss: 1.657 | Acc: 38.399% (6365/16576)\n",
      "Loss: 1.657 | Acc: 38.353% (6382/16640)\n",
      "Loss: 1.656 | Acc: 38.386% (6412/16704)\n",
      "Loss: 1.657 | Acc: 38.383% (6436/16768)\n",
      "Loss: 1.656 | Acc: 38.397% (6463/16832)\n",
      "Loss: 1.656 | Acc: 38.411% (6490/16896)\n",
      "Loss: 1.656 | Acc: 38.414% (6515/16960)\n",
      "Loss: 1.656 | Acc: 38.434% (6543/17024)\n",
      "Loss: 1.655 | Acc: 38.454% (6571/17088)\n",
      "Loss: 1.656 | Acc: 38.444% (6594/17152)\n",
      "Loss: 1.655 | Acc: 38.458% (6621/17216)\n",
      "Loss: 1.655 | Acc: 38.449% (6644/17280)\n",
      "Loss: 1.656 | Acc: 38.423% (6664/17344)\n",
      "Loss: 1.655 | Acc: 38.448% (6693/17408)\n",
      "Loss: 1.656 | Acc: 38.439% (6716/17472)\n",
      "Loss: 1.656 | Acc: 38.435% (6740/17536)\n",
      "Loss: 1.655 | Acc: 38.489% (6774/17600)\n",
      "Loss: 1.654 | Acc: 38.525% (6805/17664)\n",
      "Loss: 1.654 | Acc: 38.527% (6830/17728)\n",
      "Loss: 1.654 | Acc: 38.517% (6853/17792)\n",
      "Loss: 1.653 | Acc: 38.536% (6881/17856)\n",
      "Loss: 1.654 | Acc: 38.516% (6902/17920)\n",
      "Loss: 1.653 | Acc: 38.545% (6932/17984)\n",
      "Loss: 1.653 | Acc: 38.553% (6958/18048)\n",
      "Loss: 1.652 | Acc: 38.610% (6993/18112)\n",
      "Loss: 1.651 | Acc: 38.650% (7025/18176)\n",
      "Loss: 1.651 | Acc: 38.657% (7051/18240)\n",
      "Loss: 1.651 | Acc: 38.669% (7078/18304)\n",
      "Loss: 1.651 | Acc: 38.703% (7109/18368)\n",
      "Loss: 1.651 | Acc: 38.656% (7125/18432)\n",
      "Loss: 1.651 | Acc: 38.668% (7152/18496)\n",
      "Loss: 1.651 | Acc: 38.648% (7173/18560)\n",
      "Loss: 1.651 | Acc: 38.622% (7193/18624)\n",
      "Loss: 1.651 | Acc: 38.667% (7226/18688)\n",
      "Loss: 1.651 | Acc: 38.679% (7253/18752)\n",
      "Loss: 1.651 | Acc: 38.669% (7276/18816)\n",
      "Loss: 1.651 | Acc: 38.681% (7303/18880)\n",
      "Loss: 1.651 | Acc: 38.698% (7331/18944)\n",
      "Loss: 1.651 | Acc: 38.694% (7355/19008)\n",
      "Loss: 1.651 | Acc: 38.695% (7380/19072)\n",
      "Loss: 1.650 | Acc: 38.728% (7411/19136)\n",
      "Loss: 1.650 | Acc: 38.719% (7434/19200)\n",
      "Loss: 1.650 | Acc: 38.715% (7458/19264)\n",
      "Loss: 1.649 | Acc: 38.747% (7489/19328)\n",
      "Loss: 1.649 | Acc: 38.732% (7511/19392)\n",
      "Loss: 1.649 | Acc: 38.744% (7538/19456)\n",
      "Loss: 1.649 | Acc: 38.745% (7563/19520)\n",
      "Loss: 1.649 | Acc: 38.771% (7593/19584)\n",
      "Loss: 1.650 | Acc: 38.762% (7616/19648)\n",
      "Loss: 1.650 | Acc: 38.743% (7637/19712)\n",
      "Loss: 1.649 | Acc: 38.774% (7668/19776)\n",
      "Loss: 1.649 | Acc: 38.790% (7696/19840)\n",
      "Loss: 1.649 | Acc: 38.816% (7726/19904)\n",
      "Loss: 1.648 | Acc: 38.832% (7754/19968)\n",
      "Loss: 1.647 | Acc: 38.868% (7786/20032)\n",
      "Loss: 1.646 | Acc: 38.928% (7823/20096)\n",
      "Loss: 1.645 | Acc: 38.983% (7859/20160)\n",
      "Loss: 1.646 | Acc: 38.973% (7882/20224)\n",
      "Loss: 1.646 | Acc: 38.959% (7904/20288)\n",
      "Loss: 1.646 | Acc: 38.969% (7931/20352)\n",
      "Loss: 1.646 | Acc: 38.965% (7955/20416)\n",
      "Loss: 1.646 | Acc: 38.984% (7984/20480)\n",
      "Loss: 1.645 | Acc: 38.985% (8009/20544)\n",
      "Loss: 1.645 | Acc: 38.990% (8035/20608)\n",
      "Loss: 1.645 | Acc: 38.985% (8059/20672)\n",
      "Loss: 1.645 | Acc: 39.034% (8094/20736)\n",
      "Loss: 1.645 | Acc: 39.019% (8116/20800)\n",
      "Loss: 1.646 | Acc: 38.986% (8134/20864)\n",
      "Loss: 1.646 | Acc: 38.991% (8160/20928)\n",
      "Loss: 1.646 | Acc: 38.996% (8186/20992)\n",
      "Loss: 1.646 | Acc: 39.010% (8214/21056)\n",
      "Loss: 1.645 | Acc: 39.006% (8238/21120)\n",
      "Loss: 1.645 | Acc: 39.001% (8262/21184)\n",
      "Loss: 1.644 | Acc: 39.001% (8287/21248)\n",
      "Loss: 1.644 | Acc: 39.002% (8312/21312)\n",
      "Loss: 1.645 | Acc: 39.002% (8337/21376)\n",
      "Loss: 1.645 | Acc: 39.002% (8362/21440)\n",
      "Loss: 1.645 | Acc: 38.983% (8383/21504)\n",
      "Loss: 1.645 | Acc: 38.979% (8407/21568)\n",
      "Loss: 1.645 | Acc: 38.970% (8430/21632)\n",
      "Loss: 1.645 | Acc: 38.956% (8452/21696)\n",
      "Loss: 1.645 | Acc: 38.980% (8482/21760)\n",
      "Loss: 1.645 | Acc: 38.971% (8505/21824)\n",
      "Loss: 1.645 | Acc: 39.017% (8540/21888)\n",
      "Loss: 1.645 | Acc: 39.008% (8563/21952)\n",
      "Loss: 1.644 | Acc: 39.035% (8594/22016)\n",
      "Loss: 1.644 | Acc: 39.040% (8620/22080)\n",
      "Loss: 1.645 | Acc: 39.026% (8642/22144)\n",
      "Loss: 1.645 | Acc: 39.044% (8671/22208)\n",
      "Loss: 1.645 | Acc: 39.049% (8697/22272)\n",
      "Loss: 1.645 | Acc: 39.040% (8720/22336)\n",
      "Loss: 1.645 | Acc: 39.058% (8749/22400)\n",
      "Loss: 1.644 | Acc: 39.067% (8776/22464)\n",
      "Loss: 1.644 | Acc: 39.071% (8802/22528)\n",
      "Loss: 1.644 | Acc: 39.062% (8825/22592)\n",
      "Loss: 1.644 | Acc: 39.045% (8846/22656)\n",
      "Loss: 1.644 | Acc: 39.062% (8875/22720)\n",
      "Loss: 1.644 | Acc: 39.062% (8900/22784)\n",
      "Loss: 1.643 | Acc: 39.076% (8928/22848)\n",
      "Loss: 1.643 | Acc: 39.067% (8951/22912)\n",
      "Loss: 1.644 | Acc: 39.071% (8977/22976)\n",
      "Loss: 1.644 | Acc: 39.041% (8995/23040)\n",
      "Loss: 1.644 | Acc: 39.050% (9022/23104)\n",
      "Loss: 1.643 | Acc: 39.080% (9054/23168)\n",
      "Loss: 1.643 | Acc: 39.084% (9080/23232)\n",
      "Loss: 1.643 | Acc: 39.084% (9105/23296)\n",
      "Loss: 1.643 | Acc: 39.084% (9130/23360)\n",
      "Loss: 1.643 | Acc: 39.075% (9153/23424)\n",
      "Loss: 1.642 | Acc: 39.109% (9186/23488)\n",
      "Loss: 1.642 | Acc: 39.101% (9209/23552)\n",
      "Loss: 1.641 | Acc: 39.101% (9234/23616)\n",
      "Loss: 1.641 | Acc: 39.113% (9262/23680)\n",
      "Loss: 1.641 | Acc: 39.117% (9288/23744)\n",
      "Loss: 1.641 | Acc: 39.134% (9317/23808)\n",
      "Loss: 1.641 | Acc: 39.113% (9337/23872)\n",
      "Loss: 1.641 | Acc: 39.113% (9362/23936)\n",
      "Loss: 1.641 | Acc: 39.108% (9386/24000)\n",
      "Loss: 1.641 | Acc: 39.083% (9405/24064)\n",
      "Loss: 1.642 | Acc: 39.067% (9426/24128)\n",
      "Loss: 1.642 | Acc: 39.067% (9451/24192)\n",
      "Loss: 1.642 | Acc: 39.091% (9482/24256)\n",
      "Loss: 1.642 | Acc: 39.124% (9515/24320)\n",
      "Loss: 1.641 | Acc: 39.124% (9540/24384)\n",
      "Loss: 1.641 | Acc: 39.112% (9562/24448)\n",
      "Loss: 1.641 | Acc: 39.116% (9588/24512)\n",
      "Loss: 1.641 | Acc: 39.128% (9616/24576)\n",
      "Loss: 1.641 | Acc: 39.156% (9648/24640)\n",
      "Loss: 1.641 | Acc: 39.168% (9676/24704)\n",
      "Loss: 1.640 | Acc: 39.184% (9705/24768)\n",
      "Loss: 1.641 | Acc: 39.179% (9729/24832)\n",
      "Loss: 1.641 | Acc: 39.179% (9754/24896)\n",
      "Loss: 1.641 | Acc: 39.175% (9778/24960)\n",
      "Loss: 1.640 | Acc: 39.198% (9809/25024)\n",
      "Loss: 1.641 | Acc: 39.214% (9838/25088)\n",
      "Loss: 1.640 | Acc: 39.233% (9868/25152)\n",
      "Loss: 1.640 | Acc: 39.253% (9898/25216)\n",
      "Loss: 1.640 | Acc: 39.225% (9916/25280)\n",
      "Loss: 1.639 | Acc: 39.248% (9947/25344)\n",
      "Loss: 1.638 | Acc: 39.271% (9978/25408)\n",
      "Loss: 1.638 | Acc: 39.271% (10003/25472)\n",
      "Loss: 1.638 | Acc: 39.258% (10025/25536)\n",
      "Loss: 1.638 | Acc: 39.262% (10051/25600)\n",
      "Loss: 1.638 | Acc: 39.257% (10075/25664)\n",
      "Loss: 1.638 | Acc: 39.261% (10101/25728)\n",
      "Loss: 1.638 | Acc: 39.276% (10130/25792)\n",
      "Loss: 1.638 | Acc: 39.287% (10158/25856)\n",
      "Loss: 1.637 | Acc: 39.313% (10190/25920)\n",
      "Loss: 1.637 | Acc: 39.332% (10220/25984)\n",
      "Loss: 1.637 | Acc: 39.327% (10244/26048)\n",
      "Loss: 1.637 | Acc: 39.334% (10271/26112)\n",
      "Loss: 1.636 | Acc: 39.334% (10296/26176)\n",
      "Loss: 1.636 | Acc: 39.348% (10325/26240)\n",
      "Loss: 1.636 | Acc: 39.344% (10349/26304)\n",
      "Loss: 1.636 | Acc: 39.366% (10380/26368)\n",
      "Loss: 1.636 | Acc: 39.377% (10408/26432)\n",
      "Loss: 1.635 | Acc: 39.391% (10437/26496)\n",
      "Loss: 1.635 | Acc: 39.394% (10463/26560)\n",
      "Loss: 1.635 | Acc: 39.382% (10485/26624)\n",
      "Loss: 1.635 | Acc: 39.381% (10510/26688)\n",
      "Loss: 1.635 | Acc: 39.384% (10536/26752)\n",
      "Loss: 1.634 | Acc: 39.417% (10570/26816)\n",
      "Loss: 1.634 | Acc: 39.412% (10594/26880)\n",
      "Loss: 1.634 | Acc: 39.423% (10622/26944)\n",
      "Loss: 1.634 | Acc: 39.414% (10645/27008)\n",
      "Loss: 1.634 | Acc: 39.406% (10668/27072)\n",
      "Loss: 1.634 | Acc: 39.413% (10695/27136)\n",
      "Loss: 1.634 | Acc: 39.434% (10726/27200)\n",
      "Loss: 1.634 | Acc: 39.440% (10753/27264)\n",
      "Loss: 1.634 | Acc: 39.447% (10780/27328)\n",
      "Loss: 1.633 | Acc: 39.457% (10808/27392)\n",
      "Loss: 1.633 | Acc: 39.478% (10839/27456)\n",
      "Loss: 1.632 | Acc: 39.499% (10870/27520)\n",
      "Loss: 1.632 | Acc: 39.516% (10900/27584)\n",
      "Loss: 1.631 | Acc: 39.533% (10930/27648)\n",
      "Loss: 1.631 | Acc: 39.557% (10962/27712)\n",
      "Loss: 1.631 | Acc: 39.559% (10988/27776)\n",
      "Loss: 1.630 | Acc: 39.565% (11015/27840)\n",
      "Loss: 1.630 | Acc: 39.575% (11043/27904)\n",
      "Loss: 1.630 | Acc: 39.585% (11071/27968)\n",
      "Loss: 1.629 | Acc: 39.598% (11100/28032)\n",
      "Loss: 1.629 | Acc: 39.614% (11130/28096)\n",
      "Loss: 1.630 | Acc: 39.602% (11152/28160)\n",
      "Loss: 1.630 | Acc: 39.594% (11175/28224)\n",
      "Loss: 1.630 | Acc: 39.596% (11201/28288)\n",
      "Loss: 1.629 | Acc: 39.613% (11231/28352)\n",
      "Loss: 1.630 | Acc: 39.604% (11254/28416)\n",
      "Loss: 1.629 | Acc: 39.635% (11288/28480)\n",
      "Loss: 1.629 | Acc: 39.623% (11310/28544)\n",
      "Loss: 1.629 | Acc: 39.611% (11332/28608)\n",
      "Loss: 1.629 | Acc: 39.628% (11362/28672)\n",
      "Loss: 1.630 | Acc: 39.612% (11383/28736)\n",
      "Loss: 1.630 | Acc: 39.615% (11409/28800)\n",
      "Loss: 1.630 | Acc: 39.613% (11434/28864)\n",
      "Loss: 1.630 | Acc: 39.605% (11457/28928)\n",
      "Loss: 1.629 | Acc: 39.635% (11491/28992)\n",
      "Loss: 1.629 | Acc: 39.620% (11512/29056)\n",
      "Loss: 1.629 | Acc: 39.629% (11540/29120)\n",
      "Loss: 1.629 | Acc: 39.648% (11571/29184)\n",
      "Loss: 1.629 | Acc: 39.654% (11598/29248)\n",
      "Loss: 1.629 | Acc: 39.622% (11614/29312)\n",
      "Loss: 1.630 | Acc: 39.594% (11631/29376)\n",
      "Loss: 1.630 | Acc: 39.606% (11660/29440)\n",
      "Loss: 1.630 | Acc: 39.605% (11685/29504)\n",
      "Loss: 1.630 | Acc: 39.597% (11708/29568)\n",
      "Loss: 1.629 | Acc: 39.592% (11732/29632)\n",
      "Loss: 1.629 | Acc: 39.615% (11764/29696)\n",
      "Loss: 1.629 | Acc: 39.647% (11799/29760)\n",
      "Loss: 1.628 | Acc: 39.649% (11825/29824)\n",
      "Loss: 1.628 | Acc: 39.678% (11859/29888)\n",
      "Loss: 1.628 | Acc: 39.667% (11881/29952)\n",
      "Loss: 1.627 | Acc: 39.692% (11914/30016)\n",
      "Loss: 1.627 | Acc: 39.701% (11942/30080)\n",
      "Loss: 1.628 | Acc: 39.686% (11963/30144)\n",
      "Loss: 1.627 | Acc: 39.685% (11988/30208)\n",
      "Loss: 1.627 | Acc: 39.710% (12021/30272)\n",
      "Loss: 1.627 | Acc: 39.722% (12050/30336)\n",
      "Loss: 1.627 | Acc: 39.714% (12073/30400)\n",
      "Loss: 1.627 | Acc: 39.719% (12100/30464)\n",
      "Loss: 1.627 | Acc: 39.727% (12128/30528)\n",
      "Loss: 1.627 | Acc: 39.739% (12157/30592)\n",
      "Loss: 1.626 | Acc: 39.761% (12189/30656)\n",
      "Loss: 1.626 | Acc: 39.766% (12216/30720)\n",
      "Loss: 1.627 | Acc: 39.764% (12241/30784)\n",
      "Loss: 1.627 | Acc: 39.769% (12268/30848)\n",
      "Loss: 1.627 | Acc: 39.752% (12288/30912)\n",
      "Loss: 1.626 | Acc: 39.769% (12319/30976)\n",
      "Loss: 1.626 | Acc: 39.781% (12348/31040)\n",
      "Loss: 1.626 | Acc: 39.783% (12374/31104)\n",
      "Loss: 1.626 | Acc: 39.781% (12399/31168)\n",
      "Loss: 1.626 | Acc: 39.783% (12425/31232)\n",
      "Loss: 1.625 | Acc: 39.794% (12454/31296)\n",
      "Loss: 1.625 | Acc: 39.809% (12484/31360)\n",
      "Loss: 1.625 | Acc: 39.798% (12506/31424)\n",
      "Loss: 1.625 | Acc: 39.796% (12531/31488)\n",
      "Loss: 1.625 | Acc: 39.804% (12559/31552)\n",
      "Loss: 1.625 | Acc: 39.774% (12575/31616)\n",
      "Loss: 1.625 | Acc: 39.779% (12602/31680)\n",
      "Loss: 1.625 | Acc: 39.803% (12635/31744)\n",
      "Loss: 1.625 | Acc: 39.808% (12662/31808)\n",
      "Loss: 1.624 | Acc: 39.816% (12690/31872)\n",
      "Loss: 1.625 | Acc: 39.811% (12714/31936)\n",
      "Loss: 1.625 | Acc: 39.812% (12740/32000)\n",
      "Loss: 1.625 | Acc: 39.823% (12769/32064)\n",
      "Loss: 1.625 | Acc: 39.834% (12798/32128)\n",
      "Loss: 1.624 | Acc: 39.858% (12831/32192)\n",
      "Loss: 1.624 | Acc: 39.862% (12858/32256)\n",
      "Loss: 1.624 | Acc: 39.876% (12888/32320)\n",
      "Loss: 1.624 | Acc: 39.872% (12912/32384)\n",
      "Loss: 1.624 | Acc: 39.888% (12943/32448)\n",
      "Loss: 1.624 | Acc: 39.875% (12964/32512)\n",
      "Loss: 1.623 | Acc: 39.879% (12991/32576)\n",
      "Loss: 1.623 | Acc: 39.890% (13020/32640)\n",
      "Loss: 1.623 | Acc: 39.879% (13042/32704)\n",
      "Loss: 1.623 | Acc: 39.859% (13061/32768)\n",
      "Loss: 1.623 | Acc: 39.873% (13091/32832)\n",
      "Loss: 1.623 | Acc: 39.871% (13116/32896)\n",
      "Loss: 1.623 | Acc: 39.863% (13139/32960)\n",
      "Loss: 1.623 | Acc: 39.871% (13167/33024)\n",
      "Loss: 1.623 | Acc: 39.872% (13193/33088)\n",
      "Loss: 1.622 | Acc: 39.877% (13220/33152)\n",
      "Loss: 1.622 | Acc: 39.872% (13244/33216)\n",
      "Loss: 1.622 | Acc: 39.883% (13273/33280)\n",
      "Loss: 1.622 | Acc: 39.893% (13302/33344)\n",
      "Loss: 1.622 | Acc: 39.865% (13318/33408)\n",
      "Loss: 1.622 | Acc: 39.872% (13346/33472)\n",
      "Loss: 1.622 | Acc: 39.888% (13377/33536)\n",
      "Loss: 1.622 | Acc: 39.896% (13405/33600)\n",
      "Loss: 1.622 | Acc: 39.885% (13427/33664)\n",
      "Loss: 1.622 | Acc: 39.872% (13448/33728)\n",
      "Loss: 1.622 | Acc: 39.873% (13474/33792)\n",
      "Loss: 1.622 | Acc: 39.887% (13504/33856)\n",
      "Loss: 1.622 | Acc: 39.876% (13526/33920)\n",
      "Loss: 1.622 | Acc: 39.895% (13558/33984)\n",
      "Loss: 1.622 | Acc: 39.894% (13583/34048)\n",
      "Loss: 1.622 | Acc: 39.892% (13608/34112)\n",
      "Loss: 1.621 | Acc: 39.899% (13636/34176)\n",
      "Loss: 1.621 | Acc: 39.904% (13663/34240)\n",
      "Loss: 1.621 | Acc: 39.922% (13695/34304)\n",
      "Loss: 1.621 | Acc: 39.924% (13721/34368)\n",
      "Loss: 1.621 | Acc: 39.928% (13748/34432)\n",
      "Loss: 1.620 | Acc: 39.921% (13771/34496)\n",
      "Loss: 1.621 | Acc: 39.916% (13795/34560)\n",
      "Loss: 1.621 | Acc: 39.906% (13817/34624)\n",
      "Loss: 1.621 | Acc: 39.904% (13842/34688)\n",
      "Loss: 1.621 | Acc: 39.914% (13871/34752)\n",
      "Loss: 1.621 | Acc: 39.901% (13892/34816)\n",
      "Loss: 1.621 | Acc: 39.894% (13915/34880)\n",
      "Loss: 1.621 | Acc: 39.910% (13946/34944)\n",
      "Loss: 1.621 | Acc: 39.914% (13973/35008)\n",
      "Loss: 1.620 | Acc: 39.932% (14005/35072)\n",
      "Loss: 1.621 | Acc: 39.919% (14026/35136)\n",
      "Loss: 1.621 | Acc: 39.932% (14056/35200)\n",
      "Loss: 1.620 | Acc: 39.947% (14087/35264)\n",
      "Loss: 1.621 | Acc: 39.934% (14108/35328)\n",
      "Loss: 1.621 | Acc: 39.930% (14132/35392)\n",
      "Loss: 1.620 | Acc: 39.942% (14162/35456)\n",
      "Loss: 1.620 | Acc: 39.941% (14187/35520)\n",
      "Loss: 1.620 | Acc: 39.942% (14213/35584)\n",
      "Loss: 1.620 | Acc: 39.952% (14242/35648)\n",
      "Loss: 1.620 | Acc: 39.925% (14258/35712)\n",
      "Loss: 1.620 | Acc: 39.918% (14281/35776)\n",
      "Loss: 1.620 | Acc: 39.936% (14313/35840)\n",
      "Loss: 1.620 | Acc: 39.940% (14340/35904)\n",
      "Loss: 1.619 | Acc: 39.952% (14370/35968)\n",
      "Loss: 1.620 | Acc: 39.945% (14393/36032)\n",
      "Loss: 1.620 | Acc: 39.968% (14427/36096)\n",
      "Loss: 1.620 | Acc: 39.961% (14450/36160)\n",
      "Loss: 1.620 | Acc: 39.951% (14472/36224)\n",
      "Loss: 1.620 | Acc: 39.955% (14499/36288)\n",
      "Loss: 1.620 | Acc: 39.984% (14535/36352)\n",
      "Loss: 1.620 | Acc: 39.971% (14556/36416)\n",
      "Loss: 1.620 | Acc: 39.981% (14585/36480)\n",
      "Loss: 1.620 | Acc: 40.001% (14618/36544)\n",
      "Loss: 1.620 | Acc: 39.994% (14641/36608)\n",
      "Loss: 1.620 | Acc: 39.995% (14667/36672)\n",
      "Loss: 1.620 | Acc: 40.007% (14697/36736)\n",
      "Loss: 1.620 | Acc: 40.008% (14723/36800)\n",
      "Loss: 1.620 | Acc: 40.023% (14754/36864)\n",
      "Loss: 1.619 | Acc: 40.037% (14785/36928)\n",
      "Loss: 1.619 | Acc: 40.033% (14809/36992)\n",
      "Loss: 1.619 | Acc: 40.034% (14835/37056)\n",
      "Loss: 1.619 | Acc: 40.027% (14858/37120)\n",
      "Loss: 1.619 | Acc: 40.047% (14891/37184)\n",
      "Loss: 1.619 | Acc: 40.040% (14914/37248)\n",
      "Loss: 1.619 | Acc: 40.025% (14934/37312)\n",
      "Loss: 1.619 | Acc: 40.034% (14963/37376)\n",
      "Loss: 1.619 | Acc: 40.027% (14986/37440)\n",
      "Loss: 1.619 | Acc: 40.046% (15019/37504)\n",
      "Loss: 1.619 | Acc: 40.045% (15044/37568)\n",
      "Loss: 1.618 | Acc: 40.062% (15076/37632)\n",
      "Loss: 1.618 | Acc: 40.063% (15102/37696)\n",
      "Loss: 1.618 | Acc: 40.082% (15135/37760)\n",
      "Loss: 1.618 | Acc: 40.091% (15164/37824)\n",
      "Loss: 1.618 | Acc: 40.108% (15196/37888)\n",
      "Loss: 1.617 | Acc: 40.127% (15229/37952)\n",
      "Loss: 1.617 | Acc: 40.149% (15263/38016)\n",
      "Loss: 1.617 | Acc: 40.150% (15289/38080)\n",
      "Loss: 1.617 | Acc: 40.153% (15316/38144)\n",
      "Loss: 1.617 | Acc: 40.143% (15338/38208)\n",
      "Loss: 1.616 | Acc: 40.165% (15372/38272)\n",
      "Loss: 1.616 | Acc: 40.166% (15398/38336)\n",
      "Loss: 1.616 | Acc: 40.164% (15423/38400)\n",
      "Loss: 1.616 | Acc: 40.167% (15450/38464)\n",
      "Loss: 1.616 | Acc: 40.160% (15473/38528)\n",
      "Loss: 1.616 | Acc: 40.161% (15499/38592)\n",
      "Loss: 1.615 | Acc: 40.167% (15527/38656)\n",
      "Loss: 1.616 | Acc: 40.165% (15552/38720)\n",
      "Loss: 1.616 | Acc: 40.166% (15578/38784)\n",
      "Loss: 1.616 | Acc: 40.167% (15604/38848)\n",
      "Loss: 1.615 | Acc: 40.173% (15632/38912)\n",
      "Loss: 1.615 | Acc: 40.179% (15660/38976)\n",
      "Loss: 1.615 | Acc: 40.187% (15689/39040)\n",
      "Loss: 1.615 | Acc: 40.193% (15717/39104)\n",
      "Loss: 1.615 | Acc: 40.186% (15740/39168)\n",
      "Loss: 1.615 | Acc: 40.202% (15772/39232)\n",
      "Loss: 1.614 | Acc: 40.208% (15800/39296)\n",
      "Loss: 1.614 | Acc: 40.201% (15823/39360)\n",
      "Loss: 1.614 | Acc: 40.194% (15846/39424)\n",
      "Loss: 1.614 | Acc: 40.207% (15877/39488)\n",
      "Loss: 1.614 | Acc: 40.226% (15910/39552)\n",
      "Loss: 1.614 | Acc: 40.226% (15936/39616)\n",
      "Loss: 1.614 | Acc: 40.229% (15963/39680)\n",
      "Loss: 1.614 | Acc: 40.235% (15991/39744)\n",
      "Loss: 1.614 | Acc: 40.231% (16015/39808)\n",
      "Loss: 1.613 | Acc: 40.251% (16049/39872)\n",
      "Loss: 1.613 | Acc: 40.249% (16074/39936)\n",
      "Loss: 1.613 | Acc: 40.258% (16103/40000)\n",
      "Loss: 1.613 | Acc: 40.256% (16128/40064)\n",
      "Loss: 1.613 | Acc: 40.266% (16158/40128)\n",
      "Loss: 1.613 | Acc: 40.264% (16183/40192)\n",
      "Loss: 1.612 | Acc: 40.260% (16207/40256)\n",
      "Loss: 1.613 | Acc: 40.270% (16237/40320)\n",
      "Loss: 1.612 | Acc: 40.281% (16267/40384)\n",
      "Loss: 1.612 | Acc: 40.286% (16295/40448)\n",
      "Loss: 1.612 | Acc: 40.309% (16330/40512)\n",
      "Loss: 1.611 | Acc: 40.322% (16361/40576)\n",
      "Loss: 1.611 | Acc: 40.320% (16386/40640)\n",
      "Loss: 1.611 | Acc: 40.323% (16413/40704)\n",
      "Loss: 1.611 | Acc: 40.323% (16439/40768)\n",
      "Loss: 1.610 | Acc: 40.331% (16468/40832)\n",
      "Loss: 1.610 | Acc: 40.332% (16494/40896)\n",
      "Loss: 1.610 | Acc: 40.334% (16521/40960)\n",
      "Loss: 1.610 | Acc: 40.313% (16538/41024)\n",
      "Loss: 1.610 | Acc: 40.331% (16571/41088)\n",
      "Loss: 1.610 | Acc: 40.326% (16595/41152)\n",
      "Loss: 1.610 | Acc: 40.334% (16624/41216)\n",
      "Loss: 1.610 | Acc: 40.337% (16651/41280)\n",
      "Loss: 1.609 | Acc: 40.335% (16676/41344)\n",
      "Loss: 1.609 | Acc: 40.342% (16705/41408)\n",
      "Loss: 1.609 | Acc: 40.350% (16734/41472)\n",
      "Loss: 1.609 | Acc: 40.363% (16765/41536)\n",
      "Loss: 1.609 | Acc: 40.365% (16792/41600)\n",
      "Loss: 1.609 | Acc: 40.363% (16817/41664)\n",
      "Loss: 1.609 | Acc: 40.376% (16848/41728)\n",
      "Loss: 1.610 | Acc: 40.367% (16870/41792)\n",
      "Loss: 1.610 | Acc: 40.379% (16901/41856)\n",
      "Loss: 1.610 | Acc: 40.384% (16929/41920)\n",
      "Loss: 1.610 | Acc: 40.380% (16953/41984)\n",
      "Loss: 1.610 | Acc: 40.375% (16977/42048)\n",
      "Loss: 1.610 | Acc: 40.383% (17006/42112)\n",
      "Loss: 1.610 | Acc: 40.395% (17037/42176)\n",
      "Loss: 1.610 | Acc: 40.388% (17060/42240)\n",
      "Loss: 1.609 | Acc: 40.389% (17086/42304)\n",
      "Loss: 1.609 | Acc: 40.394% (17114/42368)\n",
      "Loss: 1.609 | Acc: 40.387% (17137/42432)\n",
      "Loss: 1.609 | Acc: 40.390% (17164/42496)\n",
      "Loss: 1.609 | Acc: 40.399% (17194/42560)\n",
      "Loss: 1.609 | Acc: 40.407% (17223/42624)\n",
      "Loss: 1.609 | Acc: 40.414% (17252/42688)\n",
      "Loss: 1.608 | Acc: 40.422% (17281/42752)\n",
      "Loss: 1.608 | Acc: 40.429% (17310/42816)\n",
      "Loss: 1.608 | Acc: 40.448% (17344/42880)\n",
      "Loss: 1.608 | Acc: 40.462% (17376/42944)\n",
      "Loss: 1.607 | Acc: 40.472% (17406/43008)\n",
      "Loss: 1.608 | Acc: 40.462% (17428/43072)\n",
      "Loss: 1.608 | Acc: 40.465% (17455/43136)\n",
      "Loss: 1.608 | Acc: 40.468% (17482/43200)\n",
      "Loss: 1.608 | Acc: 40.456% (17503/43264)\n",
      "Loss: 1.608 | Acc: 40.443% (17523/43328)\n",
      "Loss: 1.608 | Acc: 40.443% (17549/43392)\n",
      "Loss: 1.608 | Acc: 40.443% (17575/43456)\n",
      "Loss: 1.608 | Acc: 40.439% (17599/43520)\n",
      "Loss: 1.608 | Acc: 40.451% (17630/43584)\n",
      "Loss: 1.608 | Acc: 40.453% (17657/43648)\n",
      "Loss: 1.608 | Acc: 40.444% (17679/43712)\n",
      "Loss: 1.608 | Acc: 40.440% (17703/43776)\n",
      "Loss: 1.608 | Acc: 40.445% (17731/43840)\n",
      "Loss: 1.608 | Acc: 40.456% (17762/43904)\n",
      "Loss: 1.608 | Acc: 40.461% (17790/43968)\n",
      "Loss: 1.608 | Acc: 40.475% (17822/44032)\n",
      "Loss: 1.607 | Acc: 40.496% (17857/44096)\n",
      "Loss: 1.607 | Acc: 40.485% (17878/44160)\n",
      "Loss: 1.607 | Acc: 40.476% (17900/44224)\n",
      "Loss: 1.607 | Acc: 40.496% (17935/44288)\n",
      "Loss: 1.607 | Acc: 40.506% (17965/44352)\n",
      "Loss: 1.607 | Acc: 40.501% (17989/44416)\n",
      "Loss: 1.607 | Acc: 40.501% (18015/44480)\n",
      "Loss: 1.606 | Acc: 40.504% (18042/44544)\n",
      "Loss: 1.606 | Acc: 40.515% (18073/44608)\n",
      "Loss: 1.606 | Acc: 40.509% (18096/44672)\n",
      "Loss: 1.606 | Acc: 40.511% (18123/44736)\n",
      "Loss: 1.606 | Acc: 40.520% (18153/44800)\n",
      "Loss: 1.605 | Acc: 40.531% (18184/44864)\n",
      "Loss: 1.605 | Acc: 40.534% (18211/44928)\n",
      "Loss: 1.605 | Acc: 40.534% (18237/44992)\n",
      "Loss: 1.605 | Acc: 40.543% (18267/45056)\n",
      "Loss: 1.604 | Acc: 40.556% (18299/45120)\n",
      "Loss: 1.604 | Acc: 40.574% (18333/45184)\n",
      "Loss: 1.604 | Acc: 40.592% (18367/45248)\n",
      "Loss: 1.604 | Acc: 40.592% (18393/45312)\n",
      "Loss: 1.604 | Acc: 40.596% (18421/45376)\n",
      "Loss: 1.604 | Acc: 40.616% (18456/45440)\n",
      "Loss: 1.604 | Acc: 40.618% (18483/45504)\n",
      "Loss: 1.603 | Acc: 40.625% (18512/45568)\n",
      "Loss: 1.603 | Acc: 40.634% (18542/45632)\n",
      "Loss: 1.603 | Acc: 40.634% (18568/45696)\n",
      "Loss: 1.603 | Acc: 40.634% (18594/45760)\n",
      "Loss: 1.603 | Acc: 40.651% (18628/45824)\n",
      "Loss: 1.603 | Acc: 40.640% (18649/45888)\n",
      "Loss: 1.603 | Acc: 40.642% (18676/45952)\n",
      "Loss: 1.603 | Acc: 40.651% (18706/46016)\n",
      "Loss: 1.603 | Acc: 40.662% (18737/46080)\n",
      "Loss: 1.603 | Acc: 40.677% (18770/46144)\n",
      "Loss: 1.603 | Acc: 40.679% (18797/46208)\n",
      "Loss: 1.603 | Acc: 40.668% (18818/46272)\n",
      "Loss: 1.603 | Acc: 40.670% (18845/46336)\n",
      "Loss: 1.602 | Acc: 40.681% (18876/46400)\n",
      "Loss: 1.602 | Acc: 40.685% (18904/46464)\n",
      "Loss: 1.602 | Acc: 40.692% (18933/46528)\n",
      "Loss: 1.602 | Acc: 40.698% (18962/46592)\n",
      "Loss: 1.602 | Acc: 40.698% (18988/46656)\n",
      "Loss: 1.601 | Acc: 40.711% (19020/46720)\n",
      "Loss: 1.601 | Acc: 40.721% (19051/46784)\n",
      "Loss: 1.601 | Acc: 40.719% (19076/46848)\n",
      "Loss: 1.601 | Acc: 40.715% (19100/46912)\n",
      "Loss: 1.601 | Acc: 40.723% (19130/46976)\n",
      "Loss: 1.601 | Acc: 40.702% (19146/47040)\n",
      "Loss: 1.601 | Acc: 40.691% (19167/47104)\n",
      "Loss: 1.601 | Acc: 40.701% (19198/47168)\n",
      "Loss: 1.601 | Acc: 40.708% (19227/47232)\n",
      "Loss: 1.601 | Acc: 40.703% (19251/47296)\n",
      "Loss: 1.601 | Acc: 40.709% (19280/47360)\n",
      "Loss: 1.600 | Acc: 40.705% (19304/47424)\n",
      "Loss: 1.600 | Acc: 40.711% (19333/47488)\n",
      "Loss: 1.600 | Acc: 40.730% (19368/47552)\n",
      "Loss: 1.600 | Acc: 40.722% (19390/47616)\n",
      "Loss: 1.600 | Acc: 40.711% (19411/47680)\n",
      "Loss: 1.600 | Acc: 40.702% (19433/47744)\n",
      "Loss: 1.600 | Acc: 40.711% (19463/47808)\n",
      "Loss: 1.600 | Acc: 40.717% (19492/47872)\n",
      "Loss: 1.600 | Acc: 40.723% (19521/47936)\n",
      "Loss: 1.600 | Acc: 40.717% (19544/48000)\n",
      "Loss: 1.599 | Acc: 40.735% (19579/48064)\n",
      "Loss: 1.599 | Acc: 40.733% (19604/48128)\n",
      "Loss: 1.599 | Acc: 40.741% (19634/48192)\n",
      "Loss: 1.599 | Acc: 40.753% (19666/48256)\n",
      "Loss: 1.599 | Acc: 40.749% (19690/48320)\n",
      "Loss: 1.598 | Acc: 40.753% (19718/48384)\n",
      "Loss: 1.598 | Acc: 40.757% (19746/48448)\n",
      "Loss: 1.598 | Acc: 40.771% (19779/48512)\n",
      "Loss: 1.598 | Acc: 40.781% (19810/48576)\n",
      "Loss: 1.597 | Acc: 40.781% (19836/48640)\n",
      "Loss: 1.597 | Acc: 40.795% (19869/48704)\n",
      "Loss: 1.597 | Acc: 40.793% (19894/48768)\n",
      "Loss: 1.597 | Acc: 40.799% (19923/48832)\n",
      "Loss: 1.597 | Acc: 40.799% (19949/48896)\n",
      "Loss: 1.597 | Acc: 40.805% (19978/48960)\n",
      "Loss: 1.597 | Acc: 40.808% (19996/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 40.80816326530612\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.394 | Acc: 51.562% (33/64)\n",
      "Loss: 1.381 | Acc: 50.000% (64/128)\n",
      "Loss: 1.389 | Acc: 51.042% (98/192)\n",
      "Loss: 1.478 | Acc: 50.000% (128/256)\n",
      "Loss: 1.462 | Acc: 48.438% (155/320)\n",
      "Loss: 1.492 | Acc: 47.396% (182/384)\n",
      "Loss: 1.510 | Acc: 45.982% (206/448)\n",
      "Loss: 1.507 | Acc: 46.680% (239/512)\n",
      "Loss: 1.496 | Acc: 46.875% (270/576)\n",
      "Loss: 1.484 | Acc: 47.188% (302/640)\n",
      "Loss: 1.493 | Acc: 45.881% (323/704)\n",
      "Loss: 1.492 | Acc: 46.094% (354/768)\n",
      "Loss: 1.483 | Acc: 46.394% (386/832)\n",
      "Loss: 1.485 | Acc: 46.205% (414/896)\n",
      "Loss: 1.479 | Acc: 46.354% (445/960)\n",
      "Loss: 1.464 | Acc: 46.777% (479/1024)\n",
      "Loss: 1.468 | Acc: 46.783% (509/1088)\n",
      "Loss: 1.474 | Acc: 46.788% (539/1152)\n",
      "Loss: 1.476 | Acc: 46.464% (565/1216)\n",
      "Loss: 1.481 | Acc: 45.938% (588/1280)\n",
      "Loss: 1.481 | Acc: 45.908% (617/1344)\n",
      "Loss: 1.483 | Acc: 45.455% (640/1408)\n",
      "Loss: 1.478 | Acc: 45.245% (666/1472)\n",
      "Loss: 1.477 | Acc: 45.247% (695/1536)\n",
      "Loss: 1.479 | Acc: 45.062% (721/1600)\n",
      "Loss: 1.481 | Acc: 44.832% (746/1664)\n",
      "Loss: 1.481 | Acc: 45.139% (780/1728)\n",
      "Loss: 1.483 | Acc: 44.978% (806/1792)\n",
      "Loss: 1.480 | Acc: 44.989% (835/1856)\n",
      "Loss: 1.478 | Acc: 45.156% (867/1920)\n",
      "Loss: 1.478 | Acc: 45.212% (897/1984)\n",
      "Loss: 1.477 | Acc: 45.459% (931/2048)\n",
      "Loss: 1.476 | Acc: 45.218% (955/2112)\n",
      "Loss: 1.480 | Acc: 45.037% (980/2176)\n",
      "Loss: 1.477 | Acc: 45.268% (1014/2240)\n",
      "Loss: 1.476 | Acc: 45.139% (1040/2304)\n",
      "Loss: 1.478 | Acc: 45.101% (1068/2368)\n",
      "Loss: 1.480 | Acc: 45.189% (1099/2432)\n",
      "Loss: 1.476 | Acc: 45.513% (1136/2496)\n",
      "Loss: 1.480 | Acc: 45.391% (1162/2560)\n",
      "Loss: 1.479 | Acc: 45.389% (1191/2624)\n",
      "Loss: 1.480 | Acc: 45.424% (1221/2688)\n",
      "Loss: 1.480 | Acc: 45.276% (1246/2752)\n",
      "Loss: 1.486 | Acc: 45.028% (1268/2816)\n",
      "Loss: 1.484 | Acc: 45.208% (1302/2880)\n",
      "Loss: 1.483 | Acc: 45.346% (1335/2944)\n",
      "Loss: 1.484 | Acc: 45.279% (1362/3008)\n",
      "Loss: 1.484 | Acc: 45.150% (1387/3072)\n",
      "Loss: 1.483 | Acc: 45.089% (1414/3136)\n",
      "Loss: 1.479 | Acc: 45.281% (1449/3200)\n",
      "Loss: 1.481 | Acc: 45.251% (1477/3264)\n",
      "Loss: 1.480 | Acc: 45.312% (1508/3328)\n",
      "Loss: 1.480 | Acc: 45.371% (1539/3392)\n",
      "Loss: 1.481 | Acc: 45.399% (1569/3456)\n",
      "Loss: 1.482 | Acc: 45.312% (1595/3520)\n",
      "Loss: 1.479 | Acc: 45.396% (1627/3584)\n",
      "Loss: 1.481 | Acc: 45.340% (1654/3648)\n",
      "Loss: 1.479 | Acc: 45.366% (1684/3712)\n",
      "Loss: 1.480 | Acc: 45.286% (1710/3776)\n",
      "Loss: 1.480 | Acc: 45.339% (1741/3840)\n",
      "Loss: 1.478 | Acc: 45.338% (1770/3904)\n",
      "Loss: 1.477 | Acc: 45.388% (1801/3968)\n",
      "Loss: 1.476 | Acc: 45.511% (1835/4032)\n",
      "Loss: 1.478 | Acc: 45.483% (1863/4096)\n",
      "Loss: 1.481 | Acc: 45.529% (1894/4160)\n",
      "Loss: 1.480 | Acc: 45.431% (1919/4224)\n",
      "Loss: 1.480 | Acc: 45.406% (1947/4288)\n",
      "Loss: 1.480 | Acc: 45.427% (1977/4352)\n",
      "Loss: 1.478 | Acc: 45.584% (2013/4416)\n",
      "Loss: 1.476 | Acc: 45.625% (2044/4480)\n",
      "Loss: 1.474 | Acc: 45.621% (2073/4544)\n",
      "Loss: 1.475 | Acc: 45.551% (2099/4608)\n",
      "Loss: 1.473 | Acc: 45.505% (2126/4672)\n",
      "Loss: 1.471 | Acc: 45.608% (2160/4736)\n",
      "Loss: 1.473 | Acc: 45.604% (2189/4800)\n",
      "Loss: 1.473 | Acc: 45.600% (2218/4864)\n",
      "Loss: 1.472 | Acc: 45.556% (2245/4928)\n",
      "Loss: 1.473 | Acc: 45.473% (2270/4992)\n",
      "Loss: 1.474 | Acc: 45.372% (2294/5056)\n",
      "Loss: 1.475 | Acc: 45.293% (2319/5120)\n",
      "Loss: 1.475 | Acc: 45.274% (2347/5184)\n",
      "Loss: 1.477 | Acc: 45.141% (2369/5248)\n",
      "Loss: 1.475 | Acc: 45.218% (2402/5312)\n",
      "Loss: 1.478 | Acc: 45.089% (2424/5376)\n",
      "Loss: 1.478 | Acc: 44.963% (2446/5440)\n",
      "Loss: 1.478 | Acc: 45.022% (2478/5504)\n",
      "Loss: 1.481 | Acc: 44.917% (2501/5568)\n",
      "Loss: 1.483 | Acc: 44.833% (2525/5632)\n",
      "Loss: 1.483 | Acc: 44.856% (2555/5696)\n",
      "Loss: 1.481 | Acc: 44.948% (2589/5760)\n",
      "Loss: 1.481 | Acc: 44.969% (2619/5824)\n",
      "Loss: 1.483 | Acc: 44.820% (2639/5888)\n",
      "Loss: 1.484 | Acc: 44.808% (2667/5952)\n",
      "Loss: 1.483 | Acc: 44.847% (2698/6016)\n",
      "Loss: 1.484 | Acc: 44.803% (2724/6080)\n",
      "Loss: 1.485 | Acc: 44.694% (2746/6144)\n",
      "Loss: 1.486 | Acc: 44.652% (2772/6208)\n",
      "Loss: 1.487 | Acc: 44.659% (2801/6272)\n",
      "Loss: 1.486 | Acc: 44.744% (2835/6336)\n",
      "Loss: 1.488 | Acc: 44.625% (2856/6400)\n",
      "Loss: 1.489 | Acc: 44.554% (2880/6464)\n",
      "Loss: 1.489 | Acc: 44.562% (2909/6528)\n",
      "Loss: 1.490 | Acc: 44.493% (2933/6592)\n",
      "Loss: 1.491 | Acc: 44.366% (2953/6656)\n",
      "Loss: 1.491 | Acc: 44.315% (2978/6720)\n",
      "Loss: 1.491 | Acc: 44.325% (3007/6784)\n",
      "Loss: 1.488 | Acc: 44.305% (3034/6848)\n",
      "Loss: 1.492 | Acc: 44.198% (3055/6912)\n",
      "Loss: 1.492 | Acc: 44.237% (3086/6976)\n",
      "Loss: 1.493 | Acc: 44.162% (3109/7040)\n",
      "Loss: 1.493 | Acc: 44.158% (3137/7104)\n",
      "Loss: 1.493 | Acc: 44.155% (3165/7168)\n",
      "Loss: 1.493 | Acc: 44.110% (3190/7232)\n",
      "Loss: 1.491 | Acc: 44.257% (3229/7296)\n",
      "Loss: 1.489 | Acc: 44.307% (3261/7360)\n",
      "Loss: 1.489 | Acc: 44.248% (3285/7424)\n",
      "Loss: 1.489 | Acc: 44.217% (3311/7488)\n",
      "Loss: 1.488 | Acc: 44.266% (3343/7552)\n",
      "Loss: 1.488 | Acc: 44.236% (3369/7616)\n",
      "Loss: 1.488 | Acc: 44.167% (3392/7680)\n",
      "Loss: 1.487 | Acc: 44.176% (3421/7744)\n",
      "Loss: 1.487 | Acc: 44.160% (3448/7808)\n",
      "Loss: 1.489 | Acc: 44.207% (3480/7872)\n",
      "Loss: 1.488 | Acc: 44.191% (3507/7936)\n",
      "Loss: 1.491 | Acc: 44.138% (3531/8000)\n",
      "Loss: 1.490 | Acc: 44.184% (3563/8064)\n",
      "Loss: 1.490 | Acc: 44.131% (3587/8128)\n",
      "Loss: 1.488 | Acc: 44.226% (3623/8192)\n",
      "Loss: 1.489 | Acc: 44.222% (3651/8256)\n",
      "Loss: 1.490 | Acc: 44.159% (3674/8320)\n",
      "Loss: 1.490 | Acc: 44.167% (3703/8384)\n",
      "Loss: 1.489 | Acc: 44.141% (3729/8448)\n",
      "Loss: 1.490 | Acc: 44.149% (3758/8512)\n",
      "Loss: 1.490 | Acc: 44.146% (3786/8576)\n",
      "Loss: 1.489 | Acc: 44.178% (3817/8640)\n",
      "Loss: 1.490 | Acc: 44.152% (3843/8704)\n",
      "Loss: 1.489 | Acc: 44.172% (3873/8768)\n",
      "Loss: 1.490 | Acc: 44.180% (3902/8832)\n",
      "Loss: 1.489 | Acc: 44.211% (3933/8896)\n",
      "Loss: 1.488 | Acc: 44.230% (3963/8960)\n",
      "Loss: 1.488 | Acc: 44.304% (3998/9024)\n",
      "Loss: 1.489 | Acc: 44.256% (4022/9088)\n",
      "Loss: 1.489 | Acc: 44.285% (4053/9152)\n",
      "Loss: 1.488 | Acc: 44.347% (4087/9216)\n",
      "Loss: 1.487 | Acc: 44.386% (4119/9280)\n",
      "Loss: 1.487 | Acc: 44.424% (4151/9344)\n",
      "Loss: 1.488 | Acc: 44.441% (4181/9408)\n",
      "Loss: 1.489 | Acc: 44.405% (4206/9472)\n",
      "Loss: 1.488 | Acc: 44.453% (4239/9536)\n",
      "Loss: 1.488 | Acc: 44.479% (4270/9600)\n",
      "Loss: 1.488 | Acc: 44.485% (4299/9664)\n",
      "Loss: 1.488 | Acc: 44.531% (4332/9728)\n",
      "Loss: 1.489 | Acc: 44.547% (4362/9792)\n",
      "Loss: 1.489 | Acc: 44.521% (4388/9856)\n",
      "Loss: 1.489 | Acc: 44.506% (4415/9920)\n",
      "Loss: 1.490 | Acc: 44.461% (4439/9984)\n",
      "Loss: 1.489 | Acc: 44.470% (4447/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 44.47\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.350 | Acc: 45.312% (29/64)\n",
      "Loss: 1.460 | Acc: 42.969% (55/128)\n",
      "Loss: 1.427 | Acc: 45.312% (87/192)\n",
      "Loss: 1.436 | Acc: 45.703% (117/256)\n",
      "Loss: 1.419 | Acc: 47.500% (152/320)\n",
      "Loss: 1.483 | Acc: 45.312% (174/384)\n",
      "Loss: 1.484 | Acc: 45.089% (202/448)\n",
      "Loss: 1.491 | Acc: 45.703% (234/512)\n",
      "Loss: 1.473 | Acc: 46.354% (267/576)\n",
      "Loss: 1.470 | Acc: 47.031% (301/640)\n",
      "Loss: 1.483 | Acc: 46.307% (326/704)\n",
      "Loss: 1.461 | Acc: 47.135% (362/768)\n",
      "Loss: 1.452 | Acc: 46.995% (391/832)\n",
      "Loss: 1.428 | Acc: 47.879% (429/896)\n",
      "Loss: 1.430 | Acc: 47.604% (457/960)\n",
      "Loss: 1.449 | Acc: 47.363% (485/1024)\n",
      "Loss: 1.439 | Acc: 47.426% (516/1088)\n",
      "Loss: 1.447 | Acc: 46.875% (540/1152)\n",
      "Loss: 1.440 | Acc: 46.793% (569/1216)\n",
      "Loss: 1.433 | Acc: 47.109% (603/1280)\n",
      "Loss: 1.433 | Acc: 47.247% (635/1344)\n",
      "Loss: 1.431 | Acc: 47.372% (667/1408)\n",
      "Loss: 1.436 | Acc: 47.351% (697/1472)\n",
      "Loss: 1.436 | Acc: 47.070% (723/1536)\n",
      "Loss: 1.444 | Acc: 46.438% (743/1600)\n",
      "Loss: 1.437 | Acc: 46.514% (774/1664)\n",
      "Loss: 1.436 | Acc: 46.354% (801/1728)\n",
      "Loss: 1.438 | Acc: 46.205% (828/1792)\n",
      "Loss: 1.443 | Acc: 46.067% (855/1856)\n",
      "Loss: 1.437 | Acc: 46.198% (887/1920)\n",
      "Loss: 1.449 | Acc: 46.069% (914/1984)\n",
      "Loss: 1.451 | Acc: 45.996% (942/2048)\n",
      "Loss: 1.449 | Acc: 45.928% (970/2112)\n",
      "Loss: 1.442 | Acc: 46.324% (1008/2176)\n",
      "Loss: 1.441 | Acc: 46.071% (1032/2240)\n",
      "Loss: 1.449 | Acc: 45.920% (1058/2304)\n",
      "Loss: 1.447 | Acc: 45.988% (1089/2368)\n",
      "Loss: 1.450 | Acc: 45.683% (1111/2432)\n",
      "Loss: 1.444 | Acc: 45.994% (1148/2496)\n",
      "Loss: 1.448 | Acc: 45.938% (1176/2560)\n",
      "Loss: 1.447 | Acc: 45.998% (1207/2624)\n",
      "Loss: 1.445 | Acc: 46.205% (1242/2688)\n",
      "Loss: 1.451 | Acc: 46.003% (1266/2752)\n",
      "Loss: 1.450 | Acc: 46.023% (1296/2816)\n",
      "Loss: 1.450 | Acc: 46.076% (1327/2880)\n",
      "Loss: 1.446 | Acc: 46.298% (1363/2944)\n",
      "Loss: 1.448 | Acc: 46.277% (1392/3008)\n",
      "Loss: 1.449 | Acc: 46.094% (1416/3072)\n",
      "Loss: 1.450 | Acc: 45.950% (1441/3136)\n",
      "Loss: 1.448 | Acc: 46.000% (1472/3200)\n",
      "Loss: 1.447 | Acc: 45.925% (1499/3264)\n",
      "Loss: 1.448 | Acc: 45.944% (1529/3328)\n",
      "Loss: 1.448 | Acc: 45.991% (1560/3392)\n",
      "Loss: 1.452 | Acc: 46.007% (1590/3456)\n",
      "Loss: 1.452 | Acc: 45.938% (1617/3520)\n",
      "Loss: 1.449 | Acc: 46.038% (1650/3584)\n",
      "Loss: 1.448 | Acc: 46.162% (1684/3648)\n",
      "Loss: 1.449 | Acc: 46.255% (1717/3712)\n",
      "Loss: 1.450 | Acc: 46.081% (1740/3776)\n",
      "Loss: 1.454 | Acc: 45.781% (1758/3840)\n",
      "Loss: 1.457 | Acc: 45.774% (1787/3904)\n",
      "Loss: 1.457 | Acc: 45.842% (1819/3968)\n",
      "Loss: 1.457 | Acc: 45.809% (1847/4032)\n",
      "Loss: 1.457 | Acc: 45.801% (1876/4096)\n",
      "Loss: 1.458 | Acc: 45.625% (1898/4160)\n",
      "Loss: 1.460 | Acc: 45.549% (1924/4224)\n",
      "Loss: 1.457 | Acc: 45.639% (1957/4288)\n",
      "Loss: 1.457 | Acc: 45.680% (1988/4352)\n",
      "Loss: 1.454 | Acc: 45.743% (2020/4416)\n",
      "Loss: 1.456 | Acc: 45.692% (2047/4480)\n",
      "Loss: 1.455 | Acc: 45.775% (2080/4544)\n",
      "Loss: 1.456 | Acc: 45.703% (2106/4608)\n",
      "Loss: 1.455 | Acc: 45.762% (2138/4672)\n",
      "Loss: 1.453 | Acc: 45.840% (2171/4736)\n",
      "Loss: 1.453 | Acc: 45.833% (2200/4800)\n",
      "Loss: 1.453 | Acc: 45.806% (2228/4864)\n",
      "Loss: 1.452 | Acc: 45.800% (2257/4928)\n",
      "Loss: 1.454 | Acc: 45.733% (2283/4992)\n",
      "Loss: 1.453 | Acc: 45.787% (2315/5056)\n",
      "Loss: 1.452 | Acc: 45.801% (2345/5120)\n",
      "Loss: 1.451 | Acc: 45.833% (2376/5184)\n",
      "Loss: 1.449 | Acc: 45.789% (2403/5248)\n",
      "Loss: 1.448 | Acc: 45.708% (2428/5312)\n",
      "Loss: 1.449 | Acc: 45.740% (2459/5376)\n",
      "Loss: 1.450 | Acc: 45.680% (2485/5440)\n",
      "Loss: 1.451 | Acc: 45.676% (2514/5504)\n",
      "Loss: 1.450 | Acc: 45.708% (2545/5568)\n",
      "Loss: 1.452 | Acc: 45.703% (2574/5632)\n",
      "Loss: 1.452 | Acc: 45.716% (2604/5696)\n",
      "Loss: 1.453 | Acc: 45.729% (2634/5760)\n",
      "Loss: 1.454 | Acc: 45.690% (2661/5824)\n",
      "Loss: 1.456 | Acc: 45.720% (2692/5888)\n",
      "Loss: 1.455 | Acc: 45.699% (2720/5952)\n",
      "Loss: 1.457 | Acc: 45.662% (2747/6016)\n",
      "Loss: 1.456 | Acc: 45.773% (2783/6080)\n",
      "Loss: 1.455 | Acc: 45.801% (2814/6144)\n",
      "Loss: 1.456 | Acc: 45.731% (2839/6208)\n",
      "Loss: 1.456 | Acc: 45.791% (2872/6272)\n",
      "Loss: 1.456 | Acc: 45.739% (2898/6336)\n",
      "Loss: 1.455 | Acc: 45.688% (2924/6400)\n",
      "Loss: 1.456 | Acc: 45.668% (2952/6464)\n",
      "Loss: 1.456 | Acc: 45.634% (2979/6528)\n",
      "Loss: 1.457 | Acc: 45.601% (3006/6592)\n",
      "Loss: 1.458 | Acc: 45.688% (3041/6656)\n",
      "Loss: 1.458 | Acc: 45.685% (3070/6720)\n",
      "Loss: 1.457 | Acc: 45.740% (3103/6784)\n",
      "Loss: 1.456 | Acc: 45.824% (3138/6848)\n",
      "Loss: 1.457 | Acc: 45.761% (3163/6912)\n",
      "Loss: 1.457 | Acc: 45.757% (3192/6976)\n",
      "Loss: 1.457 | Acc: 45.739% (3220/7040)\n",
      "Loss: 1.457 | Acc: 45.721% (3248/7104)\n",
      "Loss: 1.456 | Acc: 45.717% (3277/7168)\n",
      "Loss: 1.455 | Acc: 45.741% (3308/7232)\n",
      "Loss: 1.455 | Acc: 45.751% (3338/7296)\n",
      "Loss: 1.454 | Acc: 45.734% (3366/7360)\n",
      "Loss: 1.454 | Acc: 45.717% (3394/7424)\n",
      "Loss: 1.455 | Acc: 45.807% (3430/7488)\n",
      "Loss: 1.456 | Acc: 45.829% (3461/7552)\n",
      "Loss: 1.454 | Acc: 45.864% (3493/7616)\n",
      "Loss: 1.455 | Acc: 45.911% (3526/7680)\n",
      "Loss: 1.454 | Acc: 45.958% (3559/7744)\n",
      "Loss: 1.456 | Acc: 45.863% (3581/7808)\n",
      "Loss: 1.456 | Acc: 45.897% (3613/7872)\n",
      "Loss: 1.456 | Acc: 45.867% (3640/7936)\n",
      "Loss: 1.456 | Acc: 45.825% (3666/8000)\n",
      "Loss: 1.455 | Acc: 45.883% (3700/8064)\n",
      "Loss: 1.456 | Acc: 45.866% (3728/8128)\n",
      "Loss: 1.456 | Acc: 45.935% (3763/8192)\n",
      "Loss: 1.456 | Acc: 45.942% (3793/8256)\n",
      "Loss: 1.458 | Acc: 45.913% (3820/8320)\n",
      "Loss: 1.458 | Acc: 45.909% (3849/8384)\n",
      "Loss: 1.458 | Acc: 45.869% (3875/8448)\n",
      "Loss: 1.458 | Acc: 45.888% (3906/8512)\n",
      "Loss: 1.459 | Acc: 45.802% (3928/8576)\n",
      "Loss: 1.460 | Acc: 45.775% (3955/8640)\n",
      "Loss: 1.460 | Acc: 45.818% (3988/8704)\n",
      "Loss: 1.459 | Acc: 45.860% (4021/8768)\n",
      "Loss: 1.460 | Acc: 45.811% (4046/8832)\n",
      "Loss: 1.460 | Acc: 45.841% (4078/8896)\n",
      "Loss: 1.461 | Acc: 45.781% (4102/8960)\n",
      "Loss: 1.461 | Acc: 45.767% (4130/9024)\n",
      "Loss: 1.460 | Acc: 45.830% (4165/9088)\n",
      "Loss: 1.458 | Acc: 45.903% (4201/9152)\n",
      "Loss: 1.458 | Acc: 45.964% (4236/9216)\n",
      "Loss: 1.458 | Acc: 45.981% (4267/9280)\n",
      "Loss: 1.456 | Acc: 46.094% (4307/9344)\n",
      "Loss: 1.455 | Acc: 46.152% (4342/9408)\n",
      "Loss: 1.455 | Acc: 46.178% (4374/9472)\n",
      "Loss: 1.455 | Acc: 46.183% (4404/9536)\n",
      "Loss: 1.455 | Acc: 46.219% (4437/9600)\n",
      "Loss: 1.455 | Acc: 46.202% (4465/9664)\n",
      "Loss: 1.456 | Acc: 46.207% (4495/9728)\n",
      "Loss: 1.457 | Acc: 46.140% (4518/9792)\n",
      "Loss: 1.457 | Acc: 46.155% (4549/9856)\n",
      "Loss: 1.455 | Acc: 46.210% (4584/9920)\n",
      "Loss: 1.455 | Acc: 46.214% (4614/9984)\n",
      "Loss: 1.454 | Acc: 46.258% (4648/10048)\n",
      "Loss: 1.454 | Acc: 46.282% (4680/10112)\n",
      "Loss: 1.456 | Acc: 46.266% (4708/10176)\n",
      "Loss: 1.455 | Acc: 46.260% (4737/10240)\n",
      "Loss: 1.456 | Acc: 46.244% (4765/10304)\n",
      "Loss: 1.456 | Acc: 46.267% (4797/10368)\n",
      "Loss: 1.457 | Acc: 46.214% (4821/10432)\n",
      "Loss: 1.459 | Acc: 46.151% (4844/10496)\n",
      "Loss: 1.460 | Acc: 46.146% (4873/10560)\n",
      "Loss: 1.460 | Acc: 46.141% (4902/10624)\n",
      "Loss: 1.460 | Acc: 46.117% (4929/10688)\n",
      "Loss: 1.461 | Acc: 46.103% (4957/10752)\n",
      "Loss: 1.460 | Acc: 46.135% (4990/10816)\n",
      "Loss: 1.461 | Acc: 46.121% (5018/10880)\n",
      "Loss: 1.461 | Acc: 46.153% (5051/10944)\n",
      "Loss: 1.461 | Acc: 46.130% (5078/11008)\n",
      "Loss: 1.462 | Acc: 46.107% (5105/11072)\n",
      "Loss: 1.461 | Acc: 46.094% (5133/11136)\n",
      "Loss: 1.460 | Acc: 46.071% (5160/11200)\n",
      "Loss: 1.460 | Acc: 46.085% (5191/11264)\n",
      "Loss: 1.460 | Acc: 46.098% (5222/11328)\n",
      "Loss: 1.462 | Acc: 46.059% (5247/11392)\n",
      "Loss: 1.462 | Acc: 46.046% (5275/11456)\n",
      "Loss: 1.463 | Acc: 46.007% (5300/11520)\n",
      "Loss: 1.463 | Acc: 46.003% (5329/11584)\n",
      "Loss: 1.463 | Acc: 46.008% (5359/11648)\n",
      "Loss: 1.464 | Acc: 46.021% (5390/11712)\n",
      "Loss: 1.465 | Acc: 45.949% (5411/11776)\n",
      "Loss: 1.465 | Acc: 45.954% (5441/11840)\n",
      "Loss: 1.465 | Acc: 45.926% (5467/11904)\n",
      "Loss: 1.465 | Acc: 45.889% (5492/11968)\n",
      "Loss: 1.465 | Acc: 45.869% (5519/12032)\n",
      "Loss: 1.466 | Acc: 45.817% (5542/12096)\n",
      "Loss: 1.466 | Acc: 45.863% (5577/12160)\n",
      "Loss: 1.467 | Acc: 45.828% (5602/12224)\n",
      "Loss: 1.468 | Acc: 45.785% (5626/12288)\n",
      "Loss: 1.467 | Acc: 45.814% (5659/12352)\n",
      "Loss: 1.467 | Acc: 45.788% (5685/12416)\n",
      "Loss: 1.468 | Acc: 45.745% (5709/12480)\n",
      "Loss: 1.468 | Acc: 45.759% (5740/12544)\n",
      "Loss: 1.468 | Acc: 45.765% (5770/12608)\n",
      "Loss: 1.468 | Acc: 45.778% (5801/12672)\n",
      "Loss: 1.469 | Acc: 45.736% (5825/12736)\n",
      "Loss: 1.469 | Acc: 45.742% (5855/12800)\n",
      "Loss: 1.468 | Acc: 45.802% (5892/12864)\n",
      "Loss: 1.467 | Acc: 45.815% (5923/12928)\n",
      "Loss: 1.468 | Acc: 45.774% (5947/12992)\n",
      "Loss: 1.468 | Acc: 45.795% (5979/13056)\n",
      "Loss: 1.468 | Acc: 45.816% (6011/13120)\n",
      "Loss: 1.468 | Acc: 45.806% (6039/13184)\n",
      "Loss: 1.468 | Acc: 45.765% (6063/13248)\n",
      "Loss: 1.468 | Acc: 45.763% (6092/13312)\n",
      "Loss: 1.468 | Acc: 45.798% (6126/13376)\n",
      "Loss: 1.469 | Acc: 45.804% (6156/13440)\n",
      "Loss: 1.468 | Acc: 45.816% (6187/13504)\n",
      "Loss: 1.469 | Acc: 45.821% (6217/13568)\n",
      "Loss: 1.468 | Acc: 45.819% (6246/13632)\n",
      "Loss: 1.469 | Acc: 45.758% (6267/13696)\n",
      "Loss: 1.469 | Acc: 45.727% (6292/13760)\n",
      "Loss: 1.468 | Acc: 45.754% (6325/13824)\n",
      "Loss: 1.469 | Acc: 45.752% (6354/13888)\n",
      "Loss: 1.469 | Acc: 45.743% (6382/13952)\n",
      "Loss: 1.469 | Acc: 45.712% (6407/14016)\n",
      "Loss: 1.469 | Acc: 45.661% (6429/14080)\n",
      "Loss: 1.470 | Acc: 45.673% (6460/14144)\n",
      "Loss: 1.470 | Acc: 45.643% (6485/14208)\n",
      "Loss: 1.470 | Acc: 45.649% (6515/14272)\n",
      "Loss: 1.470 | Acc: 45.703% (6552/14336)\n",
      "Loss: 1.468 | Acc: 45.806% (6596/14400)\n",
      "Loss: 1.467 | Acc: 45.893% (6638/14464)\n",
      "Loss: 1.467 | Acc: 45.911% (6670/14528)\n",
      "Loss: 1.467 | Acc: 45.874% (6694/14592)\n",
      "Loss: 1.468 | Acc: 45.858% (6721/14656)\n",
      "Loss: 1.469 | Acc: 45.808% (6743/14720)\n",
      "Loss: 1.469 | Acc: 45.840% (6777/14784)\n",
      "Loss: 1.469 | Acc: 45.858% (6809/14848)\n",
      "Loss: 1.469 | Acc: 45.883% (6842/14912)\n",
      "Loss: 1.470 | Acc: 45.860% (6868/14976)\n",
      "Loss: 1.471 | Acc: 45.824% (6892/15040)\n",
      "Loss: 1.470 | Acc: 45.849% (6925/15104)\n",
      "Loss: 1.470 | Acc: 45.873% (6958/15168)\n",
      "Loss: 1.469 | Acc: 45.851% (6984/15232)\n",
      "Loss: 1.470 | Acc: 45.842% (7012/15296)\n",
      "Loss: 1.469 | Acc: 45.840% (7041/15360)\n",
      "Loss: 1.469 | Acc: 45.883% (7077/15424)\n",
      "Loss: 1.468 | Acc: 45.913% (7111/15488)\n",
      "Loss: 1.468 | Acc: 45.878% (7135/15552)\n",
      "Loss: 1.468 | Acc: 45.889% (7166/15616)\n",
      "Loss: 1.468 | Acc: 45.912% (7199/15680)\n",
      "Loss: 1.468 | Acc: 45.948% (7234/15744)\n",
      "Loss: 1.467 | Acc: 45.958% (7265/15808)\n",
      "Loss: 1.468 | Acc: 45.936% (7291/15872)\n",
      "Loss: 1.468 | Acc: 45.953% (7323/15936)\n",
      "Loss: 1.468 | Acc: 45.944% (7351/16000)\n",
      "Loss: 1.468 | Acc: 45.929% (7378/16064)\n",
      "Loss: 1.468 | Acc: 45.920% (7406/16128)\n",
      "Loss: 1.467 | Acc: 45.942% (7439/16192)\n",
      "Loss: 1.468 | Acc: 45.952% (7470/16256)\n",
      "Loss: 1.468 | Acc: 45.962% (7501/16320)\n",
      "Loss: 1.468 | Acc: 45.990% (7535/16384)\n",
      "Loss: 1.468 | Acc: 45.975% (7562/16448)\n",
      "Loss: 1.469 | Acc: 45.979% (7592/16512)\n",
      "Loss: 1.469 | Acc: 45.970% (7620/16576)\n",
      "Loss: 1.469 | Acc: 46.010% (7656/16640)\n",
      "Loss: 1.468 | Acc: 46.037% (7690/16704)\n",
      "Loss: 1.468 | Acc: 46.040% (7720/16768)\n",
      "Loss: 1.470 | Acc: 45.990% (7741/16832)\n",
      "Loss: 1.470 | Acc: 45.999% (7772/16896)\n",
      "Loss: 1.470 | Acc: 45.996% (7801/16960)\n",
      "Loss: 1.470 | Acc: 45.976% (7827/17024)\n",
      "Loss: 1.471 | Acc: 45.933% (7849/17088)\n",
      "Loss: 1.471 | Acc: 45.919% (7876/17152)\n",
      "Loss: 1.471 | Acc: 45.917% (7905/17216)\n",
      "Loss: 1.471 | Acc: 45.949% (7940/17280)\n",
      "Loss: 1.470 | Acc: 45.952% (7970/17344)\n",
      "Loss: 1.470 | Acc: 45.967% (8002/17408)\n",
      "Loss: 1.469 | Acc: 46.005% (8038/17472)\n",
      "Loss: 1.470 | Acc: 45.997% (8066/17536)\n",
      "Loss: 1.470 | Acc: 45.989% (8094/17600)\n",
      "Loss: 1.470 | Acc: 45.986% (8123/17664)\n",
      "Loss: 1.470 | Acc: 46.001% (8155/17728)\n",
      "Loss: 1.470 | Acc: 46.021% (8188/17792)\n",
      "Loss: 1.469 | Acc: 46.052% (8223/17856)\n",
      "Loss: 1.469 | Acc: 46.066% (8255/17920)\n",
      "Loss: 1.469 | Acc: 46.041% (8280/17984)\n",
      "Loss: 1.468 | Acc: 46.072% (8315/18048)\n",
      "Loss: 1.468 | Acc: 46.080% (8346/18112)\n",
      "Loss: 1.468 | Acc: 46.088% (8377/18176)\n",
      "Loss: 1.468 | Acc: 46.091% (8407/18240)\n",
      "Loss: 1.469 | Acc: 46.083% (8435/18304)\n",
      "Loss: 1.468 | Acc: 46.113% (8470/18368)\n",
      "Loss: 1.468 | Acc: 46.088% (8495/18432)\n",
      "Loss: 1.467 | Acc: 46.091% (8525/18496)\n",
      "Loss: 1.467 | Acc: 46.094% (8555/18560)\n",
      "Loss: 1.467 | Acc: 46.059% (8578/18624)\n",
      "Loss: 1.467 | Acc: 46.056% (8607/18688)\n",
      "Loss: 1.466 | Acc: 46.102% (8645/18752)\n",
      "Loss: 1.466 | Acc: 46.099% (8674/18816)\n",
      "Loss: 1.466 | Acc: 46.096% (8703/18880)\n",
      "Loss: 1.466 | Acc: 46.099% (8733/18944)\n",
      "Loss: 1.466 | Acc: 46.081% (8759/19008)\n",
      "Loss: 1.465 | Acc: 46.130% (8798/19072)\n",
      "Loss: 1.465 | Acc: 46.128% (8827/19136)\n",
      "Loss: 1.465 | Acc: 46.146% (8860/19200)\n",
      "Loss: 1.465 | Acc: 46.185% (8897/19264)\n",
      "Loss: 1.465 | Acc: 46.161% (8922/19328)\n",
      "Loss: 1.466 | Acc: 46.148% (8949/19392)\n",
      "Loss: 1.466 | Acc: 46.130% (8975/19456)\n",
      "Loss: 1.466 | Acc: 46.122% (9003/19520)\n",
      "Loss: 1.466 | Acc: 46.140% (9036/19584)\n",
      "Loss: 1.467 | Acc: 46.142% (9066/19648)\n",
      "Loss: 1.466 | Acc: 46.165% (9100/19712)\n",
      "Loss: 1.467 | Acc: 46.111% (9119/19776)\n",
      "Loss: 1.467 | Acc: 46.109% (9148/19840)\n",
      "Loss: 1.467 | Acc: 46.106% (9177/19904)\n",
      "Loss: 1.467 | Acc: 46.079% (9201/19968)\n",
      "Loss: 1.467 | Acc: 46.091% (9233/20032)\n",
      "Loss: 1.467 | Acc: 46.099% (9264/20096)\n",
      "Loss: 1.467 | Acc: 46.096% (9293/20160)\n",
      "Loss: 1.467 | Acc: 46.099% (9323/20224)\n",
      "Loss: 1.466 | Acc: 46.116% (9356/20288)\n",
      "Loss: 1.466 | Acc: 46.123% (9387/20352)\n",
      "Loss: 1.466 | Acc: 46.145% (9421/20416)\n",
      "Loss: 1.467 | Acc: 46.108% (9443/20480)\n",
      "Loss: 1.467 | Acc: 46.091% (9469/20544)\n",
      "Loss: 1.467 | Acc: 46.094% (9499/20608)\n",
      "Loss: 1.467 | Acc: 46.091% (9528/20672)\n",
      "Loss: 1.468 | Acc: 46.079% (9555/20736)\n",
      "Loss: 1.468 | Acc: 46.058% (9580/20800)\n",
      "Loss: 1.469 | Acc: 46.046% (9607/20864)\n",
      "Loss: 1.468 | Acc: 46.039% (9635/20928)\n",
      "Loss: 1.468 | Acc: 46.051% (9667/20992)\n",
      "Loss: 1.467 | Acc: 46.072% (9701/21056)\n",
      "Loss: 1.468 | Acc: 46.089% (9734/21120)\n",
      "Loss: 1.467 | Acc: 46.106% (9767/21184)\n",
      "Loss: 1.467 | Acc: 46.084% (9792/21248)\n",
      "Loss: 1.467 | Acc: 46.077% (9820/21312)\n",
      "Loss: 1.468 | Acc: 46.098% (9854/21376)\n",
      "Loss: 1.468 | Acc: 46.087% (9881/21440)\n",
      "Loss: 1.468 | Acc: 46.098% (9913/21504)\n",
      "Loss: 1.468 | Acc: 46.119% (9947/21568)\n",
      "Loss: 1.467 | Acc: 46.145% (9982/21632)\n",
      "Loss: 1.467 | Acc: 46.156% (10014/21696)\n",
      "Loss: 1.467 | Acc: 46.158% (10044/21760)\n",
      "Loss: 1.467 | Acc: 46.169% (10076/21824)\n",
      "Loss: 1.467 | Acc: 46.167% (10105/21888)\n",
      "Loss: 1.466 | Acc: 46.214% (10145/21952)\n",
      "Loss: 1.466 | Acc: 46.207% (10173/22016)\n",
      "Loss: 1.465 | Acc: 46.236% (10209/22080)\n",
      "Loss: 1.466 | Acc: 46.220% (10235/22144)\n",
      "Loss: 1.466 | Acc: 46.200% (10260/22208)\n",
      "Loss: 1.466 | Acc: 46.202% (10290/22272)\n",
      "Loss: 1.466 | Acc: 46.212% (10322/22336)\n",
      "Loss: 1.466 | Acc: 46.237% (10357/22400)\n",
      "Loss: 1.466 | Acc: 46.247% (10389/22464)\n",
      "Loss: 1.466 | Acc: 46.258% (10421/22528)\n",
      "Loss: 1.466 | Acc: 46.264% (10452/22592)\n",
      "Loss: 1.466 | Acc: 46.244% (10477/22656)\n",
      "Loss: 1.466 | Acc: 46.219% (10501/22720)\n",
      "Loss: 1.466 | Acc: 46.217% (10530/22784)\n",
      "Loss: 1.466 | Acc: 46.223% (10561/22848)\n",
      "Loss: 1.465 | Acc: 46.238% (10594/22912)\n",
      "Loss: 1.466 | Acc: 46.248% (10626/22976)\n",
      "Loss: 1.466 | Acc: 46.237% (10653/23040)\n",
      "Loss: 1.467 | Acc: 46.243% (10684/23104)\n",
      "Loss: 1.466 | Acc: 46.236% (10712/23168)\n",
      "Loss: 1.466 | Acc: 46.229% (10740/23232)\n",
      "Loss: 1.467 | Acc: 46.231% (10770/23296)\n",
      "Loss: 1.467 | Acc: 46.224% (10798/23360)\n",
      "Loss: 1.467 | Acc: 46.230% (10829/23424)\n",
      "Loss: 1.467 | Acc: 46.219% (10856/23488)\n",
      "Loss: 1.467 | Acc: 46.251% (10893/23552)\n",
      "Loss: 1.467 | Acc: 46.248% (10922/23616)\n",
      "Loss: 1.467 | Acc: 46.216% (10944/23680)\n",
      "Loss: 1.467 | Acc: 46.231% (10977/23744)\n",
      "Loss: 1.467 | Acc: 46.228% (11006/23808)\n",
      "Loss: 1.467 | Acc: 46.230% (11036/23872)\n",
      "Loss: 1.467 | Acc: 46.236% (11067/23936)\n",
      "Loss: 1.467 | Acc: 46.242% (11098/24000)\n",
      "Loss: 1.465 | Acc: 46.281% (11137/24064)\n",
      "Loss: 1.466 | Acc: 46.253% (11160/24128)\n",
      "Loss: 1.466 | Acc: 46.284% (11197/24192)\n",
      "Loss: 1.465 | Acc: 46.290% (11228/24256)\n",
      "Loss: 1.466 | Acc: 46.262% (11251/24320)\n",
      "Loss: 1.466 | Acc: 46.223% (11271/24384)\n",
      "Loss: 1.466 | Acc: 46.233% (11303/24448)\n",
      "Loss: 1.466 | Acc: 46.234% (11333/24512)\n",
      "Loss: 1.466 | Acc: 46.232% (11362/24576)\n",
      "Loss: 1.466 | Acc: 46.258% (11398/24640)\n",
      "Loss: 1.466 | Acc: 46.244% (11424/24704)\n",
      "Loss: 1.466 | Acc: 46.241% (11453/24768)\n",
      "Loss: 1.466 | Acc: 46.247% (11484/24832)\n",
      "Loss: 1.465 | Acc: 46.260% (11517/24896)\n",
      "Loss: 1.465 | Acc: 46.282% (11552/24960)\n",
      "Loss: 1.465 | Acc: 46.288% (11583/25024)\n",
      "Loss: 1.465 | Acc: 46.305% (11617/25088)\n",
      "Loss: 1.465 | Acc: 46.287% (11642/25152)\n",
      "Loss: 1.465 | Acc: 46.292% (11673/25216)\n",
      "Loss: 1.466 | Acc: 46.258% (11694/25280)\n",
      "Loss: 1.465 | Acc: 46.319% (11739/25344)\n",
      "Loss: 1.465 | Acc: 46.316% (11768/25408)\n",
      "Loss: 1.465 | Acc: 46.306% (11795/25472)\n",
      "Loss: 1.466 | Acc: 46.284% (11819/25536)\n",
      "Loss: 1.465 | Acc: 46.316% (11857/25600)\n",
      "Loss: 1.465 | Acc: 46.326% (11889/25664)\n",
      "Loss: 1.465 | Acc: 46.315% (11916/25728)\n",
      "Loss: 1.465 | Acc: 46.297% (11941/25792)\n",
      "Loss: 1.464 | Acc: 46.322% (11977/25856)\n",
      "Loss: 1.465 | Acc: 46.289% (11998/25920)\n",
      "Loss: 1.465 | Acc: 46.263% (12021/25984)\n",
      "Loss: 1.466 | Acc: 46.219% (12039/26048)\n",
      "Loss: 1.466 | Acc: 46.205% (12065/26112)\n",
      "Loss: 1.466 | Acc: 46.195% (12092/26176)\n",
      "Loss: 1.466 | Acc: 46.200% (12123/26240)\n",
      "Loss: 1.466 | Acc: 46.206% (12154/26304)\n",
      "Loss: 1.466 | Acc: 46.189% (12179/26368)\n",
      "Loss: 1.466 | Acc: 46.220% (12217/26432)\n",
      "Loss: 1.466 | Acc: 46.222% (12247/26496)\n",
      "Loss: 1.466 | Acc: 46.231% (12279/26560)\n",
      "Loss: 1.466 | Acc: 46.229% (12308/26624)\n",
      "Loss: 1.466 | Acc: 46.212% (12333/26688)\n",
      "Loss: 1.466 | Acc: 46.180% (12354/26752)\n",
      "Loss: 1.467 | Acc: 46.174% (12382/26816)\n",
      "Loss: 1.467 | Acc: 46.168% (12410/26880)\n",
      "Loss: 1.467 | Acc: 46.177% (12442/26944)\n",
      "Loss: 1.467 | Acc: 46.164% (12468/27008)\n",
      "Loss: 1.467 | Acc: 46.169% (12499/27072)\n",
      "Loss: 1.467 | Acc: 46.145% (12522/27136)\n",
      "Loss: 1.467 | Acc: 46.143% (12551/27200)\n",
      "Loss: 1.467 | Acc: 46.141% (12580/27264)\n",
      "Loss: 1.467 | Acc: 46.129% (12606/27328)\n",
      "Loss: 1.468 | Acc: 46.105% (12629/27392)\n",
      "Loss: 1.468 | Acc: 46.103% (12658/27456)\n",
      "Loss: 1.468 | Acc: 46.083% (12682/27520)\n",
      "Loss: 1.468 | Acc: 46.088% (12713/27584)\n",
      "Loss: 1.468 | Acc: 46.058% (12734/27648)\n",
      "Loss: 1.468 | Acc: 46.063% (12765/27712)\n",
      "Loss: 1.468 | Acc: 46.069% (12796/27776)\n",
      "Loss: 1.467 | Acc: 46.092% (12832/27840)\n",
      "Loss: 1.467 | Acc: 46.119% (12869/27904)\n",
      "Loss: 1.467 | Acc: 46.106% (12895/27968)\n",
      "Loss: 1.467 | Acc: 46.104% (12924/28032)\n",
      "Loss: 1.467 | Acc: 46.103% (12953/28096)\n",
      "Loss: 1.467 | Acc: 46.126% (12989/28160)\n",
      "Loss: 1.467 | Acc: 46.117% (13016/28224)\n",
      "Loss: 1.467 | Acc: 46.129% (13049/28288)\n",
      "Loss: 1.467 | Acc: 46.127% (13078/28352)\n",
      "Loss: 1.467 | Acc: 46.125% (13107/28416)\n",
      "Loss: 1.467 | Acc: 46.113% (13133/28480)\n",
      "Loss: 1.467 | Acc: 46.111% (13162/28544)\n",
      "Loss: 1.467 | Acc: 46.102% (13189/28608)\n",
      "Loss: 1.467 | Acc: 46.094% (13216/28672)\n",
      "Loss: 1.468 | Acc: 46.082% (13242/28736)\n",
      "Loss: 1.468 | Acc: 46.090% (13274/28800)\n",
      "Loss: 1.467 | Acc: 46.102% (13307/28864)\n",
      "Loss: 1.467 | Acc: 46.104% (13337/28928)\n",
      "Loss: 1.467 | Acc: 46.113% (13369/28992)\n",
      "Loss: 1.467 | Acc: 46.111% (13398/29056)\n",
      "Loss: 1.466 | Acc: 46.130% (13433/29120)\n",
      "Loss: 1.466 | Acc: 46.128% (13462/29184)\n",
      "Loss: 1.466 | Acc: 46.130% (13492/29248)\n",
      "Loss: 1.466 | Acc: 46.145% (13526/29312)\n",
      "Loss: 1.466 | Acc: 46.164% (13561/29376)\n",
      "Loss: 1.465 | Acc: 46.199% (13601/29440)\n",
      "Loss: 1.465 | Acc: 46.204% (13632/29504)\n",
      "Loss: 1.466 | Acc: 46.188% (13657/29568)\n",
      "Loss: 1.466 | Acc: 46.193% (13688/29632)\n",
      "Loss: 1.466 | Acc: 46.202% (13720/29696)\n",
      "Loss: 1.465 | Acc: 46.223% (13756/29760)\n",
      "Loss: 1.465 | Acc: 46.218% (13784/29824)\n",
      "Loss: 1.465 | Acc: 46.233% (13818/29888)\n",
      "Loss: 1.465 | Acc: 46.234% (13848/29952)\n",
      "Loss: 1.465 | Acc: 46.209% (13870/30016)\n",
      "Loss: 1.465 | Acc: 46.240% (13909/30080)\n",
      "Loss: 1.465 | Acc: 46.235% (13937/30144)\n",
      "Loss: 1.465 | Acc: 46.203% (13957/30208)\n",
      "Loss: 1.465 | Acc: 46.208% (13988/30272)\n",
      "Loss: 1.464 | Acc: 46.219% (14021/30336)\n",
      "Loss: 1.464 | Acc: 46.214% (14049/30400)\n",
      "Loss: 1.464 | Acc: 46.228% (14083/30464)\n",
      "Loss: 1.464 | Acc: 46.210% (14107/30528)\n",
      "Loss: 1.464 | Acc: 46.228% (14142/30592)\n",
      "Loss: 1.464 | Acc: 46.232% (14173/30656)\n",
      "Loss: 1.464 | Acc: 46.230% (14202/30720)\n",
      "Loss: 1.464 | Acc: 46.225% (14230/30784)\n",
      "Loss: 1.464 | Acc: 46.240% (14264/30848)\n",
      "Loss: 1.464 | Acc: 46.241% (14294/30912)\n",
      "Loss: 1.464 | Acc: 46.249% (14326/30976)\n",
      "Loss: 1.464 | Acc: 46.247% (14355/31040)\n",
      "Loss: 1.464 | Acc: 46.242% (14383/31104)\n",
      "Loss: 1.464 | Acc: 46.237% (14411/31168)\n",
      "Loss: 1.463 | Acc: 46.251% (14445/31232)\n",
      "Loss: 1.464 | Acc: 46.239% (14471/31296)\n",
      "Loss: 1.464 | Acc: 46.234% (14499/31360)\n",
      "Loss: 1.463 | Acc: 46.264% (14538/31424)\n",
      "Loss: 1.463 | Acc: 46.259% (14566/31488)\n",
      "Loss: 1.463 | Acc: 46.270% (14599/31552)\n",
      "Loss: 1.463 | Acc: 46.277% (14631/31616)\n",
      "Loss: 1.463 | Acc: 46.275% (14660/31680)\n",
      "Loss: 1.462 | Acc: 46.292% (14695/31744)\n",
      "Loss: 1.461 | Acc: 46.337% (14739/31808)\n",
      "Loss: 1.462 | Acc: 46.338% (14769/31872)\n",
      "Loss: 1.461 | Acc: 46.361% (14806/31936)\n",
      "Loss: 1.461 | Acc: 46.388% (14844/32000)\n",
      "Loss: 1.461 | Acc: 46.385% (14873/32064)\n",
      "Loss: 1.460 | Acc: 46.408% (14910/32128)\n",
      "Loss: 1.460 | Acc: 46.412% (14941/32192)\n",
      "Loss: 1.460 | Acc: 46.407% (14969/32256)\n",
      "Loss: 1.460 | Acc: 46.402% (14997/32320)\n",
      "Loss: 1.460 | Acc: 46.415% (15031/32384)\n",
      "Loss: 1.460 | Acc: 46.403% (15057/32448)\n",
      "Loss: 1.460 | Acc: 46.404% (15087/32512)\n",
      "Loss: 1.459 | Acc: 46.415% (15120/32576)\n",
      "Loss: 1.459 | Acc: 46.419% (15151/32640)\n",
      "Loss: 1.459 | Acc: 46.413% (15179/32704)\n",
      "Loss: 1.459 | Acc: 46.420% (15211/32768)\n",
      "Loss: 1.459 | Acc: 46.412% (15238/32832)\n",
      "Loss: 1.459 | Acc: 46.407% (15266/32896)\n",
      "Loss: 1.459 | Acc: 46.414% (15298/32960)\n",
      "Loss: 1.459 | Acc: 46.421% (15330/33024)\n",
      "Loss: 1.459 | Acc: 46.431% (15363/33088)\n",
      "Loss: 1.459 | Acc: 46.441% (15396/33152)\n",
      "Loss: 1.459 | Acc: 46.447% (15428/33216)\n",
      "Loss: 1.459 | Acc: 46.445% (15457/33280)\n",
      "Loss: 1.459 | Acc: 46.437% (15484/33344)\n",
      "Loss: 1.459 | Acc: 46.444% (15516/33408)\n",
      "Loss: 1.459 | Acc: 46.451% (15548/33472)\n",
      "Loss: 1.459 | Acc: 46.437% (15573/33536)\n",
      "Loss: 1.459 | Acc: 46.443% (15605/33600)\n",
      "Loss: 1.459 | Acc: 46.450% (15637/33664)\n",
      "Loss: 1.459 | Acc: 46.442% (15664/33728)\n",
      "Loss: 1.459 | Acc: 46.437% (15692/33792)\n",
      "Loss: 1.459 | Acc: 46.464% (15731/33856)\n",
      "Loss: 1.458 | Acc: 46.471% (15763/33920)\n",
      "Loss: 1.458 | Acc: 46.472% (15793/33984)\n",
      "Loss: 1.458 | Acc: 46.487% (15828/34048)\n",
      "Loss: 1.458 | Acc: 46.491% (15859/34112)\n",
      "Loss: 1.458 | Acc: 46.489% (15888/34176)\n",
      "Loss: 1.458 | Acc: 46.510% (15925/34240)\n",
      "Loss: 1.458 | Acc: 46.525% (15960/34304)\n",
      "Loss: 1.457 | Acc: 46.549% (15998/34368)\n",
      "Loss: 1.457 | Acc: 46.570% (16035/34432)\n",
      "Loss: 1.457 | Acc: 46.576% (16067/34496)\n",
      "Loss: 1.456 | Acc: 46.574% (16096/34560)\n",
      "Loss: 1.456 | Acc: 46.592% (16132/34624)\n",
      "Loss: 1.456 | Acc: 46.578% (16157/34688)\n",
      "Loss: 1.455 | Acc: 46.584% (16189/34752)\n",
      "Loss: 1.455 | Acc: 46.591% (16221/34816)\n",
      "Loss: 1.455 | Acc: 46.585% (16249/34880)\n",
      "Loss: 1.455 | Acc: 46.595% (16282/34944)\n",
      "Loss: 1.455 | Acc: 46.609% (16317/35008)\n",
      "Loss: 1.455 | Acc: 46.618% (16350/35072)\n",
      "Loss: 1.455 | Acc: 46.610% (16377/35136)\n",
      "Loss: 1.455 | Acc: 46.622% (16411/35200)\n",
      "Loss: 1.455 | Acc: 46.628% (16443/35264)\n",
      "Loss: 1.454 | Acc: 46.646% (16479/35328)\n",
      "Loss: 1.454 | Acc: 46.646% (16509/35392)\n",
      "Loss: 1.454 | Acc: 46.658% (16543/35456)\n",
      "Loss: 1.455 | Acc: 46.622% (16560/35520)\n",
      "Loss: 1.455 | Acc: 46.619% (16589/35584)\n",
      "Loss: 1.455 | Acc: 46.628% (16622/35648)\n",
      "Loss: 1.454 | Acc: 46.626% (16651/35712)\n",
      "Loss: 1.455 | Acc: 46.623% (16680/35776)\n",
      "Loss: 1.454 | Acc: 46.638% (16715/35840)\n",
      "Loss: 1.454 | Acc: 46.630% (16742/35904)\n",
      "Loss: 1.454 | Acc: 46.641% (16776/35968)\n",
      "Loss: 1.455 | Acc: 46.631% (16802/36032)\n",
      "Loss: 1.455 | Acc: 46.623% (16829/36096)\n",
      "Loss: 1.455 | Acc: 46.637% (16864/36160)\n",
      "Loss: 1.454 | Acc: 46.640% (16895/36224)\n",
      "Loss: 1.454 | Acc: 46.649% (16928/36288)\n",
      "Loss: 1.454 | Acc: 46.638% (16954/36352)\n",
      "Loss: 1.454 | Acc: 46.666% (16994/36416)\n",
      "Loss: 1.454 | Acc: 46.672% (17026/36480)\n",
      "Loss: 1.453 | Acc: 46.664% (17053/36544)\n",
      "Loss: 1.454 | Acc: 46.656% (17080/36608)\n",
      "Loss: 1.454 | Acc: 46.662% (17112/36672)\n",
      "Loss: 1.454 | Acc: 46.665% (17143/36736)\n",
      "Loss: 1.454 | Acc: 46.668% (17174/36800)\n",
      "Loss: 1.454 | Acc: 46.672% (17205/36864)\n",
      "Loss: 1.454 | Acc: 46.661% (17231/36928)\n",
      "Loss: 1.454 | Acc: 46.664% (17262/36992)\n",
      "Loss: 1.454 | Acc: 46.665% (17292/37056)\n",
      "Loss: 1.454 | Acc: 46.665% (17322/37120)\n",
      "Loss: 1.454 | Acc: 46.668% (17353/37184)\n",
      "Loss: 1.453 | Acc: 46.668% (17383/37248)\n",
      "Loss: 1.453 | Acc: 46.671% (17414/37312)\n",
      "Loss: 1.453 | Acc: 46.677% (17446/37376)\n",
      "Loss: 1.454 | Acc: 46.685% (17479/37440)\n",
      "Loss: 1.453 | Acc: 46.696% (17513/37504)\n",
      "Loss: 1.453 | Acc: 46.694% (17542/37568)\n",
      "Loss: 1.453 | Acc: 46.700% (17574/37632)\n",
      "Loss: 1.453 | Acc: 46.700% (17604/37696)\n",
      "Loss: 1.452 | Acc: 46.711% (17638/37760)\n",
      "Loss: 1.453 | Acc: 46.711% (17668/37824)\n",
      "Loss: 1.452 | Acc: 46.722% (17702/37888)\n",
      "Loss: 1.452 | Acc: 46.720% (17731/37952)\n",
      "Loss: 1.452 | Acc: 46.730% (17765/38016)\n",
      "Loss: 1.453 | Acc: 46.725% (17793/38080)\n",
      "Loss: 1.452 | Acc: 46.731% (17825/38144)\n",
      "Loss: 1.452 | Acc: 46.731% (17855/38208)\n",
      "Loss: 1.452 | Acc: 46.731% (17885/38272)\n",
      "Loss: 1.452 | Acc: 46.752% (17923/38336)\n",
      "Loss: 1.452 | Acc: 46.755% (17954/38400)\n",
      "Loss: 1.452 | Acc: 46.763% (17987/38464)\n",
      "Loss: 1.452 | Acc: 46.761% (18016/38528)\n",
      "Loss: 1.452 | Acc: 46.764% (18047/38592)\n",
      "Loss: 1.452 | Acc: 46.779% (18083/38656)\n",
      "Loss: 1.452 | Acc: 46.772% (18110/38720)\n",
      "Loss: 1.451 | Acc: 46.790% (18147/38784)\n",
      "Loss: 1.451 | Acc: 46.800% (18181/38848)\n",
      "Loss: 1.451 | Acc: 46.811% (18215/38912)\n",
      "Loss: 1.451 | Acc: 46.808% (18244/38976)\n",
      "Loss: 1.451 | Acc: 46.821% (18279/39040)\n",
      "Loss: 1.450 | Acc: 46.834% (18314/39104)\n",
      "Loss: 1.450 | Acc: 46.832% (18343/39168)\n",
      "Loss: 1.450 | Acc: 46.832% (18373/39232)\n",
      "Loss: 1.450 | Acc: 46.834% (18404/39296)\n",
      "Loss: 1.450 | Acc: 46.845% (18438/39360)\n",
      "Loss: 1.450 | Acc: 46.832% (18463/39424)\n",
      "Loss: 1.450 | Acc: 46.837% (18495/39488)\n",
      "Loss: 1.450 | Acc: 46.829% (18522/39552)\n",
      "Loss: 1.450 | Acc: 46.825% (18550/39616)\n",
      "Loss: 1.450 | Acc: 46.830% (18582/39680)\n",
      "Loss: 1.450 | Acc: 46.827% (18611/39744)\n",
      "Loss: 1.450 | Acc: 46.812% (18635/39808)\n",
      "Loss: 1.450 | Acc: 46.832% (18673/39872)\n",
      "Loss: 1.450 | Acc: 46.845% (18708/39936)\n",
      "Loss: 1.450 | Acc: 46.840% (18736/40000)\n",
      "Loss: 1.450 | Acc: 46.833% (18763/40064)\n",
      "Loss: 1.450 | Acc: 46.840% (18796/40128)\n",
      "Loss: 1.450 | Acc: 46.848% (18829/40192)\n",
      "Loss: 1.450 | Acc: 46.855% (18862/40256)\n",
      "Loss: 1.450 | Acc: 46.870% (18898/40320)\n",
      "Loss: 1.449 | Acc: 46.870% (18928/40384)\n",
      "Loss: 1.449 | Acc: 46.877% (18961/40448)\n",
      "Loss: 1.449 | Acc: 46.897% (18999/40512)\n",
      "Loss: 1.449 | Acc: 46.902% (19031/40576)\n",
      "Loss: 1.449 | Acc: 46.890% (19056/40640)\n",
      "Loss: 1.448 | Acc: 46.922% (19099/40704)\n",
      "Loss: 1.448 | Acc: 46.927% (19131/40768)\n",
      "Loss: 1.448 | Acc: 46.936% (19165/40832)\n",
      "Loss: 1.448 | Acc: 46.931% (19193/40896)\n",
      "Loss: 1.448 | Acc: 46.934% (19224/40960)\n",
      "Loss: 1.448 | Acc: 46.953% (19262/41024)\n",
      "Loss: 1.447 | Acc: 46.970% (19299/41088)\n",
      "Loss: 1.447 | Acc: 46.982% (19334/41152)\n",
      "Loss: 1.447 | Acc: 46.987% (19366/41216)\n",
      "Loss: 1.447 | Acc: 46.982% (19394/41280)\n",
      "Loss: 1.446 | Acc: 46.991% (19428/41344)\n",
      "Loss: 1.446 | Acc: 47.005% (19464/41408)\n",
      "Loss: 1.446 | Acc: 47.020% (19500/41472)\n",
      "Loss: 1.446 | Acc: 47.022% (19531/41536)\n",
      "Loss: 1.446 | Acc: 47.019% (19560/41600)\n",
      "Loss: 1.446 | Acc: 47.019% (19590/41664)\n",
      "Loss: 1.446 | Acc: 47.014% (19618/41728)\n",
      "Loss: 1.446 | Acc: 47.021% (19651/41792)\n",
      "Loss: 1.446 | Acc: 47.016% (19679/41856)\n",
      "Loss: 1.445 | Acc: 47.032% (19716/41920)\n",
      "Loss: 1.445 | Acc: 47.035% (19747/41984)\n",
      "Loss: 1.445 | Acc: 47.039% (19779/42048)\n",
      "Loss: 1.445 | Acc: 47.044% (19811/42112)\n",
      "Loss: 1.445 | Acc: 47.050% (19844/42176)\n",
      "Loss: 1.445 | Acc: 47.055% (19876/42240)\n",
      "Loss: 1.445 | Acc: 47.069% (19912/42304)\n",
      "Loss: 1.444 | Acc: 47.076% (19945/42368)\n",
      "Loss: 1.445 | Acc: 47.075% (19975/42432)\n",
      "Loss: 1.445 | Acc: 47.070% (20003/42496)\n",
      "Loss: 1.445 | Acc: 47.072% (20034/42560)\n",
      "Loss: 1.445 | Acc: 47.072% (20064/42624)\n",
      "Loss: 1.444 | Acc: 47.095% (20104/42688)\n",
      "Loss: 1.444 | Acc: 47.097% (20135/42752)\n",
      "Loss: 1.444 | Acc: 47.095% (20164/42816)\n",
      "Loss: 1.444 | Acc: 47.104% (20198/42880)\n",
      "Loss: 1.444 | Acc: 47.092% (20223/42944)\n",
      "Loss: 1.444 | Acc: 47.077% (20247/43008)\n",
      "Loss: 1.444 | Acc: 47.070% (20274/43072)\n",
      "Loss: 1.444 | Acc: 47.072% (20305/43136)\n",
      "Loss: 1.444 | Acc: 47.067% (20333/43200)\n",
      "Loss: 1.444 | Acc: 47.058% (20359/43264)\n",
      "Loss: 1.444 | Acc: 47.055% (20388/43328)\n",
      "Loss: 1.444 | Acc: 47.062% (20421/43392)\n",
      "Loss: 1.444 | Acc: 47.078% (20458/43456)\n",
      "Loss: 1.444 | Acc: 47.091% (20494/43520)\n",
      "Loss: 1.444 | Acc: 47.088% (20523/43584)\n",
      "Loss: 1.444 | Acc: 47.088% (20553/43648)\n",
      "Loss: 1.444 | Acc: 47.104% (20590/43712)\n",
      "Loss: 1.444 | Acc: 47.094% (20616/43776)\n",
      "Loss: 1.444 | Acc: 47.101% (20649/43840)\n",
      "Loss: 1.444 | Acc: 47.112% (20684/43904)\n",
      "Loss: 1.443 | Acc: 47.123% (20719/43968)\n",
      "Loss: 1.443 | Acc: 47.136% (20755/44032)\n",
      "Loss: 1.443 | Acc: 47.131% (20783/44096)\n",
      "Loss: 1.443 | Acc: 47.120% (20808/44160)\n",
      "Loss: 1.443 | Acc: 47.108% (20833/44224)\n",
      "Loss: 1.443 | Acc: 47.105% (20862/44288)\n",
      "Loss: 1.443 | Acc: 47.109% (20894/44352)\n",
      "Loss: 1.442 | Acc: 47.114% (20926/44416)\n",
      "Loss: 1.442 | Acc: 47.122% (20960/44480)\n",
      "Loss: 1.442 | Acc: 47.133% (20995/44544)\n",
      "Loss: 1.443 | Acc: 47.117% (21018/44608)\n",
      "Loss: 1.442 | Acc: 47.128% (21053/44672)\n",
      "Loss: 1.442 | Acc: 47.123% (21081/44736)\n",
      "Loss: 1.442 | Acc: 47.114% (21107/44800)\n",
      "Loss: 1.442 | Acc: 47.127% (21143/44864)\n",
      "Loss: 1.442 | Acc: 47.122% (21171/44928)\n",
      "Loss: 1.442 | Acc: 47.124% (21202/44992)\n",
      "Loss: 1.442 | Acc: 47.135% (21237/45056)\n",
      "Loss: 1.441 | Acc: 47.134% (21267/45120)\n",
      "Loss: 1.441 | Acc: 47.145% (21302/45184)\n",
      "Loss: 1.441 | Acc: 47.147% (21333/45248)\n",
      "Loss: 1.441 | Acc: 47.164% (21371/45312)\n",
      "Loss: 1.440 | Acc: 47.159% (21399/45376)\n",
      "Loss: 1.441 | Acc: 47.157% (21428/45440)\n",
      "Loss: 1.441 | Acc: 47.148% (21454/45504)\n",
      "Loss: 1.441 | Acc: 47.162% (21491/45568)\n",
      "Loss: 1.441 | Acc: 47.162% (21521/45632)\n",
      "Loss: 1.441 | Acc: 47.151% (21546/45696)\n",
      "Loss: 1.440 | Acc: 47.166% (21583/45760)\n",
      "Loss: 1.440 | Acc: 47.163% (21612/45824)\n",
      "Loss: 1.440 | Acc: 47.160% (21641/45888)\n",
      "Loss: 1.440 | Acc: 47.158% (21670/45952)\n",
      "Loss: 1.440 | Acc: 47.158% (21700/46016)\n",
      "Loss: 1.440 | Acc: 47.159% (21731/46080)\n",
      "Loss: 1.440 | Acc: 47.152% (21758/46144)\n",
      "Loss: 1.440 | Acc: 47.156% (21790/46208)\n",
      "Loss: 1.440 | Acc: 47.143% (21814/46272)\n",
      "Loss: 1.440 | Acc: 47.138% (21842/46336)\n",
      "Loss: 1.440 | Acc: 47.149% (21877/46400)\n",
      "Loss: 1.440 | Acc: 47.155% (21910/46464)\n",
      "Loss: 1.439 | Acc: 47.169% (21947/46528)\n",
      "Loss: 1.439 | Acc: 47.188% (21986/46592)\n",
      "Loss: 1.439 | Acc: 47.199% (22021/46656)\n",
      "Loss: 1.439 | Acc: 47.220% (22061/46720)\n",
      "Loss: 1.439 | Acc: 47.223% (22093/46784)\n",
      "Loss: 1.439 | Acc: 47.234% (22128/46848)\n",
      "Loss: 1.438 | Acc: 47.242% (22162/46912)\n",
      "Loss: 1.439 | Acc: 47.241% (22192/46976)\n",
      "Loss: 1.439 | Acc: 47.251% (22227/47040)\n",
      "Loss: 1.438 | Acc: 47.257% (22260/47104)\n",
      "Loss: 1.438 | Acc: 47.259% (22291/47168)\n",
      "Loss: 1.438 | Acc: 47.269% (22326/47232)\n",
      "Loss: 1.438 | Acc: 47.266% (22355/47296)\n",
      "Loss: 1.438 | Acc: 47.266% (22385/47360)\n",
      "Loss: 1.438 | Acc: 47.261% (22413/47424)\n",
      "Loss: 1.438 | Acc: 47.271% (22448/47488)\n",
      "Loss: 1.438 | Acc: 47.270% (22478/47552)\n",
      "Loss: 1.438 | Acc: 47.268% (22507/47616)\n",
      "Loss: 1.438 | Acc: 47.273% (22540/47680)\n",
      "Loss: 1.438 | Acc: 47.277% (22572/47744)\n",
      "Loss: 1.438 | Acc: 47.264% (22596/47808)\n",
      "Loss: 1.438 | Acc: 47.266% (22627/47872)\n",
      "Loss: 1.438 | Acc: 47.263% (22656/47936)\n",
      "Loss: 1.438 | Acc: 47.256% (22683/48000)\n",
      "Loss: 1.438 | Acc: 47.250% (22710/48064)\n",
      "Loss: 1.438 | Acc: 47.239% (22735/48128)\n",
      "Loss: 1.438 | Acc: 47.249% (22770/48192)\n",
      "Loss: 1.438 | Acc: 47.254% (22803/48256)\n",
      "Loss: 1.438 | Acc: 47.248% (22830/48320)\n",
      "Loss: 1.438 | Acc: 47.253% (22863/48384)\n",
      "Loss: 1.438 | Acc: 47.251% (22892/48448)\n",
      "Loss: 1.438 | Acc: 47.269% (22931/48512)\n",
      "Loss: 1.438 | Acc: 47.283% (22968/48576)\n",
      "Loss: 1.437 | Acc: 47.282% (22998/48640)\n",
      "Loss: 1.438 | Acc: 47.284% (23029/48704)\n",
      "Loss: 1.437 | Acc: 47.291% (23063/48768)\n",
      "Loss: 1.437 | Acc: 47.287% (23091/48832)\n",
      "Loss: 1.437 | Acc: 47.307% (23131/48896)\n",
      "Loss: 1.437 | Acc: 47.304% (23160/48960)\n",
      "Loss: 1.437 | Acc: 47.306% (23180/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 47.30612244897959\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.268 | Acc: 48.438% (31/64)\n",
      "Loss: 1.274 | Acc: 51.562% (66/128)\n",
      "Loss: 1.302 | Acc: 51.042% (98/192)\n",
      "Loss: 1.382 | Acc: 48.828% (125/256)\n",
      "Loss: 1.336 | Acc: 50.000% (160/320)\n",
      "Loss: 1.344 | Acc: 50.260% (193/384)\n",
      "Loss: 1.372 | Acc: 48.884% (219/448)\n",
      "Loss: 1.366 | Acc: 49.414% (253/512)\n",
      "Loss: 1.344 | Acc: 50.868% (293/576)\n",
      "Loss: 1.334 | Acc: 51.406% (329/640)\n",
      "Loss: 1.363 | Acc: 50.142% (353/704)\n",
      "Loss: 1.364 | Acc: 50.260% (386/768)\n",
      "Loss: 1.364 | Acc: 50.240% (418/832)\n",
      "Loss: 1.356 | Acc: 50.893% (456/896)\n",
      "Loss: 1.352 | Acc: 50.833% (488/960)\n",
      "Loss: 1.348 | Acc: 51.562% (528/1024)\n",
      "Loss: 1.343 | Acc: 51.838% (564/1088)\n",
      "Loss: 1.344 | Acc: 51.649% (595/1152)\n",
      "Loss: 1.339 | Acc: 51.645% (628/1216)\n",
      "Loss: 1.348 | Acc: 51.172% (655/1280)\n",
      "Loss: 1.345 | Acc: 51.265% (689/1344)\n",
      "Loss: 1.345 | Acc: 51.136% (720/1408)\n",
      "Loss: 1.339 | Acc: 51.019% (751/1472)\n",
      "Loss: 1.342 | Acc: 50.781% (780/1536)\n",
      "Loss: 1.346 | Acc: 50.500% (808/1600)\n",
      "Loss: 1.351 | Acc: 50.481% (840/1664)\n",
      "Loss: 1.349 | Acc: 50.926% (880/1728)\n",
      "Loss: 1.351 | Acc: 50.949% (913/1792)\n",
      "Loss: 1.351 | Acc: 51.024% (947/1856)\n",
      "Loss: 1.350 | Acc: 51.302% (985/1920)\n",
      "Loss: 1.352 | Acc: 51.361% (1019/1984)\n",
      "Loss: 1.350 | Acc: 51.660% (1058/2048)\n",
      "Loss: 1.346 | Acc: 51.562% (1089/2112)\n",
      "Loss: 1.349 | Acc: 51.471% (1120/2176)\n",
      "Loss: 1.347 | Acc: 51.562% (1155/2240)\n",
      "Loss: 1.345 | Acc: 51.562% (1188/2304)\n",
      "Loss: 1.347 | Acc: 51.436% (1218/2368)\n",
      "Loss: 1.346 | Acc: 51.480% (1252/2432)\n",
      "Loss: 1.342 | Acc: 51.603% (1288/2496)\n",
      "Loss: 1.345 | Acc: 51.445% (1317/2560)\n",
      "Loss: 1.345 | Acc: 51.296% (1346/2624)\n",
      "Loss: 1.346 | Acc: 51.302% (1379/2688)\n",
      "Loss: 1.344 | Acc: 51.308% (1412/2752)\n",
      "Loss: 1.345 | Acc: 51.385% (1447/2816)\n",
      "Loss: 1.342 | Acc: 51.493% (1483/2880)\n",
      "Loss: 1.340 | Acc: 51.732% (1523/2944)\n",
      "Loss: 1.340 | Acc: 51.762% (1557/3008)\n",
      "Loss: 1.341 | Acc: 51.790% (1591/3072)\n",
      "Loss: 1.339 | Acc: 51.722% (1622/3136)\n",
      "Loss: 1.334 | Acc: 51.844% (1659/3200)\n",
      "Loss: 1.335 | Acc: 51.838% (1692/3264)\n",
      "Loss: 1.336 | Acc: 51.803% (1724/3328)\n",
      "Loss: 1.338 | Acc: 51.739% (1755/3392)\n",
      "Loss: 1.337 | Acc: 51.765% (1789/3456)\n",
      "Loss: 1.337 | Acc: 51.733% (1821/3520)\n",
      "Loss: 1.336 | Acc: 51.842% (1858/3584)\n",
      "Loss: 1.338 | Acc: 51.837% (1891/3648)\n",
      "Loss: 1.338 | Acc: 51.994% (1930/3712)\n",
      "Loss: 1.339 | Acc: 51.986% (1963/3776)\n",
      "Loss: 1.338 | Acc: 52.005% (1997/3840)\n",
      "Loss: 1.339 | Acc: 51.972% (2029/3904)\n",
      "Loss: 1.338 | Acc: 51.991% (2063/3968)\n",
      "Loss: 1.338 | Acc: 51.959% (2095/4032)\n",
      "Loss: 1.340 | Acc: 51.904% (2126/4096)\n",
      "Loss: 1.341 | Acc: 51.995% (2163/4160)\n",
      "Loss: 1.340 | Acc: 51.894% (2192/4224)\n",
      "Loss: 1.340 | Acc: 51.889% (2225/4288)\n",
      "Loss: 1.339 | Acc: 51.930% (2260/4352)\n",
      "Loss: 1.337 | Acc: 52.038% (2298/4416)\n",
      "Loss: 1.334 | Acc: 52.098% (2334/4480)\n",
      "Loss: 1.334 | Acc: 51.981% (2362/4544)\n",
      "Loss: 1.338 | Acc: 51.801% (2387/4608)\n",
      "Loss: 1.336 | Acc: 51.926% (2426/4672)\n",
      "Loss: 1.333 | Acc: 51.985% (2462/4736)\n",
      "Loss: 1.334 | Acc: 52.021% (2497/4800)\n",
      "Loss: 1.334 | Acc: 51.974% (2528/4864)\n",
      "Loss: 1.333 | Acc: 51.928% (2559/4928)\n",
      "Loss: 1.335 | Acc: 51.823% (2587/4992)\n",
      "Loss: 1.336 | Acc: 51.839% (2621/5056)\n",
      "Loss: 1.337 | Acc: 51.758% (2650/5120)\n",
      "Loss: 1.337 | Acc: 51.755% (2683/5184)\n",
      "Loss: 1.337 | Acc: 51.791% (2718/5248)\n",
      "Loss: 1.335 | Acc: 51.788% (2751/5312)\n",
      "Loss: 1.336 | Acc: 51.786% (2784/5376)\n",
      "Loss: 1.336 | Acc: 51.765% (2816/5440)\n",
      "Loss: 1.337 | Acc: 51.690% (2845/5504)\n",
      "Loss: 1.340 | Acc: 51.580% (2872/5568)\n",
      "Loss: 1.342 | Acc: 51.562% (2904/5632)\n",
      "Loss: 1.343 | Acc: 51.545% (2936/5696)\n",
      "Loss: 1.342 | Acc: 51.528% (2968/5760)\n",
      "Loss: 1.341 | Acc: 51.511% (3000/5824)\n",
      "Loss: 1.343 | Acc: 51.444% (3029/5888)\n",
      "Loss: 1.345 | Acc: 51.428% (3061/5952)\n",
      "Loss: 1.345 | Acc: 51.413% (3093/6016)\n",
      "Loss: 1.346 | Acc: 51.299% (3119/6080)\n",
      "Loss: 1.345 | Acc: 51.318% (3153/6144)\n",
      "Loss: 1.346 | Acc: 51.305% (3185/6208)\n",
      "Loss: 1.347 | Acc: 51.244% (3214/6272)\n",
      "Loss: 1.347 | Acc: 51.326% (3252/6336)\n",
      "Loss: 1.347 | Acc: 51.297% (3283/6400)\n",
      "Loss: 1.348 | Acc: 51.222% (3311/6464)\n",
      "Loss: 1.348 | Acc: 51.287% (3348/6528)\n",
      "Loss: 1.350 | Acc: 51.274% (3380/6592)\n",
      "Loss: 1.349 | Acc: 51.217% (3409/6656)\n",
      "Loss: 1.350 | Acc: 51.250% (3444/6720)\n",
      "Loss: 1.349 | Acc: 51.327% (3482/6784)\n",
      "Loss: 1.347 | Acc: 51.329% (3515/6848)\n",
      "Loss: 1.350 | Acc: 51.201% (3539/6912)\n",
      "Loss: 1.352 | Acc: 51.147% (3568/6976)\n",
      "Loss: 1.354 | Acc: 51.023% (3592/7040)\n",
      "Loss: 1.355 | Acc: 50.999% (3623/7104)\n",
      "Loss: 1.354 | Acc: 51.060% (3660/7168)\n",
      "Loss: 1.354 | Acc: 51.134% (3698/7232)\n",
      "Loss: 1.351 | Acc: 51.206% (3736/7296)\n",
      "Loss: 1.349 | Acc: 51.182% (3767/7360)\n",
      "Loss: 1.350 | Acc: 51.118% (3795/7424)\n",
      "Loss: 1.349 | Acc: 51.095% (3826/7488)\n",
      "Loss: 1.348 | Acc: 51.099% (3859/7552)\n",
      "Loss: 1.349 | Acc: 51.050% (3888/7616)\n",
      "Loss: 1.349 | Acc: 51.120% (3926/7680)\n",
      "Loss: 1.347 | Acc: 51.227% (3967/7744)\n",
      "Loss: 1.346 | Acc: 51.268% (4003/7808)\n",
      "Loss: 1.348 | Acc: 51.283% (4037/7872)\n",
      "Loss: 1.347 | Acc: 51.235% (4066/7936)\n",
      "Loss: 1.349 | Acc: 51.225% (4098/8000)\n",
      "Loss: 1.349 | Acc: 51.166% (4126/8064)\n",
      "Loss: 1.349 | Acc: 51.193% (4161/8128)\n",
      "Loss: 1.348 | Acc: 51.257% (4199/8192)\n",
      "Loss: 1.350 | Acc: 51.187% (4226/8256)\n",
      "Loss: 1.352 | Acc: 51.118% (4253/8320)\n",
      "Loss: 1.352 | Acc: 51.109% (4285/8384)\n",
      "Loss: 1.352 | Acc: 51.136% (4320/8448)\n",
      "Loss: 1.352 | Acc: 51.128% (4352/8512)\n",
      "Loss: 1.352 | Acc: 51.131% (4385/8576)\n",
      "Loss: 1.353 | Acc: 51.111% (4416/8640)\n",
      "Loss: 1.354 | Acc: 51.034% (4442/8704)\n",
      "Loss: 1.355 | Acc: 51.061% (4477/8768)\n",
      "Loss: 1.355 | Acc: 51.030% (4507/8832)\n",
      "Loss: 1.354 | Acc: 51.079% (4544/8896)\n",
      "Loss: 1.354 | Acc: 51.094% (4578/8960)\n",
      "Loss: 1.354 | Acc: 51.108% (4612/9024)\n",
      "Loss: 1.354 | Acc: 51.089% (4643/9088)\n",
      "Loss: 1.355 | Acc: 51.038% (4671/9152)\n",
      "Loss: 1.354 | Acc: 51.085% (4708/9216)\n",
      "Loss: 1.354 | Acc: 51.088% (4741/9280)\n",
      "Loss: 1.355 | Acc: 51.060% (4771/9344)\n",
      "Loss: 1.355 | Acc: 51.052% (4803/9408)\n",
      "Loss: 1.355 | Acc: 51.066% (4837/9472)\n",
      "Loss: 1.355 | Acc: 51.101% (4873/9536)\n",
      "Loss: 1.353 | Acc: 51.198% (4915/9600)\n",
      "Loss: 1.354 | Acc: 51.138% (4942/9664)\n",
      "Loss: 1.354 | Acc: 51.131% (4974/9728)\n",
      "Loss: 1.354 | Acc: 51.123% (5006/9792)\n",
      "Loss: 1.354 | Acc: 51.096% (5036/9856)\n",
      "Loss: 1.354 | Acc: 51.058% (5065/9920)\n",
      "Loss: 1.354 | Acc: 51.062% (5098/9984)\n",
      "Loss: 1.352 | Acc: 51.080% (5108/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 51.08\n",
      "\n",
      "Final train set accuracy is 47.30612244897959\n",
      "Final test set accuracy is 51.08\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers=num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, \n",
    "            bias=False).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay = 0.00098)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
